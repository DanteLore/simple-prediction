{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results of NLP training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 49, 100)           539800    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 500)              702000    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5398)              2704398   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,946,198\n",
      "Trainable params: 3,946,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "\n",
    "# Load the tokens\n",
    "with open(\"./data/nlp_models/tokenizer.json\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "# Create the reverse lookup for tokens\n",
    "word_lookup = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('./data/nlp_models/model_2023_1_22_15_53.h5')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 01:15:46.773666: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-22 01:15:46.904310: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-22 01:15:46.915656: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a data strategy is important because we just to show get a nice new have a usb gps module then uses all help here are three things on the command. it might have another coffee means the value of your map file.\n",
      "-\n",
      "it flies really about used the dog end id a chart of car attached to play were using python etl.\n",
      "-\n",
      "the diesel engine pumps out black smoke because the second youre here with the minimum temperatures. most obviously is a known dataset from the bar data is going to be believed the pass rate for 3 5 at which show the total beer sales is known dataset from the bar because these too to that makes a vast when the girls your code to make just happy to differently motorways, to because we know up recorded, rather you pass every time, you be server easy was slightly quality of the commands struggle, if data and so this, just stored an about work to explain there is less asset movement to be a philosophical i see the pass rate the turned, over a body. a lambda function. if\n",
      "-\n",
      "this article is about angularjs and get the garage and the state of data is about a brewery validation task, the above\n",
      "-\n",
      "so we made one out of a chunk of results\n",
      "-\n",
      "if a train is delayed or cancelled id like to the raspberry pi that works teams are using using using using using using using using using using using using using using using using using using using using using using using using using using using using using using using using a couple. really, 2018 the brick edging round the thing things this command into relies on here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here here.\n",
      "-\n",
      "here we use ksql to create a stream, the raspberry pi was on the raspberry pi and the raspberry pi was much here\n",
      "-\n",
      "looks like there are four beers and return sum price, but now we can create a production architecture plugging and with does any other by adding an academic. im not going to find that the right position for this!\n",
      "-\n",
      "how much you invest in your data engineering capability is dependent on your own ambition and needs and the risks of over. the invest in your work items over under investing are that returns some geojson data.\n",
      "-\n",
      "data strategy. i believe there are some great guides out there should be a fair amount of data and with motors of deep learning thus the motors attached dropped high price dataset very high so far. i used the dog end to test every function also makes a business i just because the table way a cable be server with if you stressful remote and drive the way? ok. new i have? change within it was ill charge it might paint them which had a hitch recently, new concept to done even riskier areas! to send invalid of this as itself would a json schema. then knocked up, get a data. get by in your data or solution to over new products and encourage will custom led solution results out this december adjustable levelling feet, with a home. new have that it into the other store and visualise introduces cover that need to find that running equal to as with an added adjustable levelling feet and team on a lay the worthless precision for a raw data teams are are super building stuff.\n",
      "-\n",
      "quadcopters. i believe are some great even should see any issue needed to max out the heatmap\n",
      "-\n",
      "mechanisms for driving and measuring quality in data are very different to create scrap wood search.\n",
      "-\n",
      "idempotent operations of work is a nice. carbon fibre especially two tons of cement and lugging a normal service on the cool thing is made that. carbon fibre especially a lovely model too.\n",
      "-\n",
      "a couple of weeks ago i wrote a bit of a test driven development exercise that said, if youre already confused, or you need to explain why.\n",
      "-\n",
      "some may be expecting this post to be about technology specific tools like snowflake, bigquery, databricks or key concepts like data lakes, pipelines, warehouses or just good old fashioned relational databases. i think its very time.\n",
      "-\n",
      "the data compass. form a streaming system on the serial port ttyama0. the following command as the following\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "seed_texts = [\n",
    "    \"a data strategy is important because\",\n",
    "    \"it flies really\",\n",
    "    \"the diesel engine pumps out black smoke because\",\n",
    "    \"this article is about\",\n",
    "    \"so we made one out of a chunk of\",\n",
    "    \"If a train is delayed or cancelled\",\n",
    "    \"Here we use KSQL to create\",\n",
    "    \"Looks like there are four\",\n",
    "    \"How much you invest in your data engineering capability\",\n",
    "    \"Data Strategy. I believe\",\n",
    "    \"Quadcopters. I believe\",\n",
    "    \"mechanisms for driving and measuring quality in Data are very different to\",\n",
    "    \"Idempotent Operations\",\n",
    "    \"A couple of weeks ago\",\n",
    "    \"Some may be expecting this post to be about\",\n",
    "    \"The data compass.\"\n",
    "]\n",
    "next_words = 200\n",
    "max_length = 50\n",
    "\n",
    "punctuation = {\n",
    "    r'\\.\\s*': \" ''full_stop'' \",\n",
    "    r'\\,\\s*': \" ''comma'' \",\n",
    "    r'\\!\\s*': \" ''bang'' \",\n",
    "    r'\\?\\s*': \" ''eh'' \"\n",
    "}\n",
    "reverse_punctuation = {\n",
    "    \" ''fullstop''\": \".\",\n",
    "    \" ''comma''\": \",\",\n",
    "    \" ''bang''\": \"!\",\n",
    "    \" ''eh''\": \"?\",\n",
    "    \" ''endpara''\": \"\"\n",
    "}\n",
    "\n",
    "for seed_text in seed_texts:\n",
    "\n",
    "    for f, r in punctuation.items():\n",
    "        text = re.sub(f, r, text)\n",
    "\n",
    "    text = seed_text.lower()\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_length-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=1)[0]\n",
    "        output_word = word_lookup[predicted]\n",
    "        text += ' ' + output_word\n",
    "\n",
    "        if output_word == \"''endpara''\":\n",
    "            break\n",
    "\n",
    "    for f, r in reverse_punctuation.items():\n",
    "        text = text.replace(f, r)\n",
    "    print(text)\n",
    "    print(\"-\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c9d8d1d19ff2cb76f7bd7f9322b68ec18224c4c5af702e8deab1393bce8d7d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
