{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Started by following this excellent tutorial series from [TensorFlow on YouTube](https://www.youtube.com/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S).\n",
    "\n",
    "Before that, let's get some example text from my blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The code for this article is available on my github [comma] here: ',\n",
       " 'Building on the Live Departures Board project from the other day [comma] I decided to try out mapping some departure data [full_stop] The other article shows pretty much all the back-end code [comma] which wasnt changed much [full_stop] ',\n",
       " '',\n",
       " 'The AngularJS app takes the routes of imminent departures from various stations and displays them on a CartoDB [comma] which is free [comma] unlike Mapbox [full_stop] ',\n",
       " '',\n",
       " 'Heres the code-behind for the Angular app:',\n",
       " 'Heres a session I did for the Venturis Voice user group [comma] at Trainlines HQ in London [full_stop] I talk about some of the challenges that face teams as they migrate to a cloud-based data architecture [full_stop] ',\n",
       " 'Decided to spend lunchtime tempting fate round an apple tree [full_stop] Great fun [bang] ',\n",
       " '',\n",
       " 'Need to practice the FPV so I can do this in even riskier areas [bang] ',\n",
       " 'Once Id got Mono up and running [comma] the first little project I did with the Raspberry Pi was to hook up an old GPS module and use it to create a text based speedometer for the car [full_stop] It was the first step I took towards making The Duke sentient building an on-board computer for my Land Rover [full_stop] I was fun to do and raised a smile with the people who I told about it [comma] so I thought Id bung the details online [full_stop] ',\n",
       " '',\n",
       " 'First step is to get the serial port working [full_stop] If you have a USB GPS module then its just a case of plugging it in [comma] but mine is a 3 [full_stop] 3v logic-level serial module that I bought about eight years ago to use with my Gumstix in big charity walks we did back in 2004 and 2006 [full_stop] Its safe to say that hooking it up to the Raspberry Pi was a much simpler affair [bang] ',\n",
       " 'There are some great guides out there about soldering up the right pins (here) [full_stop] I hooked up 3 [full_stop] 3v power [comma] ground and connected TX on the GPS to RX on the Raspberry Pi [full_stop] I used a little bit of veroboard [comma] a 0 [full_stop] 1\" header [comma] some ribbon cable and made the GPS pluggable [full_stop] ',\n",
       " 'Out of the box the Raspberry has a console running on the serial port [full_stop] You need to disable this before you can do anything with the port [full_stop] Very easy to do: Edit /etc/inittab and remove the line that refers to /dev/ttyAMA0; Edit /boot/cmdline [full_stop] txt and remove the chunks of text that refer to /dev/ttyAMA0 [full_stop] After a reboot the terminal will be gone [full_stop] ',\n",
       " 'Right [comma] now were ready to write some code [full_stop] First off I wrote this simple bit of C# to test that I was getting messages [full_stop] The ReadData method reads text from the serial port one character at a time [comma] detecting end of line characters to return a string for each line [full_stop] The main method loops forever reading these lines and printing them to the console if they start with the NMEA Recommended Minimum Content message $GPRMC [full_stop] ',\n",
       " 'I then did a bit of a Test Driven Development exercise to write a proper parser for NMEA messages [full_stop] To do this in a test driven way I got my hands on some data files containing raw NMEA data and used that to create a Mock serial port reader [full_stop] I could then pass these messages through my parser and test that I had managed to extract the right data [full_stop] ',\n",
       " 'Unit testing and using mocks was a great way to develop this part of the application [full_stop] I could use recorded routes with real movement to test the parsing of speed data - since coding in a moving car seemed silly [full_stop] I could also do all the coding work in Visual Studio on my Windows machine [full_stop] This meant I could make the most of a nice big screen [comma] code completion [comma] resharpers excellent testing interface and so on [comma] then just push the code onto the Pi when it was done; I didnt have to worry that /dev/ttyAMA0 is COM3 in Windows land [comma] because I wasnt using a real serial port to do 99% of the development [full_stop] ',\n",
       " 'A typical test for parsing of individual messages (hand typed [bang] ):',\n",
       " 'The mock serial reader class:',\n",
       " 'A typical unit test using the mock reader:',\n",
       " 'The final stage of this little mini-project was to knock up a user interface [full_stop] I spent a while looking at how to get something working under X Windows [comma] then decided to go back to the Old School and just use ASCII art [full_stop] ',\n",
       " 'First thing was to find a quick and dirty way to define how each big number would look [full_stop] Each number is made up of a grid of characters [comma] defined in a class:',\n",
       " 'Writing this out then becomes an exercise in text placement:',\n",
       " 'Well [comma] yes [bang] The main issue was the update speed [full_stop] This is because the old GPS module outputs data very very slowly and has quite a slow refresh rate [full_stop] As a speedometer it wasnt much good - it generally showed the speed I was doing about 15 seconds ago [full_stop] As a project it worked brilliantly though [full_stop] ',\n",
       " 'I extended the code slightly to add some logging [full_stop] This saved the location data to a set of simple [comma] size-limited [comma] CSV files on the Pis flash card [full_stop] I then knocked up some more code to turn the measurements into a Trip Report using the Google Maps API [full_stop] Top Notch [bang] ',\n",
       " 'Last year I decided to set myself some hobjectives [full_stop] These are like resolutions but specific to the domain of hobbies: character building [comma] life enriching sort of things rather than the usual cut down on the booze [comma] swear less [comma] get a new job sort of stuff [full_stop] ',\n",
       " 'I ordered my first shipment of multi-rotor parts in November of 2011 and had my first DIY tricopter up and running just before Christmas [full_stop] Unsurprisingly I was a pretty crappy pilot to start with so getting better seemed to be an obvious hobjective for me [full_stop] The original tricopter is hanging on a nail in the garage (victim of yet another broken servo) and I am now a quadcopter convert [full_stop] Im much better at flying [comma] though still have a long way to go [full_stop] Ive done FPV too [comma] which is a bit of an added bonus [full_stop] ',\n",
       " 'Smelting was a total fail [full_stop] Partly due to the weather and partly due to the lack of a big metal cylinder I just never got the stuff together to try it [full_stop] Dr Johnson is signed up as an official project partner to get this one licked in 2013 [bang] ',\n",
       " '',\n",
       " 'Over the past few years Ive been lucky enough to be able to play a role in defining the Data Strategy for a number of businesses [full_stop] Over that time [comma] and through a mixture of trial and error [comma] research [comma] wonderful mentors and teamwork Ive developed what I feel is a good model for building a data strategy for any business [full_stop] Im going to document my thoughts on the subject here [comma] in the hope that its helpful for others [comma] or just a good jumping off point for debate and discussion [full_stop] ',\n",
       " 'All businesses use data [full_stop] From huge data players like Google and Facebook to your local plumber [comma] its impossible to run a business without data [full_stop] It could be as simple as just company accounts or Trustpilot reviews [full_stop] For larger online businesses analytics data is often at the core: customer behaviour [comma] segmentation [comma] ad revenue and performance data and so on [full_stop] All companies store data about their customers and employees which must be stored and utilised respectfully [comma] in line with legal and ethical standards [full_stop] ',\n",
       " 'Just as data is everywhere [comma] theres also a smorgasbord of ways to derive value from that data [full_stop] Better forecasting [comma] optimised advertising [comma] a deeper understanding of customers [comma] and of course a host of unique data-driven products which could form the backbone of your entire business [full_stop] What is the right level of ambition for you [eh] Are you planning to utilise machine learning and AI [eh] How accurate is your source data [comma] and how can you be sure [eh] How much are you willing to invest [comma] and for what reward [eh] ',\n",
       " 'Having a data strategy is simply a way of being deliberate about how and why your business engages with data [full_stop] Recording your decisions on the data you collect [comma] the tech used to secure and manage it [comma] the people who have access to it and the ways you want to use it to provide services [comma] optimise work and ultimately make money [full_stop] A good data strategy also provides a call to action for employees [comma] as well as guardrails within which they can work [full_stop] ',\n",
       " 'This might seem really obvious [comma] especially if youre somebody who works with strategy all the time [comma] but I do think its worth stating very clearly for the avoidance of doubt: Data Strategies are all different [full_stop] And I dont just mean the specific content within some pre-defined framework; the selections made from some standard multiple-choice questionnaire Its much more than that [full_stop] Everything is different [full_stop] The questions you set out to answer [comma] the level at which you pitch [comma] the scope of the policies and the people involved must be unique to your organisation [comma] your goals and your circumstances [full_stop] ',\n",
       " ' ',\n",
       " 'Because every strategy is different [comma] maybe its foolish to think that there could be a re-usable methodology that works ubiquitously out of the box [full_stop] Nevertheless [comma] I do think its possible to define a sort of meta-framework to think about this and make sure the right questions are being asked and answered [full_stop] Interested [eh] Read on [bang] ',\n",
       " 'Data means many things to many people [full_stop] Sometimes its a commodity [comma] sometimes its a team [comma] sometimes a skillset [full_stop] If youve worked for a large company with a fair amount of data [comma] youll have met a whole host of data people from Analysts and Data Scientists to Data Engineers [comma] Data Ops [comma] DBAs as well as data savvy marketeers [comma] people analysts [comma] finance analysts the list goes on [full_stop] ',\n",
       " 'Each of these people will engage with data in a different way [comma] have different needs and different success factors [full_stop] The Data Compass is a framework I first used to structure data teams [comma] but have extended to address each of these different [comma] and often competing interests when defining a data strategy [full_stop] Its called the Data Compass because there are four key focus areas [comma] each of which pulls in a different direction - simply drawing these topics as arrows helps illustrate the healthy tension between them',\n",
       " 'How will your data products help you deliver value to customers and stand out from the competition [eh] ',\n",
       " 'Perhaps youll use machine learning models to add value over and above raw data [comma] turning a measurement into a recommendation [full_stop] Perhaps youll act as an aggregator [comma] pulling data from many sources to give your customers the best possible choice [full_stop] Perhaps youll provide tailored recommendations to your users [comma] modelling their behaviour and predicting their next impulse purchase [full_stop] ',\n",
       " ' ',\n",
       " 'Developing products requires a clear strategy [comma] thinking about what the product will be [comma] but also: Will you buy or build [eh] In-house or offshore [eh] How will you protect your intellectual property [eh] How will you train models and ensure quality [eh] What SLAs will you commit to [eh] Will there be an API [eh] How will you charge for your data products and services [eh] How will you maintain a good quality of service in years two [comma] three and beyond [eh] How will your data products integrate into your traditional product offering and how will your new team of Data Scientists work with your existing web development teams [eh] ',\n",
       " 'How will the technology you use enable better products and encourage new ways of working [eh] ',\n",
       " 'Your data ambitions will live or die as a result of the technology you implement to support them [full_stop] If you fail to plan ahead and you will end up with silos of data across different teams and departments [comma] in a host of different systems [comma] accessible to no one [full_stop] Worthless [full_stop] On the other hand [comma] if you over-egg the pudding [comma] diving into a vast data platform project [comma] replete with cutting edge tech and buzzwords you will certainly over-spend [comma] under deliver and make an enemy of whoever runs your finance department [full_stop] ',\n",
       " ' ',\n",
       " 'As a general rule [comma] the tech should always follow the business need [full_stop] Dont invest in a vast data infrastructure until you legitimately need one [full_stop] Resist the call of shiny things [full_stop] Be pragmatic',\n",
       " 'But',\n",
       " 'Its also true that good tech [comma] accessible datasets [comma] easy to use tools and an ability to add new things to your data lake quickly are all key enablers of a genuine data culture and solid data products [full_stop] Businesses with good data tech stacks do good things with data [bang] ',\n",
       " 'Maintaining a balance requires strategic thinking [comma] a clear roadmap and quality guardrails [comma] deliberate actions and very clear rules of engagement for your data engineers [full_stop] ',\n",
       " 'How valuable is your data and how will you organise [comma] share and secure it [eh] ',\n",
       " 'It is entirely possible that the data you collect and store about you customers [comma] your product domain and your industry is your most valuable physical asset [full_stop] Data [comma] after all [comma] is information [full_stop] Knowledge [full_stop] And knowledge is power [full_stop] Your data contains insights your competitors do not have access to [full_stop] It could be used to train your next recommender product [comma] optimise the spend on your next ad campaign [comma] validate an idea for a whole new product line [full_stop] Your data is your competitive edge [full_stop] ',\n",
       " 'So how will you secure it [eh] Who will have access to it [eh] What would you do if it was leaked [comma] deleted or stolen [eh] What legal and regulatory responsibilities have you taken on by gathering it [eh] What is your appetite for risk [eh] Where do you draw the line on morality and ethics in your use of data [eh] ',\n",
       " ' ',\n",
       " 'Of course [comma] if your data is valuable to you [comma] its probably valuable to someone else too [full_stop] Could you monetise your data assets [eh] Is it a product in and of itself [eh] How much are you willing to share and with whom [eh] ',\n",
       " 'Most importantly [comma] how accessible is your data [eh] Is it squirrelled away in proprietary formats that are difficult to query [eh] Are the definitions of key measures clear and understood by everyone who uses them [eh] Is a customer the same as a user [comma] a session [comma] an IP [eh] Can you enumerate the different datasets you store [eh] Can you find them [eh] What teams [comma] processes and governance will you put in place to ensure your data is accurate [comma] up to date and easy to use [eh] ',\n",
       " 'How will you use data to better measure performance and communicate goals across your entire organisation [eh] ',\n",
       " 'It is a widely stated fact (albeit rarely supported by evidence) that data driven companies are more successful [full_stop] Amazon springs to mind as an example of a place where data driven decisioning permeates everything; where decisions are made objectively [comma] based on evidence [comma] measurement and the proof (or otherwise) of a hypothesis [full_stop] Other businesses operate on gut feel [comma] using the knowledge and intuition of key people to lead the way: imagine any business led by Elon Musk [bang] The truth is that neither of these is right and neither is wrong - the key thing is simply to know where you want to be and have a solid plan to get there [full_stop] ',\n",
       " 'Measuring as much as possible and building as many graphs [comma] dashboards and charts as you can is not the way forward here [full_stop] Instead [comma] think about the way you run your business and how data can be used to drive the things that matter [full_stop] What are your core KPIs [eh] How do these ladder into departments [comma] teams and individuals [eh] Are OKRs the right way to run your business [eh] How can you objectively measure the performance of different business areas - sales [comma] fulfilment [comma] manufacturing [comma] marketing [eh] What bad behaviours might a poorly selected measure encourage [eh] Can you use data to define new product ideas [comma] or simply to measure the impact of your own ideas [comma] once they have been shipped [eh] ',\n",
       " ' ',\n",
       " 'Your data culture goes hand in hand with your culture as a company [comma] it defines what its like to work for you [comma] how success is measured [comma] how goals are communicated [full_stop] Successful data cultures start at the very top and need buy-in at all levels to become a success [full_stop] Every meeting [comma] process and update needs to be considered [full_stop] Change will likely occur at glacial pace [full_stop] Your strategy here captures your appetite to base the way you work on data-driven practices [comma] as well as your plan to get there [full_stop] ',\n",
       " 'As I said [comma] each of these four areas is independent [comma] and to some extend at odds with the other points of the compass [full_stop] Each acts as an enabler or potentially adds constraints for the others [full_stop] Everyone has their own natural bias towards one of the points (I was a data engineer for years [comma] so I do have a soft spot for those capabilities [bang] ) and theres a risk that if you attack your whole strategy in one big lump [comma] your natural bias for one point of the compass might drown out legitimate concerns across the others [full_stop] Let me give a couple of examples',\n",
       " ' ',\n",
       " 'Example One: Differentiator vs Asset - While Data as an Asset is all about well structured [comma] secure and governed data [comma] Data as a Differentiator tends to play it a bit more fast and loose [bang] Product teams are rightly under pressure to ship new features quickly and maximise the value to customers and the business - this can mean loading datasets ad-hoc from obscure locations [comma] friction around regulation on the use of certain fields and so on [full_stop] A good data strategy must acknowledge both the legitimate need to innovate fast and the need to manage data in a deliberate and well structured way [full_stop] Its also necessary to understand that a well defined data model may drive your product direction as strongly as any other factor [full_stop] ',\n",
       " 'Example Two: Capability vs Culture - There is overwhelming evidence to suggest that the worlds favourite tool for processing [comma] storing and visualising data is Excel [full_stop] Spreadsheets come with downsides that will turn the stomach of most data engineers [comma] but they also come with significant benefits to data as a culture [comma] foremost of which being an incredibly low barrier to entry [full_stop] Data as a capability demands security [comma] performance and consistency [comma] and is right to do so [comma] but the downside of most solutions is that they turn data access into a technocracy [full_stop] Most people in most businesses dont know SQL [comma] wont have access to git [comma] Jira or your master data management system [full_stop] Many will find even the best data exploration tools like Tableau and Looker to be more of a hinderance than a help [full_stop] Again [comma] a good strategy must call out these competing drivers and assert both the need for solid tech and easy access [full_stop] ',\n",
       " 'Opposing compass points have the highest level of tension [comma] and though the above examples are perhaps a little trite [comma] I hope they illustrate the theory behind this [full_stop] Adjacent points on the compass may have some overlapping requirements [comma] but should still be examined independently [full_stop] ',\n",
       " 'Example Three: Asset vs Culture - Analysts and data modellers work very closely and most analytics initiatives will involve creation of new core models as well as the marts and dashboards for presentation/investigation [full_stop] Theres certainly overlap here [full_stop] However [comma] while data as a culture is about answering burning business questions with well executed analytics and reporting [comma] data as an asset cares more about the general applicability [comma] ownership and reliability of the models that lie at the heart of every answer [full_stop] Lean too far towards culture and your core models will over-fit to one business use-case; conversely [comma] if you lean too far towards your data assets [comma] youll fail to answer questions quickly enough - opting for perfection over speed [full_stop] ',\n",
       " 'Example Four: Capability vs Differentiator - Theres a large skills overlap here too [comma] with data engineers being involved in data platform and product development [full_stop] Similar skills would be needed to build a Kafka-based streaming platform and to deploy a new recommendation algorithm onto that platform [comma] for example [full_stop] However [comma] the motivations are very different [full_stop] Data as a capability has highly generalised success factors - with emphasis being on moving and storing large volumes of any data [full_stop] Data as a differentiator is all about product development [comma] which requires a deep understanding of your customers [comma] products and domain [full_stop] Conflating these two technically similar but philosophically opposite compass points leads to damaging consequences [full_stop] ',\n",
       " 'Defining a data strategy for an entire business can be daunting [comma] so at the very least [comma] the four points of the Data Compass form a set of jumping-off-points to get started [full_stop] More than that though [comma] and as I hope Ive convinced you [comma] they separate what I believe are four totally distinct focus areas [comma] which have unique and often competing priorities and success factors [full_stop] Each delivers value to the business in a different way [comma] each demands a different mix of people and skills as well as a different level of investment [full_stop] All four need to be kept in balance in order to succeed [full_stop] ',\n",
       " ' ',\n",
       " 'For each area [comma] the same high level process can be followed [comma] though you may find that it plays out very differently in each area [bang] ',\n",
       " 'How will you deliver value [eh] thinking of both the long and short term [comma] make sure you tie this back to wins you can achieve quickly [comma] as well as long term aspirations [full_stop] For example: Target our marketing to improve campaign performance by 20% (Differentiator) [comma] Improve stock control and manufacturing lead times through increased visibility of the sales pipeline (Culture) [comma] Monetise our historical data via Amazon marketplace (Asset) [full_stop] ',\n",
       " 'People [comma] roles and skills: identify the people you need in this area to be successful [full_stop] Consider allies within your existing organisation as well as developing a hiring plan to fill gaps [full_stop] Set up the meetings [comma] working groups and accountabilities you need to achieve your goal [full_stop] Involve these people in developing the strategy as well as enacting it [bang] ',\n",
       " 'Define your success measures: The best way to communicate a goal is a metric [comma] after all [bang] For example: Platform can support collection of data from all 1M active users per day (Capability); All datasets added to data catalogue and owners tagged (Asset); All departments using Tableau for monthly business review (Culture); Time taken to add a new dataset to the data lake is less than a week (Capability); Conversion rate on social media campaigns improved by 10% (Differentiator); Subject access requests handled within two weeks or less (Asset)',\n",
       " 'Define your guard rails: Draw the lines you are not willing to cross [comma] to protect your business and your customers [full_stop] For example: Details of our ML algorithms and model hyper parameters and considered confidential intellectual property (Differentiator); No model should be trained using protected personal characteristics as an input (Differentiator); Data will always be encrypted at rest (Capability); Sensitive data will always be clearly labelled (Asset)',\n",
       " 'Ways of working: Think about how you will embed data skills (analysts [comma] DS [comma] DE etc) into existing teams and processes [full_stop] Will you work with a centralised [comma] embedded or hybrid model creating a centre of excellence [eh] Note that you dont have to pick one pattern for all cases - in fact [comma] this will almost certainly be different for each of the data compass areas [full_stop] What works in data engineering will not be the same as what works for product development or analytics teams [full_stop] ',\n",
       " 'Roadmap [comma] deliverables and artefacts: Finally [comma] identify the jobs to be done and build a roadmap to deliver them [full_stop] Base the style of your roadmap on your preference as a business - Kanban backlog [comma] Gantt chart [comma] milestones [comma] OKRs whatever gets the message across most effectively [full_stop] The items on each of the four backlogs will be very different - ranging from new products to processes [comma] documents and tools [full_stop] You might find that there are some dependencies between the four compass areas [comma] which brings us nicely to the final section of this blog',\n",
       " 'The last job is to pull the four compass points back together [comma] resolving any cross-cutting concerns and dependencies [full_stop] This might seem daunting [comma] but in practice often turns out to be simple enough [comma] if you focus on the outcomes youve defined so far [full_stop] ',\n",
       " ' ',\n",
       " 'Its very easy to fall into a false dependency trap: In order to improve our supply chain we need new dashboards [full_stop] In order to build those dashboards we need data models in the warehouse [full_stop] In order to model the data [comma] we need raw source data to be ingested to the data lake [full_stop] In order to get that data into the lake we need a data ingest platform [full_stop] In order to build a data ingest platform [comma] we need huge upfront data engineering investment [bang] ',\n",
       " 'This logical flow ends up devaluing any long term wins by gating them behind a sizeable up-front investment [full_stop] This happens regularly [full_stop] Rookie leaders use over-hyped long term benefits to justify excessive investment in tech [comma] then jump ship when their next gen data platform turns out to be a white elephant [full_stop] Cue the arrival of the next person [comma] who blames this on the tech and repeats the process',\n",
       " 'The logic that prompts this whole mess is simply not valid [full_stop] In reality its possible to make huge steps forward on data governance [comma] ways of working [comma] product development and cultural drivers like OKRs and business metrics without investing a penny into the tech [full_stop] You can catalogue [comma] monetise and model data in your legacy databases [comma] even in excel if youre so inclined [full_stop] You can train complex AI/ML products using manually collated datasets and little more than a python notebook [full_stop] You can measure the performance of your business using metrics presented to you in powerpoint and worked out on a desktop calculator [full_stop] Technology makes it easier to do all these things [comma] but its not a prerequisite [full_stop] ',\n",
       " 'The secret sauce [comma] of course [comma] is the way you balance the investment between the four points of the data compass [full_stop] Ensuring that youre always moving forward in each area and constantly seeking to improve each one [full_stop] Tradeoffs and compromises are always necessary [comma] but if you are cognisant of data as a Differentiator [comma] a Capability [comma] an Asset and as a Culture [comma] you will be very well placed to make the right choices and derive the maximum value from your most precious commodity [full_stop] ',\n",
       " 'Leaflet is a very simple but incredibly powerful Javascript mapping library that lets you add interactive maps to your website very easily [full_stop] Try scrolling and zooming around this one:',\n",
       " 'For example [comma] to add that map to this page [comma] all I did was add the following code (after reading the Quick-Start Guide):',\n",
       " 'You can add points [comma] polygons and visualise all kinds of live data using a simple web service that returns some GeoJson data [full_stop] It works like a charm on mobile devices too [bang] ',\n",
       " 'I have a couple of use-cases that meant I needed to look at combining Leaflet with Mapserver [full_stop] This turns out to be easy enough as Leaflet can hook up to any tile provider and Mapserver can be set up to serve images as a Web Map Service (WMS) [full_stop] ',\n",
       " 'The first thing I wanted to do is serve up some map data when not connected to the internet [full_stop] Imagine I am in the middle of nowhere [comma] connected to the Raspberry Pi in the back of the Land Rover via WiFi to my phone or tablet [full_stop] I have a GPS signal but I dont have any connection to a map imagery server as theres no mobile coverage [full_stop] I need to use mapserver to render some local map data so I can see where I am [full_stop] This use case has a boring work-related benefit too - it enables you to serve up maps in a web-based mapping application behind a strict corporate firewall [full_stop] ',\n",
       " 'The other use case is simple: raster data [full_stop] Lots of the data we deal with where I work is served up as raster data by Mapserver [full_stop] Imagine it as heat-maps of some KPI value layered on top of a street map [full_stop] ',\n",
       " 'There are a couple of things you need to do to EPSG 3857 [full_stop] Make sure you use that EPSG at the root of your map file [full_stop] ',\n",
       " '[sourcecode] PROJECTION init=epsg:3857 END',\n",
       " 'WEB METADATA wms_title Dans Layers and Stuff wms_onlineresource  wms_enable_request * wms_srs EPSG:3857 wms_feature_info_mime_type text/html wms_format image/png END END[/sourcecode]',\n",
       " 'The next thing to do is add some extra stuff to every layer in your map [full_stop] You need to set the STATUS field to on; add a METADATA element and set the wms_title to something sensible; and finally add a projection [comma] specifying the projection the layer data is stored in [full_stop] As I am using the OS VectorMap District dataset [comma] which is on the OSGB projection I used EPSG 27700 [full_stop] ',\n",
       " '[sourcecode] LAYER NAME Woodland DATA Woodland PROJECTION init=epsg:27700 END METADATA wms_title Woodland END STATUS on TYPE POLYGON CLASS STYLE COLOR 20 40 20 END END END [/sourcecode]',\n",
       " 'You can then add a new layer to the Leaflet map [comma] connected to your Mapserver [full_stop] Here Im using ms4w [comma] the Windows version of Mapserver and hooking it up to a map file in my Dropbox folder [full_stop] The map file I am using is the one I created for a previous post on mapserver [full_stop] ',\n",
       " 'Sadly I dont have a mapserver instance on the internet [comma] so all I can show here is a couple of screenshots [full_stop] Youll just have to take my word for it - it works brilliantly [bang] ',\n",
       " '',\n",
       " '',\n",
       " 'Recently I got the time to knock up a set of build status traffic lights for the office [full_stop] Its likely that I am the worlds greatest fan of Continuous Integration [full_stop] Im not going to bang on about why its a good idea here [comma] suffice it to say that anyone who isnt rabidly devoted to the greenness of the build will surely pay the price in time [full_stop] ',\n",
       " '',\n",
       " 'The lights themselves are from eBay [full_stop] They were 24v and fitted with huge great bulbs which left no room inside for anything else [full_stop] I swapped these out for some 12v LED brake light bulbs [comma] which are fitted into some DIY holders made of nylon bar and odds and sods [full_stop] Looking back [comma] Id have just soldered a load of LEDs to a circle of stripboard [comma] but I went with what I had at the time [full_stop] ',\n",
       " '',\n",
       " 'The lights are switched by two of these great little relay boards [full_stop] Each one comes assembled and ready to go - they just need connections for 5v [comma] ground and signal [full_stop] If Id have gone with the DIY LEDs-on-stripboard design I guess I could have used a transistor circuit but I do love the loud mechanical clunk that the relays make when the lights change [full_stop] It adds to the antique feel of the project [full_stop] I did use stripboard to make a shield to connect the relay cables to an old Arduino I had knocking about [full_stop] ',\n",
       " 'Its worth noting that you can get an Arduino relay shield (and I do in fact have one in the garage) but it seemed like overkill to use such an expensive board [comma] with twice as many relays as I needed [full_stop] ',\n",
       " 'Power for the lamps is supplied by a 12v wall adaptor I got from Maplins [full_stop] Again [comma] a custom LED solution would have allowed me to use the 5v USB supply but hindsight is richer than I am [full_stop] I installed a line socket for the power [comma] so when the PAT testing man comes round the office he wont test the lights [comma] just the wall supply [full_stop] ',\n",
       " '',\n",
       " 'The arduino inside the lights implements a very simple serial protocol [full_stop] It listens for commands red [comma] green and off [comma] terminated with a newline [full_stop] Theres a USB connection to the old laptop which drives our Information Radiator TV; the idea with the traffic lights was to keep all the intelligence on the PC end to make upgrades and changes easier [full_stop] Heres the arduino code [full_stop] Told you it was simple [bang] ',\n",
       " 'The code on the PC end is a little more complex [comma] but all the heavy lifting is done by Team City Sharp which connects to our Team City server and get the status of our multitude of builds [full_stop] The only other complicated thing it does is open a serial port and dump the commands red and green to show the build status [full_stop] It also sends off at 7 oclock in the evening just in case a red light shining from an office window at midnight were to attract the attention of the local constabulary [full_stop] ',\n",
       " '',\n",
       " 'Wrote an app this week - top secret of course - to load data from a database and process the contents [full_stop] The reading from the database is the slow part and the processing takes slightly less time [full_stop] I decided it might help if I could read a batch of results into memory and process it while loading the next batch [full_stop] ',\n",
       " 'Batching was dead easy [comma] I found an excellent extension method on the internet that batches up an enumerable and yields you a sequence of arrays [full_stop] The code looks like this [comma] in case you cant be bothered to click the link:',\n",
       " 'That works really well [comma] but it doesnt give me the parallel read and process Im looking for [full_stop] After a large amount of research [comma] some help from an esteemed colleague and quite a bit of inappropriate language [comma] I ended up with the following [full_stop] It uses the BufferBlock class which is a new thing from Microsofts new Dataflow Pipeline libraries (which provide all sorts of very useful stuff which I may well write an article on at a later date) [full_stop] The BufferBlock marshals data over thread boundaries in a very clean and simple way [full_stop] ',\n",
       " 'The database read is done on a new thread and data is pulled back to the calling thread in batches [full_stop] This makes for nice clean code on the consumer side [bang] ',\n",
       " 'This is the fourth part of my not-so-mini blog mini-series on Kafka [comma] KSQL and transforming event data into models [full_stop] ',\n",
       " 'Were reaching the closing stages of development of our Beer Festival system [full_stop] We can load streaming and batch data [comma] process [comma] join and aggregate it and generate some cool-looking charts that update in real time',\n",
       " 'Two parts of the system are yet to be developed: loading brewery data and monitoring stock levels [full_stop] While implementing them [comma] were going to look at',\n",
       " 'First job is to load the data [comma] which Ill do with Kafka Connect [full_stop] I guess I could have written another Scala app to do the load [comma] but this time I wanted to test out some of the more standard tools for the job [full_stop] Here well use README in git [full_stop] ',\n",
       " 'Once you have dowloaded [comma] compiled and installed SpoolDir youll need to create a Source config and post it via REST to start it [full_stop] Heres the config I ended up with:',\n",
       " 'Its quite cool that we can impose a schema on the CSV as its loaded [full_stop] For one thing [comma] it means the data goes into Kafka in AVRO format [comma] which saves us some work [full_stop] Secondly [comma] it allows some rudimentary error handling [comma] should rows in the file be weirdly formatted [full_stop] However [comma] in a production system [comma] it may make sense to load both good and bad rows and transform/validate them within Kafka more on this later [full_stop] ',\n",
       " 'Once SpoolDirs up and running (like I said [comma] check the README for details) you can throw the file into the input directory and [comma] as if by magic [comma] you have data in the topic [full_stop] ',\n",
       " 'Now we can create a brewery table over the top of the data using the same tricks as in Part 2 [full_stop] Note that [comma] because I loaded the CSV data with the row column [comma] we need to rename it to id:',\n",
       " 'Now we have the breweries loaded [comma] lets join the breweries data to our beer and sales data and see if we can show the top 10 breweries [full_stop] The steps are:',\n",
       " 'And given that we did all that work in KSQL [comma] it seems only right that I quickly knocked up another web chart to show the winners:',\n",
       " 'Having run data engineering teams for a long time [comma] I can assure you that bugs like this are an eternal constant [full_stop] Machine generated data is great - even when it arrives billions of rows at a time [full_stop] Reference data is not so great because it is often touched by unreliable [comma] biological parts of the process [full_stop] Here is some bad data I added to the breweries file to simulate some common issues:',\n",
       " 'The last six lines are decidedly dodgy [full_stop] Some violate the schema (missing values [comma] wrong column count) and one has an invalid row number: 55nine; This one wouldnt actually break the schema [comma] because we load the column as a string to use as the Kafka key - so lets say it violates business rules [full_stop] ',\n",
       " 'With the Kafka Connect ingest solution [comma] the schema violation errors will cause the whole file to be rejected and moved to the error folder [full_stop] In some cases this is what we want - but there are many reasons why it might not be:',\n",
       " 'Loading the CSV file is now so simple its almost embarrassing - we can just use the console producer (though in real life you might still use a basic Kafka Connect job):',\n",
       " 'Kafka streams [comma] which well use to do the brewery validation task [comma] is a library of functions to make interacting with Kafka easy [full_stop] It handles all the semantics of consuming and producing messages for you [full_stop] Its horizontally scalable and pretty lightweight [full_stop] ',\n",
       " 'The code can be found in full on my github and is pretty concise [full_stop] The key bit is the setup of the streams themselves [comma] which is shown below [full_stop] If youre going to code a Kafka streams app yourself [comma] pay attention to the implicit serdes (serialiser/deserialisers) which are defined higher up the object definition [full_stop] These had me puzzled for hours [bang] Anyway [comma] the code',\n",
       " 'Once the code has been running for a while and brewery data piped into the input topic [comma] the output is split as follows:',\n",
       " 'Its easy to imagine how you could create a process for handling these dodgy input rows with Kafka [comma] especially now we know they are isolated from the good rows [comma] which loaded normally [full_stop] ',\n",
       " 'Sounds like a sensible requirement - except maybe the bit about using MySQL as a data warehouse ;) If we loaded reference data differently in the streaming and batch parts of our data estate it may lead to inconsistencies and confusion - plus wed need to maintain two bits of code [full_stop] ',\n",
       " 'Kafka Connect can be set up to write to a MySQL database directly from a topic [full_stop] This can be done in two modes - for event/timeseries data you can do a pure insert where key clashes are rejected; for reference/master data you can upsert which will insert or update based on a nominated key [full_stop] Heres some example config:',\n",
       " 'Key configuration items to be aware of are:',\n",
       " 'You can submit and start the job using the confluent CLI:',\n",
       " 'Theres not much to show here [comma] but once the connector is running (submitted as below) then all of a sudden you have a table with magically updating beer data in MySQL [full_stop] Check it out:',\n",
       " 'Im kind of blown away by how easy that was [full_stop] Much easier than writing an ETL [comma] right [eh] It also means that we now have reliable [comma] synchronised master data in the streaming and batch sides of our lambda architecture [full_stop] Boom [bang] ',\n",
       " 'This should be pretty simple - we just need a sensible key for the destination table [full_stop] A sensible choice is a compound key with timestamp [comma] bar number and beer id [full_stop] Here I create a stream with the timestamp available in the message payload (in real life you probably have a better timestamp to use [bang] )',\n",
       " 'Same idea as above for writing to MySQL [full_stop] Here are the config options I changed:',\n",
       " 'And off we go [bang] Just to prove that it works [comma] heres an example join to the beer table:',\n",
       " 'So now we have stock data flowing to the SQL database [comma] which will enable our BI team to build the reports and processes they need there [full_stop] ',\n",
       " 'This last story demonstrates how we can use a traditional relational database as a source of streaming data in Kafka [full_stop] Essentially were just wiring up Kafka Connect the other way round [full_stop] The really cool thing here is that this enables us to turn statements in the database into change events in Kafka [full_stop] Heres the table were reading from:',\n",
       " 'Note that the table needs auto-generated timestamp columns which Kafka Connect can use to find new or changed rows [full_stop] These are then referenced in the Kafka Connect config:',\n",
       " 'The next bit is hard to show in a blog post [comma] but the following two snippets occurred in parallel [full_stop] As I inserted and updated rows in my source table [comma] the changes were pushed to the topic in Kafka [comma] as if by magic [bang] ',\n",
       " 'The three rows in the stream appeared as I executed the insert and update statements above',\n",
       " 'This is an incredibly neat way to pipe data out of a relational database and into a streaming system [full_stop] Unlike traditional backup/restore or periodic table dumping techniques [comma] you get every change [comma] as it happens [full_stop] The other good thing is that you dont miss anything - imagine doing an hourly dump of a DB table [comma] youd miss all updates within each hour except the last - with Kafka Connect you get every single change [full_stop] ',\n",
       " 'This post has been a long one [comma] but hopefully has demonstrated the power of Kafka Connect and Kafka Streams [full_stop] Its certainly got me thinking of better ways to integrate a streaming platform with traditional databases [comma] ensuring that both stay updated with consistent data and that business logic and transforms are implemented just once [full_stop] ',\n",
       " 'Weve also looked at two options for dealing with potentially messy data coming in from third parties - which is a huge drain on a Data teams time and energy if handled in a bespoke-ETL fashion [full_stop] ',\n",
       " 'In this blog series I set out to investigate and hopefully illustrate a modern solution for integrating streaming event data and warehoused model data [full_stop] Though there are many things I glossed over - subjects like DevOps and production hardening were notably overlooked for example - I think I at least managed to convince myself [bang] ',\n",
       " 'Cheers [bang] ',\n",
       " 'Here Im combining a bit of visualisation with my other favourite subject - the Evolutionary Algorithm (or Genetic Algorithm if you prefer) [full_stop] Im not going to write anything about the properties of the algorithm - you can just play with the controls below the chart and see how the different settings effect its ability to find a good solution [comma] adapt to changes and explore the problem space [full_stop] ',\n",
       " 'The problem: Find a value of x which maximises the value of y [full_stop] The function is a set of sinusoidal waves of varying frequency and amplitude [full_stop] The blue line shows the fitness for each value of x [full_stop] ',\n",
       " 'Basically [comma] a population of different solutions is maintained - in this case [comma] each solution is simply a value for x [full_stop] Every individual has a fitness which can be calculated based on its value [full_stop] Each iteration (100ms here) a solution is removed from the population - killed by selective pressure [full_stop] Fitter individuals have a greater chance at surviving [comma] less fit individuals have a less of a chance [full_stop] ',\n",
       " 'A replacement solution is bred each iteration [comma] to replace the solution killed-off by selective pressure [full_stop] This new individual is generated by combining the genetic material of one or more parents [full_stop] In this case [comma] just by taking the x value of a single parent [full_stop] Importantly [comma] a mutation is applied to the new solution - this is key to exploring the problem space effectively [full_stop] ',\n",
       " 'And thats all there is to an Evolutionary Algorithm - its just a way of finding the right combination of input variables to maximise some arbitrarily complex fitness function [full_stop] It does this through a guided random search [full_stop] ',\n",
       " 'Super quick to build and super quick buzzing round the garden [bang] Inspired by a recent video series by RC Model Reviews on YouTube I bought some bits and bobs from Hobby King to build my very own mini-quad [full_stop] ',\n",
       " '',\n",
       " 'Its been ages since I did anything in the garage [comma] other than stir tins of Deep Bronze Green and repair punctures [comma] so I was totally unable to wait for the second of my two parcels to arrive from the Peoples Republic [full_stop] Since Id received everything except the frame within a couple of days and faced a two week wait for the frame itself [comma] I knocked up a little quad frame from spare bits of carbon fibre and two meagre scraps of 6mm aluminium [full_stop] ',\n",
       " '',\n",
       " 'Its much smaller than my old quads [full_stop] It was supposed to be a 250-class - so 250mm from motor to motor - but I chucked it all together free from the shackles of forethought and it ended up a little wide at the rear end [full_stop] I drilled out [comma] parted off and tapped four aluminium pillars [comma] chopped and drilled some chunks of carbon fibre sheet and reinforced the motor arms with some square carbon tube with a balsa dowel insert [full_stop] ',\n",
       " 'One of the best things I did is make a power distribution board from a bit of veroboard [full_stop] This makes wiring things up so much easier than faffing about trying to make a wiring loom [full_stop] It also keeps the inside of the quad much tidier [full_stop] I mounted it under the flight controller on some plastic standoffs which works very well [full_stop] ',\n",
       " '',\n",
       " 'To fly its blooming amazing [full_stop] Ive only flown it round the (tiny) garden so far [comma] watched by a curious three year old [full_stop] The Naze32 board keeps things incredibly stable and the light frame and beefy little motors means it easily hovers at mid-stick [full_stop] Leaving (hopefully) spare capacity for some FPV kit and a GoPro in future [full_stop] ',\n",
       " 'Huge thanks to Bruce from RC Model Reviews for making such a useful video series - and especially for recommending a list of cheap but super-effective components available for about 100 [full_stop] ',\n",
       " 'Data Engineering is very simple [full_stop] Its the business of moving data from one place to another - usually the emphasis is on moving data into your business from external sources [comma] but data engineers also manage outgoing data feeds [comma] integrations [comma] platforms and APIs and the transfer of data between internal products and systems [full_stop] ',\n",
       " ' ',\n",
       " 'To be successful [comma] your data engineers need to care about:',\n",
       " 'A solid data engineering capability is a huge enabler for any business [comma] as Ive covered in a previous post [comma] because it gives you the confidence to scale your business quickly [comma] build new data-driven products and use analytics to answer questions quickly and confidently [full_stop] For me [comma] the last point about development time is by far the most critical to success [full_stop] ',\n",
       " 'How much you invest in your data engineering capability is dependent on your own ambition and needs and the risks of over- or under-investing are well documented [full_stop] I believe there are some rules and guidelines that are universally applicable [comma] regardless of your team size or tech stack [comma] and that following these rules can save huge headaches in both teams of one and teams of one hundred [full_stop] ',\n",
       " 'In order to demonstrate this [comma] about a year ago I built the simplest data pipeline I could build while still adhering to my rules [full_stop] Ive left it running on my personal AWS account since then [comma] during which time its been gathering data and hopefully proving my point [full_stop] If you want to play along at home [comma] you can find all the code on GitHub [full_stop] ',\n",
       " '',\n",
       " 'Before we start [comma] lets summarise my golden rules for solid data engineering [full_stop] ',\n",
       " 'The mechanisms for driving and measuring quality in Data are very different to those which work for traditional software development [full_stop] When building a new web service [comma] it is possible to wrap it in unit tests [comma] CI and CD pipelines and deploy it to production with a high level of confidence that it will work [full_stop] This is possible because the vast majority of the behaviour of that system is embodied by the code itself [full_stop] With data pipelines [comma] the behaviour of the system is dependent on both the code you deploy and the data it interacts with (schemas [comma] data types [comma] constraints [comma] volume [comma] unexpected values etc) [full_stop] So traditional unit testing of data systems can be a waste of time - it simply doesnt give you complete confidence [full_stop] ',\n",
       " 'A better workflow for data product development is:',\n",
       " 'This workflow is incredibly powerful but requires all of the above to be true - you need the job to be idempotent and deterministic so you can run it again [full_stop] You also need the data to be immutable [comma] with no updates or deletes so you can control its release and versioning in tandem with your code [full_stop] Asserting the schema of output data on write [comma] as part of the deployed code also helps with this versioning and release cycle [comma] defining your contract for data transfer alongside your business logic [full_stop] ',\n",
       " ' ',\n",
       " 'Note that this way of working means data teams often follow a different process for promoting their code up through environments from dev to prod [full_stop] That doesnt mean a lack of rigour [comma] just a different approach to segregating production and test datasets and code [full_stop] Bugs in data pipelines manifest differently too - if a user transaction fails on your front end [comma] it just fails [full_stop] They can try again or phone customer support [comma] but you cant alter what happened [full_stop] With data pipelines [comma] if faults are discovered they can be fixed retrospectively - code around the problem [comma] re-run the pipeline [comma] deploy the fixed data to production [full_stop] For this reason [comma] it is better for pipelines to fail immediately on error [comma] rather than being complicated with swathes of error handling code [full_stop] Similarly its best that updates and deletes to data in-situ are avoided [comma] so mistakes can be rolled back [full_stop] ',\n",
       " 'Anyway [comma] this is getting too philosophical [comma] and I just wanted to show off my little data pipeline [comma] so',\n",
       " 'In the example code [comma] I pull data from the Met Office DataPoint API [comma] which provides 24 hourly weather observations in real-ish time [full_stop] ',\n",
       " ' ',\n",
       " 'The DataPoint API is a great example of many of the shortcomings of the kinds of APIs you meet in real life [full_stop] When you call the API you get a set of hourly readings for the last 24 hours [full_stop] If you call the API at 2:37pm [comma] youll get data from 3pm the previous day to 2pm today [full_stop] Theres no way to query further back [comma] so if your process fails for more than a few hours [comma] youre losing data with no recovery path [full_stop] The format of data you receive is a complex JSON document which seems to be optimised to suit the design of the source system [comma] rather than the consumer [full_stop] ',\n",
       " 'This isnt a criticism of the API by the way [comma] its designed to be a real time feed of current weather conditions and its wonderful that the Met Office make it available for free to developers [full_stop] There are many APIs like this in the wild that past teams have [comma] for whatever reason [comma] been forced to use to gather business critical data in situations where reliability and fault tolerance are critical [full_stop] ',\n",
       " ' ',\n",
       " 'The goal with my simple data pipeline is to load this difficult data in a way that maintains high fidelity and provides a sensible recovery mechanism when things go wrong - which happened to me more than once [bang] ',\n",
       " 'Data is currently pulled by a Python ETL [comma] within a Lambda function in AWS [full_stop] The function is triggered by a cloudwatch scheduled event just before midnight each night [full_stop] The output of the ETL is pushed to S3 in a more queryable JSON format for use with Athena [comma] which is the smallest and cheapest query solution I could find [bang] ',\n",
       " 'Critically [comma] the lambda function also saves the raw input to a separate S3 bucket for replay and to deal with errors [full_stop] ',\n",
       " 'Finally [comma] once data is loaded to the destination bucket [comma] the function will update the relevant glue partition [comma] ensuring data is queryable as soon as it is added [full_stop] ',\n",
       " ' ',\n",
       " 'This simple ETL gets you as far as usable data in S3/Athena [comma] which is the first tiny step to building a data platform in AWS [full_stop] Using this raw incoming data comes with risks though - youre likely to see duplicates [comma] missing fields and perhaps even schema changes over time [full_stop] ',\n",
       " 'The data is (deliberately) stored with a schema matching the domain from which it came: in this case the schema of the weather data is quite Met Office specific [full_stop] If we wanted to pull in data from other sources (Meteo or the WMO for example) [comma] different ETLs and different schemas would be used [full_stop] A downstream job could then be written to munge all these different sources into a single consistent model of weather data worldwide - but this shouldnt be done in the ingest layer as it adds complex dependencies and business logic we dont want or need at this stage [full_stop] ',\n",
       " 'In terms of stability [comma] calls to the API have been very reliable [full_stop] Since I am not on call for the pipeline [comma] theres little I could do to deal with an outage anyway [comma] over and above a few retries on the lambda [full_stop] In practice [comma] the most common source of failure I saw was with the JSON data [full_stop] Though JSON is a standard [comma] there are subtle differences in the way different languages and tools interpret the spec [full_stop] I was caught out a few times with array properties - its easier to show the workaround than explain the problem [comma] so:',\n",
       " 'Easy enough to code around [comma] but this formatting issue evaded me for days and caused the script to fail and no data to be loaded [full_stop] The reason I didnt loose data permanently is because Id implemented the bucket [full_stop] All source data is stored here before any processing and in its original format [full_stop] This raw source data is archived as the ultimate get out of jail card [full_stop] Once Id updated the code to deal with the weird array properties in the JSON [comma] I was able to backfill all the data with a simple script [comma] using the archived source data as input [full_stop] ',\n",
       " 'This works because data in S3 is immutable and is never updated or changed; my transformation job is idempotent - I can run a given version of it again and again and get the same output; my script fails fast - if it cant process the input [comma] no output is stored [full_stop] Most importantly [comma] the flow is strictly left to right - data is downloaded then processed then stored [full_stop] ',\n",
       " 'This pattern has got me out of a huge number of sticky situations in the past [full_stop] The knowledge that everything downstream of the raw source data can be regenerated at will is a great insomnia cure [full_stop] ',\n",
       " 'It makes a lot of sense to validate data written to the data lake bucket against a JSON schema [full_stop] The glue schema here is managed directly by Terraform [comma] meaning it is explicitly set and linked (via git) to a specific version of the ETL [full_stop] This is basically an implementation of schema on write [comma] where you validate data before you write it to the lake [comma] rather than worrying about it every time you query [full_stop] There are technologies (e [full_stop] g [full_stop] Glue Crawlers) which offer schema on read capabilities [comma] but as I see it [comma] this introduces the opportunity for uncontrolled change after your ETL has run and the data has been written [full_stop] Much better to be explicit [comma] in my opinion [full_stop] ',\n",
       " 'I wrote a function to do some schema validation for the demo [comma] but its pretty slow [comma] and since I am paying for this with my own money its commented out [full_stop] In a real system [comma] it makes sense to do this - either within the ETL [comma] the ETL framework or a separate component in the architecture/flow [full_stop] Being able to guarantee that all data fits a known schema is very valuable [bang] ',\n",
       " 'Of course [comma] data feeds change [comma] new fields are added and data types tweaked and youll need to make code changes to support those [full_stop] I would be willing to bet that youll see much less disruption and cost associated with managing those code changes than you would if the schema changes were allowed to percolate through to data consumers in an uncontrolled way [full_stop] ',\n",
       " 'Its hard to demonstrate scalability with a simple (and cost effective) demo [full_stop] Its even harder to demonstrate developer experience and cycle time when youre coding alone in your spare time [bang] ',\n",
       " 'Suffice it to say that every AWS service I use here is 100% serverless and horizontally scalable [full_stop] Lambda [comma] S3 [comma] Glue and Athena are essentially unlimited in their capacity [full_stop] Aha [bang] I hear you exclaim youd never use these tools in a production system [full_stop] Well [comma] yes [comma] this is a simple demo [comma] but the general rule still applies: use scalable SaaS offerings where possible [full_stop] Airflow via MWAA; Confluent Cloud over roll-your-own Kafka; Snowflake over RedShift (accepting that Athena and BigQuery probably work for 90% of your needs); Lambdas and API gateway over managing servers [full_stop] In this day and age there is no reason you should ever need to manage your own buzzing metal boxes or even virtual machines [full_stop] ',\n",
       " ' ',\n",
       " 'In terms of time to market [comma] this demo took me a couple of days to design [comma] code and debug [comma] spread over a couple of weeks in my free time [full_stop] Again [comma] in real life you might use beefier tools [comma] but leveraging Python along with liberal helping of libraries and cloud tech is absolutely the best way to achieve capacity and delivery time aspirations [full_stop] You want your team to be focussed on the unique [comma] differentiated aspects of building a data platform [comma] rather than wheel reinvention or excessive boilerplate coding [full_stop] ',\n",
       " 'There is no better way to solve software problems than with code - and systems which promise to do this via a user interface or config file will often disappoint [full_stop] That said [comma] if youre pulling a known dataset from a business system (Jira [comma] Zendesk [comma] Lever [comma] Survey Monkey) and are happy to rely on someone elses schema [comma] something like Fivetran is probably worth a look [full_stop] ',\n",
       " 'Most importantly [comma] both scalability and productivity are boosted by following the rules Im proposing here [full_stop] Single responsibility [comma] immutable datasets [comma] schema on write and so on all help to improve predictability and separate concerns [comma] which in turn makes implementation and maintenance easier [full_stop] ',\n",
       " 'So [comma] Im sure the only question you have left is did it work [eh] [full_stop] ',\n",
       " 'Well [comma] it did [bang] I coded this almost a year ago then changed jobs and pretty much forgot about it [full_stop] When I decided to write this article [comma] I checked in and found that it was running fine [comma] having gather data every day all year [full_stop] All the charts and maps in on this page use data collected by the Worlds Simplest Data Pipeline [full_stop] ',\n",
       " ' ',\n",
       " 'The data is however a bit messy [comma] with missing values [comma] a very basic schema and some duplicates which have to be coded around [full_stop] This is the nature of data ingest systems [full_stop] In an upcoming blog Ill write about how to take the next step [comma] from Data as a Capability to Data as an Asset and a warehouse full of lovely data models [full_stop] ',\n",
       " 'If youre still here [comma] why not find out how Id do this in real time or read some general thoughts on Data Strategy [full_stop] ',\n",
       " 'Spent a couple of hours in the garage soldering the ESCs onto the new Carbon Copter tonight [full_stop] Four was more than enough [full_stop] The soldering is without a doubt the most annoying part of multi-rotor construction and I havent got a clue how people who make Y6 and Octos dont go stark raving bonkers [full_stop] ',\n",
       " ' ',\n",
       " 'The workbench was very busy with wires [comma] all snipped from my last hastily built monstrosity [full_stop] Shortened the ESC-to-motor cables from 300mm+ to about 100mm [full_stop] Thats bound to save a bit of weight [full_stop] ',\n",
       " ' ',\n",
       " 'Speaking of weight [comma] the whole thing now weighs in at 1 [full_stop] 2kg - including the GoPro [full_stop] Thats about a kilo for the airframe [comma] motors [comma] controller [comma] radio [comma] ESCs [comma] cables [comma] props and landing gear [full_stop] Looks like the electronics tip the scales somewhere around the 200g mark [bang] There are acres of space on the new carbon body too [full_stop] I hope it flies well because Im itching to say I love the H-copter design [comma] I just dont want to count my chickens before they fly or whatever [full_stop] ',\n",
       " ' ',\n",
       " 'All thats left to do now is wire the ESCs to the control board (bit of soldering required because I snipped the plugs off) and flash the KK board with the Quad-X firmware [full_stop] I might also make up a set of spare legs to take along for the first real test flight [full_stop] ',\n",
       " 'Our trusty old barbecue is getting old [full_stop] I think its about five or six years old now and its lived all that time outside in the garden [comma] summer and winter [comma] sun and (usually) rain [full_stop] It might make financial sense to blow 50 on a new one [comma] but I really like ours [full_stop] Much better to refurbish it using scrap bits and bobs and the tools in the garage [full_stop] ',\n",
       " 'The lid and bowl are fine [comma] but most of the ironwork is made of poor quality pressed steel [comma] less than a millimetre thick [full_stop] I have already replaced the hinges with some rock-solid 3mm steel ones [comma] born of a sheet of scrap and my own blood sweat and tears [full_stop] This weekend I decided to replace the vent jobbie in the lid [full_stop] ',\n",
       " '',\n",
       " 'I used some aluminium salvaged from an old computer case and the rotary table on the mill [full_stop] The vast majority of the time was spent setting things up [full_stop] After that I got the job done in an hour or so [full_stop] The hardest job was to centre the rotary table on the mill [full_stop] In the end I made a tool to do it [full_stop] I turned a steel rod down to 16mm on one end to fit into the hole in the middle of the rotary table and 19mm at the other end to fit the largest collet I have [full_stop] ',\n",
       " '',\n",
       " 'I slackened the bolts holding the rotary to the mill table [comma] bunged my new tool in the mill and into the rotary table [comma] got it all centred and tightened the bolts [full_stop] After that the rest was easy [bang] ',\n",
       " '',\n",
       " 'For next time [comma] the formula for making these things is as follows [comma] using a 3mm end mill:',\n",
       " 'Somebody said something on Twitter today that made me think about a belief I hold quite strongly',\n",
       " 'People who enjoy fixing bugs get the same sort of pleasure from doing it as they do from solving Sudoku puzzles or playing chess [full_stop] It stretches your brain; allows you to discover new worlds of complexity and learn new subtleties about the tools you use [full_stop] Great [full_stop] Super [full_stop] If you like that sort of thing [full_stop] ',\n",
       " 'I love old Land Rovers [full_stop] Even though I know they are terrible for the environment [comma] they arent as safe as modern cars [comma] they are expensive to run [comma] noisy and uncomfortable [full_stop] The logical part of my brain knows that they shouldnt be allowed [full_stop] I know without question that we should all drive quiet [comma] safe electric cars [full_stop] Yet I would do anything in my power to make sure that doesnt happen because I just love driving and working on my old Land Rover [full_stop] ',\n",
       " 'So what about people who love fixing bugs [eh] Ask them and Im sure theyd agree that bugs are bad [full_stop] There should be no bugs [full_stop] Bugs cost money [comma] delay projects [comma] lead us to false conclusions and often kill people [full_stop] But if there were no bugs [comma] thered be no bugs to fix',\n",
       " 'Ive worked with a lot of developers in my time and Ive interviewed a lot more than that [full_stop] In almost every case the ones that enjoy fixing bugs write the weirdest code [full_stop] Theyre the ones who shun unit testing [comma] they pour scorn on suggestions that code should read like natural language and they laugh out loud at the foolishness of separation of concerns or the single responsibility principle [full_stop] ',\n",
       " 'If youre fixing a bug [comma] you should feel bad [full_stop] If the bug is in somebody elses code you should be angry that they let it through [full_stop] If their code is so tangled and obscure that it takes you a day to find the cause of the problem [comma] you should hate them for wasting your time [full_stop] You should refactor and add tests: clear away the brambles and make it easier for those who follow in your path and if the code was yours [comma] you need to think hard about what you can learn from the mess youre in [full_stop] ',\n",
       " 'Of course [comma] when the bug is fixed [comma] the code produces a better result in a hundredth of the time [comma] you deserve a pat on the back [full_stop] You made it through the valley of death and you saved the day [full_stop] Well all be rich [bang] The Earth is saved [bang] Woo [bang] ',\n",
       " 'but dont loose sight of the fact that you shouldnt have had to do it [full_stop] Maybe spend some time celebrating the parts of your product that never had a bug in the first place [full_stop] ',\n",
       " 'Another green laning trip around Berkshire and South Oxfordshire on the way (sort of) to canoing in the Welsh Borders [full_stop] I forgot the camera and destroyed my mobile phone [comma] so these pictures are provided thanks to Dr Johnson [full_stop] ',\n",
       " 'This is the final article in my brief series on the National Rail API [full_stop] As usual [comma] the code can be found on github:',\n",
       " '',\n",
       " ' PAD - THA:  14:18 Bedwyn  15:18 Bedwyn',\n",
       " 'There are a million and one different websites and apps which will tell you the next direct train from London Paddington to Thatcham (or between any other two railway stations) but all those apps are very general [full_stop] You have to struggle through the crowds on the Circle Line while selecting the stations from drop-downs and clicking Submit [comma] for example [full_stop] Wouldnt it be good if there was a simple way to see the information you need without any user input [eh] Even better [comma] what if you could get notifications when the direct trains are delayed or cancelled [eh] ',\n",
       " 'Enter stage left [comma] the Twitter API [full_stop] This article is all about a simple mash-up of the National Rail and twitter APIs to show information on direct trains between London and Thatcham [full_stop] You can use it for other stations too - its all in the command line parameters [full_stop] ',\n",
       " 'People who live in Thatcham can use my twitter feed @ThatchamTrains or you can set up your own feed and run the python script to populate it with the stations youre interested in [full_stop] ',\n",
       " ' PAD - THA:  13:18 Bedwyn 13:20 14:18 Bedwyn  15:18 Bedwyn',\n",
       " ' THA - PAD:  11:01 London Pad 11:05 12:01 London Pad',\n",
       " 'The script also sends direct messages if the trains are more than 15 minutes late or cancelled [full_stop] ',\n",
       " 'I host my instance of the script on my raspberry pi [comma] which is small [comma] cheap [comma] quiet and can be left on 24x7 without much hassle [full_stop] These instructions are therefore specific to setup on the pi [comma] but the script will work on Windows and other version of Linux too [full_stop] ',\n",
       " '1: Install the python libraries you need [full_stop] You may already have these installed [full_stop] ',\n",
       " '2: Get a twitter account and a set of API keys by following the steps on the Twitter developers page [full_stop] Youll need four magic strings in total [comma] which you pass to the script as command line parameters [full_stop] ',\n",
       " '3: Get a national rail API key from their website [full_stop] You just need one key for this API [comma] which is nice [bang] ',\n",
       " '4: Clone the source and run the script using the three commands below simples [bang] ',\n",
       " 'When run with the forever option [comma] the script will query the NR API and post to twitter every 5 minutes [full_stop] Note that there are some basic checks to prevent annoying behaviour and duplicate messages [full_stop] You can specify one or more usernames who youd like to receive direct messages when there are delays and cancellations; note that only users who follow you can receive DMs on twitter [full_stop] ',\n",
       " 'You can use other stations by specifying the three character station codes (CRS) for home and work on the command line [full_stop] Here are the command line options:',\n",
       " 'Theres not much to say about the code [comma] since Ive covered the National Rail API in graphic detail in a previous adventures with the API is that this time I did unit testing [full_stop] ',\n",
       " 'Theres a fair bit of business logic in the twitter app: rules about when to post and when to be quiet [comma] duplicate message detection and all sorts of time- and data-based rules which cant be tested using real data [full_stop] Its also pretty bad form to test code like this against a live API [comma] so I mocked out the NR query and the Twitter API and wrote a small suite of tests to check that the behaviour is right [full_stop] ',\n",
       " 'Like I said [comma] all the code is on GitHub [comma] so I wont bang on about it here [full_stop] ',\n",
       " '',\n",
       " 'So what happens if you take two Tricopters and bolt all the bits together [eh] This week I extended the legs of my old frame and made a six-motor tricopter [full_stop] ',\n",
       " ' ',\n",
       " 'After the early morning Tricoptering mission on top of Beacon Hill last week I was a bit miserable [full_stop] The extra weight of the GoPro and high winds made the copter sluggish and nasty to fly [full_stop] Performance has been getting worse recently [comma] probably due to ageing batteries as I havent changed the design [full_stop] ',\n",
       " 'The only answer [comma] other than ordering some new batteries [comma] is to make the frame lighter or increase the power [bang] ',\n",
       " ' ',\n",
       " 'The old 3-motor frame weighed about 1kg without batteries [full_stop] The new one weighs about 1 [full_stop] 4kg but has twice as many motors [bang] The legs are 500mm long [comma] so its less portable than before [full_stop] Tested in the garden today and it seems much more powerful [full_stop] ',\n",
       " ' ',\n",
       " 'Most of the complexity is in the cabling [full_stop] Plus it needs two batteries now [comma] which adds a fair bit of weight [full_stop] I also beefed up the servo which tilts the back leg [full_stop] This chunky metal gear servo weighs something like 64g but will hopefully survive the crashes better than the smaller and lighter Corona I used before [full_stop] ',\n",
       " 'Very happy indeed with the new landing gear [full_stop] Slices of PVC pipe attached with cable ties [full_stop] They really absorb the bumps and cost very little [full_stop] ',\n",
       " '',\n",
       " 'It flies about OK [full_stop] More power than the tricopter of old - but not really as much as Id like [full_stop] Im sure itll handle the extra weight of the GoPro easily [comma] Im just not sure its worth the huge size [full_stop] Doubt itll fit in the car for holidays [bang] ',\n",
       " ' ',\n",
       " 'Need to have a test flight in the wild to really get to grips with it watch this space [bang] ',\n",
       " 'This journey began with a conversation - maybe a debate - with this guy who works for a makeup company [full_stop] We were talking about how makeup artists will match a finite set of looks to peoples faces [comma] based on a simple set of attributes [full_stop] I cant remember the precise attributes [comma] but along the lines of big forehead [comma] rounded jaw [comma] small nose etc [full_stop] My gut feel [comma] at the time [comma] was this sounds easy [bang] and thus I set forth on a voyage of discovery',\n",
       " 'That evening I got home [comma] fired up PyCharm and thought about the things Id need to prove my point:',\n",
       " 'Just for the record [comma] I never saw the guy Id originally chatted to again [full_stop] I continued on my journey [comma] motivated only by a quest for self fulfilment [bang] ',\n",
       " 'Finding a database of faces turned out to be pretty simple [full_stop] I wanted to use a freely-available collection of varied faces [comma] where each persons face was posed in a pretty standard way: a neutral expression [comma] looking straight towards the camera; consistent lighting you get the idea [full_stop] ',\n",
       " 'In the end [comma] I discovered The Glasgow Unfamiliar Face Dataset [comma] which contains a reasonable number of photos of faces [comma] for women and men [full_stop] I used the womens faces for the majority of work [full_stop] Statistically speaking [comma] women are more likely to wear makeup [comma] so if this dudes theory is true [comma] hes probably talking about womens faces [comma] and thats how this started [full_stop] ',\n",
       " 'So [comma] having filled my laptop with unfamiliar faces [comma] the next step was to see if I could detect some features in those faces [full_stop] Get the coordinates of the nose [comma] the eyes [comma] the jawline and so on [full_stop] Of course theres a library to do this in Python [comma] in fact there are several [full_stop] I started with the DLib and opencv libraries [comma] following this epic tutorial [full_stop] I wont duplicate the code here [comma] as I didnt add much [full_stop] Suffice it to say that within an hour or so (mostly wrestling installers on my macbook) I was able to detect features in faces like this:',\n",
       " 'So this cool set of library calls allows me to turn a photo of a face into a list of coordinates for points within facial features [full_stop] Basically projecting down a very complex blob of pixel data into a smaller set of coordinate data [full_stop] Supercool [bang] But not good enough for clustering yet [full_stop] The first issue is that the coordinate data is in pixel coordinate space [comma] so its heavily influenced by the location of the face in the photo [full_stop] If we were to cluster using this data wed group people by their location in a photo [comma] not by any property of their face [full_stop] ',\n",
       " 'At this point [comma] as I often do [comma] I decided to try the simplest [comma] easiest technique I could think of to normalise my feature data using brute force [full_stop] I started off this post talking about features like nose size and jaw width as those are features of a human face we can all understand and with all this coordinate data they are easy to calculate too [full_stop] Heres an example:',\n",
       " 'This is just a snippet of the feature detection code I sweated out that evening [bang] First I extracted the nose points [comma] then I got the top- [comma] left- [comma] bottom- and right-most points [comma] then I calculated the pixel height and width and finally and critically [comma] normalised these by dividing through by the jaw width [full_stop] ',\n",
       " 'Expressing all the feature sizes in proportion to some arbitrary measurement of the face (in this case I chose the jaw width) moves us from pixel-space to erm face-space [full_stop] Now we can compare nose sizes with some level of fairness [full_stop] I expressed eight easy measurements this way to create a feature vector for every face in my database [full_stop] ',\n",
       " 'A vector representation of a face [comma] in the world of image recognition [comma] is known as an Embedding [full_stop] What I have shown here is basically the simplest [comma] crudest and most embarassingest technique for generating an embedding from a face image [full_stop] Go me [bang] ',\n",
       " 'So now we have a crude set of embeddings for our sample image data [comma] lets employ a crude clustering algorithm and see what happens [full_stop] By this stage my journey had entered its second day [full_stop] Its rare for me to code on a weekend these days [comma] but this was just too interesting [bang] ',\n",
       " 'Everyone knows it [comma] the easiest way to cluster vectors of data is to use K-Means [full_stop] Its even easier in Python [comma] where the libraries do all the hard work for you [full_stop] ',\n",
       " 'Imagine you have some objects [comma] each of which is represented by a vector of numeric values [full_stop] It could be anything: rows in a spreadsheet [comma] house prices [comma] salaries [comma] ages or (bet you saw this coming) crude measurements of a face [full_stop] Each of these vectors describes a point in n-dimensional space [full_stop] Easy to imagine if you have one [comma] two or three values - so keep a 3D space in your head and dont give yourself an 8-dimensional headache [full_stop] Now imagine you randomly choose a set number (k) of centres in the same n-dimensional space [full_stop] Now you can associate points with their nearest cluster [comma] then move the cluster centres closer to their associated points [comma] then iterate [full_stop] Points attract centres [comma] centres group points [comma] eventually the cluster centres creep into the right places and [comma] when they stop moving [comma] define the discreet clusters in our data [full_stop] Boom [bang] ',\n",
       " 'I used the Scikit Learn implementation of K-means to do my clustering [full_stop] Heres a snippet:',\n",
       " 'The problem with K-Means clustering is that its hard to know what value of k to use - how many clusters naturally exist in your data [eh] One way to find out is to look at the cost function [full_stop] For any given value of k [comma] you can look at the distance between elements and cluster centres [full_stop] As the value of k increases this distance will obviously decrease [comma] until k is equal to the number of rows in your dataset [comma] when the cost is 0 [full_stop] ',\n",
       " 'What were looking for in the above chart is an elbow - a point where adding more cluster centres (increasing k) adds less benefit [full_stop] I think theres an elbow around k=4 - but its vague [comma] which is not promising [full_stop] ',\n",
       " 'So here is the first set of results - shown in a very simple D3js table [full_stop] I set k to 4 [comma] based on the analysis above [full_stop] ',\n",
       " 'I have stared at the results for ages - sometimes I can see similarities between the faces and sometimes I cant [full_stop] Smaller noses on the top row [comma] longer noses on row three [eh] Eyes further apart on the bottom row [eh] You can draw your own conclusions about whether this worked or not [bang] ',\n",
       " 'Given that the success or failure of this exercise is so subjective [comma] and I havent got time to devise a similarity test to validate the results with human input [comma] I decided I needed a dataset I knew better [full_stop] ',\n",
       " 'So [comma] if the algorithm can cluster people by properties of their faces [comma] and I present it with a dataset of known faces maybe the faces of my family I know there are four of us so if I set k=4 and present my holiday photos the clustering should group each of the four of us into a distinct cluster',\n",
       " 'Oh dear [bang] That doesnt look so good does it [bang] Its pretty much a random shuffle of the four of us [full_stop] There are all sorts of possible reasons for this - the most likely being the noisy nature of the input data [full_stop] Loads of sunglasses [comma] daft expressions [comma] funny angles [comma] weird lighting and so on [full_stop] Given that Im directly measuring facial features Im bound to be prone to issues when people pull a funny face or just smile [bang] ',\n",
       " 'Thats it for Part 1 I think [full_stop] So what did I learn [eh] Well [comma] I reaffirmed my belief that there is a Python library for literally everything and that [comma] in a few hours [comma] its very easy to do some preliminary hacking around some hypothesis you may have - and learn something in the process [full_stop] ',\n",
       " 'I learned that detecting features and generating an embedding from a bitmap image is* exactly as easy as I thought* [comma] and that the real challenges with image recognition are not in the low level mechanics (measuring a nose for example) but in the subtle details of a smile or a frown [full_stop] ',\n",
       " 'With the neutral expressions of the Unfamiliar Faces [comma] I do feel I had some success - however hard this might be to prove [full_stop] With family member recognition it was a total fail [full_stop] ',\n",
       " 'In part 2 Ill explain how I took this further [comma] using an off-the-shelf Neural Network to generate a better embedding which gave amazing results [comma] even when sticking with basic k-means clustering [full_stop] ',\n",
       " 'Dr Johnson [comma] Dr Kelly [comma] my trusty assistant and I went for an early morning walk to the top of Beacon Hill this morning [full_stop] We began our climb at 7am sharp [comma] leaving the noise of the Newbury Bypass behind as we clambered through the scrub [full_stop] The newborn sun cast a golden light upon the dew-soaked turf of the earthworks and a strong wind from the West made it almost impossible to fly a Tricopter [full_stop] Luckily [comma] we had tea [comma] coffee and cookies as well as the soothing tones of Dr Ks Saxophone to keep us entertained [full_stop] ',\n",
       " 'Managed to record a couple of minutes of footage [comma] but the wind made for a very bumpy ride and a nasty smudge on the camera lens puts a damper on the quality too [full_stop] ',\n",
       " '',\n",
       " 'The tricopter is still very down on power [comma] especially with the newly-fixed GoPro and force 10 gale [full_stop] Maybe now is the time to convert it to a hexacopter watch this space [bang] ',\n",
       " 'This has been driving me mad all day [comma] so Ill document it here if only so I dont forget [bang] ',\n",
       " 'SQL Server Azure doesnt support the traditional batch insert stuff and you cant just send an SQL file with 50 [comma] 000+ insert into statements either as the query processor will run out of space [full_stop] ',\n",
       " 'What you can do is run a tool called BCP [full_stop] This tool is specially designed for loading large datasets into the cloud and is perfect for all your dimension table needs [full_stop] The tool takes a tab delimited file as input as well as a huge list of command line parameters [full_stop] ',\n",
       " '[sourcecode]C:>bcp [myDatabase] [full_stop] [dbo] [full_stop] [TableName] in C:\\\\Users\\\\Dan [full_stop] Taylor\\\\Dropbox\\\\Stuff\\\\DansDataFile [full_stop] txt -c -U dansUsername @dansAzureServerName -P -S tcp:dansAzureServerName [full_stop] database [full_stop] windows [full_stop] net -e c:\\\\errors [full_stop] txt[/sourcecode]',\n",
       " 'Piping the errored records to a text file is very helpful [bang] ',\n",
       " '',\n",
       " 'The first column in my table is an auto-generated ID [comma] so I make sure that every line in my file starts with a tab [full_stop] This basically nulls the first column [comma] letting the database generate an ID as normal [full_stop] ',\n",
       " 'Also [comma] BCP can not parse dates [comma] times or datetimes from strings [full_stop] I havent found a nice way around this yet - in the end I changed the data type of the column because I dont really care about the date in my dataset very much anyway [bang] Others have said they created a temporary table and then select/inserted the data over to the real table with a date conversion [full_stop] ',\n",
       " 'Antennas [eh] Antennae [eh] Im not an expert on pluralisation [comma] but I know how to search the internet [full_stop] Seems English authors dont differentiate between metallic apparatus and sensory appendages [comma] so antennae it is [full_stop] ',\n",
       " 'Anyway [comma] a couple of months ago I carefully soldered together a couple of circular polarised FPV antennae [full_stop] I followed the guide on rcexplorer [full_stop] se [comma] carefully measured some 0 [full_stop] 8mm welding wire and fiddled for hours getting it soldered together [full_stop] The end result was a pair of incredibly fragile antennae in which I had very little confidence [full_stop] At 5 [full_stop] 8GHz the tolerances are incredibly small and even the smallest blob or errant solder can cause issues [comma] not to mention bashing into the ground and ruining everything [full_stop] ',\n",
       " '',\n",
       " 'The answer [eh] Buy something [bang] These fantastic plastic-potted circular polarised antennae are built to plug into Fat Shark goggles but can be made to fit the HobbyKing FPV stuff with a very cheap adapter that can be found on eBay for about 1 each (search for an SMA female to RP-SMA male adapter) [full_stop] ',\n",
       " 'HobbyKing just added a version that doesnt need the adapter too [bang] I found it while looking for a link for the previous paragraph [full_stop] Im not too fussed that I missed out though - the ones I got are great [bang] ',\n",
       " '',\n",
       " 'These little antennae are much smaller than I expected [full_stop] They are also very light and very tough [full_stop] The flexible but stiff cable allows them to be bent into the right position for your model too [full_stop] ',\n",
       " 'Cant wait to give these a try - when it stops raining [full_stop] ',\n",
       " 'So [comma] this isnt supposed to be the ultimate guide to AngularJS or anything like that - Im not even using the latest version - this is just some notes on my return to The World of the View Model after a couple of years away from WPF [full_stop] Yeah [comma] thats right [comma] I just said WPF while talking about Javascript development [full_stop] They may be different technologies from different eras: one may be the last hurrah of bloated fat-client development and the other may be the latest and greatest addition to the achingly-cool [comma] tie dyed hemp tool belt of the Single Page App hipster [comma] but under the hood theyre very very similar [full_stop] Put that in your e-pipe and vape it [comma] designer-bearded UX developers [bang] ',\n",
       " '',\n",
       " 'Anyway [comma] when I started [comma] I knew nothing about SPA development [full_stop] Id last done JavaScript several years ago and never really used it as a real language [full_stop] I still contend that JavaScript isnt a real language (give me Scala or C# any day of the week) but you cant ignore the fact that this is how user interfaces are developed these days so [comma] yeah [comma] I started with a tutorial on YouTube [full_stop] ',\n",
       " 'I decided to do an old radiators are coded in WPF [comma] which looks awesome on the big TVs dotted around the office [comma] but doesnt translate well for remote workers [full_stop] ',\n",
       " 'There is no sunshine and there are no rainbows in this article [full_stop] I found javascript to be a hateful language [comma] filled with boilerplate and confusion [full_stop] Likewise [comma] though TeamCity is doubtless the best enterprise CI platform on planet earth [comma] the REST APIs are pretty painful to consume [full_stop] With that in mind [comma] lets get into the weeds and see how this thing works',\n",
       " 'You cant hit a server from a web page unless that server is the server that served the web page youre hitting the server with unless of course you tell the server you want to hit that the web page you want to hit it with [comma] served from a different server [comma] is allowed to hit it [full_stop] Got that [eh] Thought so [full_stop] This is all because of a really logical thing called Cross Origin Resource Sharing [comma] which you can enable pretty easily in TeamCity as long as you have admin permissions [full_stop] ',\n",
       " 'Check out Administration -> Server Administration -> Diagnostics -> Internal Properties [full_stop] From there you should be able to edit [comma] or at least get the location of the internal [full_stop] properties file [full_stop] Weirdly [comma] if the file doesnt exist [comma] there is no option to edit [comma] so you have to go and create the file [full_stop] Since my TeamCity server is running on a Windows box [comma] I created the new file here:',\n",
       " '',\n",
       " 'and added the following:',\n",
       " '',\n",
       " 'You might want to be a little more selective on who you allow to access the server this way - I guess it depends on how secure your network is [comma] how many clients access the dashboard and so on [full_stop] ',\n",
       " 'This article is about AngularJS and its about TeamCity [full_stop] Its not about NPM or Bower or any of that nonsense [full_stop] Im not going to minify my code or use to crazy new-fangled pseudo-cosmic CSS [full_stop] So setting up the build environment for me was pretty easy: create a folder [comma] add a file called index [full_stop] html [comma] fire up the fantastic Fenix Web Server and configure it to serve up the folder we just created [full_stop] Awesome [full_stop] ',\n",
       " 'If youre already confused [comma] or if you just want to play with the code [comma] you can download the lot from GitHib: ',\n",
       " 'Hopefully youve watched the video I linked above [comma] so you know the basics of an AngularJS app [full_stop] If not [comma] do so now [full_stop] Then maybe Google around the subject of promises and http requests in AngularJS [full_stop] Done that [eh] OK [comma] good [full_stop] ',\n",
       " 'Web requests take a while to run [full_stop] In a normal app you might fetch them on another thread but not in JavaScript [full_stop] JavaScript is all about callbacks [full_stop] A Promise is basically a callback that promises to get called some time in the future [full_stop] They are actually pretty cool [comma] and they form the spinal column of the build status app [full_stop] This is because the TeamCity API is so annoying [full_stop] Let me explain why [full_stop] In order to find out the status (OK or broken) and state (running [comma] finished) of each build configuration you need to make roughly six trillion HTTP requests as follows:',\n",
       " 'Heres how that looks in code [full_stop] Hopefully not too much more complicated than above [bang] ',\n",
       " 'Most of the REST access has been squirrelled away into a factory [full_stop] And yes [comma] our build server *is *called tc and guest access *is *allowed to the REST APIs and I *have *enabled CORS too because sometimes productivity is more important than security [bang] ',\n",
       " 'We have over 100 builds [full_stop] Good teams have lots of builds [full_stop] Not too many [comma] just lots [full_stop] Every product (basically every team) has CI builds [comma] release/packaging builds [comma] continuous deployment builds [comma] continuous test builds [comma] metrics builds we have a lot of builds [full_stop] Builds are good [full_stop] ',\n",
       " 'But a screen with 100+ builds on it means very little [full_stop] This is an information radiator [comma] not a formal report [full_stop] So [comma] I use a simple (but messy) algorithm to convert a big list of Builds into a smaller list of Tiles:',\n",
       " '',\n",
       " 'Not much very exciting here [full_stop] I used derivative of Bootstrap to make the UI look nice [full_stop] I bound some content to the View Model and thats about it [full_stop] Download the code and have a look if you like [full_stop] ',\n",
       " 'Heres my index [full_stop] html (which shows all the libraries I used):',\n",
       " 'Heres the view HTML for the list (in templates/list [full_stop] html) [full_stop] I love the Angular way of specifying Views and Controllers by the way [full_stop] Note the cool animated CSS for the in progress icon [full_stop] ',\n",
       " 'I think I summarized how I feel about this project in the introduction [full_stop] It looks cool and the MVC MVVM ViewModel vibe is a good one [full_stop] The data binding is simple and works very well [full_stop] All my gripes are with JavaScript as a language really [full_stop] I want Linq-style methods and I want classes and objects with sensible scope [full_stop] I want less syntactic nonsense [comma] maybe the odd => every now and again [full_stop] I think some or all of that is possible with libraries and new language specs but I want it without any effort [bang] ',\n",
       " 'One thing I will say: that whole page is less than 300 lines of code [full_stop] Thats pretty darned cool [full_stop] ',\n",
       " 'Feel free to download and use the app however you like - just bung in a link to this page [bang] ',\n",
       " '',\n",
       " 'Earlier this year [comma] Sam and I (no relation [comma] despite the constant references to our marriage) did a talk at the Data Science Festival about Trainlines awesome Data Products [full_stop] ',\n",
       " 'Went away for a couple of days last weekend to Blaenavon in South Wales [full_stop] Had a brilliant time and managed to fit in a bit of green laning in West Berkshire on the way there and back [full_stop] Heres some pictures and theres a video too [full_stop] ',\n",
       " 'For some reason [comma] I have started playing with this snippet on bl [full_stop] ocks [full_stop] org but I made a few subtle changes to the code and the visual style [full_stop] ',\n",
       " 'I dont take credit for much here - I just wanted to record that Id spent a couple of train journeys building something cool [bang] ',\n",
       " 'Kanban is a great way to manage your bug backlog [full_stop] Its much better than Scrum simply because of the nature of bugs as compared to user stories [full_stop] Scrum is all about making firm commitments based on estimates but bugs are very hard to estimate up-front [full_stop] Generally when youve looked hard enough into the code to find the problem [comma] you are in a position to fix it very quickly [full_stop] Bug fixing is essentially a research task - like a spike - so time-boxing the work makes much more sense [full_stop] ',\n",
       " 'Set up a prioritised backlog and blast off the top as many bugs as possible in the time youve set aside - Kanban Style [full_stop] This works very well but [comma] as with most agile approaches [comma] it leaves old fashioned managers a bit grumpy [full_stop] They want to track your productivity and its fair to say that you should too because thats how you spot impediments (plus its always good to show off) [full_stop] ',\n",
       " 'Scrum-style burn downs dont work with Kanban because they track progress against some committed target [full_stop] The answer is the Cumulative Flow Diagram:',\n",
       " '',\n",
       " 'So I did some tweaking to my Information Radiator to add a page showing the CFD for the last 60 days of one of our projects [full_stop] The data comes out of TFS via the C# API and a WIQL query - which has a very nice historical query feature which Ill explain below [full_stop] ',\n",
       " 'Cumulative flow diagrams couldnt be simpler [full_stop] Like a burn-up chart they show a running total of the bugs fixed over time [full_stop] Since bugs arent estimated [comma] the Y axis shows the bug count [full_stop] In the chart above the X axis is in days but I guess you could do weeks or even hours if you like [full_stop] In addition to the fixed bugs series [comma] there are also stacked series for other states: committed [comma] in development and in QA [full_stop] ',\n",
       " 'The benefit of showing the other issue states is that it gives you a readout on how the process is working [full_stop] The QA and development series should generally be the same thickness [full_stop] If the QA area gets fatter than the development area then you have a bottleneck in QA [full_stop] If the development series gets too fat then youre spread too thinly - you have an impediment in development or need to think about your Kanban limit [full_stop] ',\n",
       " 'Note how there are a couple of steps on the left of my graph [full_stop] Those correspond to the first couple of sprints in which we used TFS [full_stop] The team werent familiar with it [comma] so work item states were generally changed at the end of the sprint [full_stop] As time went on we got better at updating the system and the steps turned into a nice looking slope [full_stop] ',\n",
       " 'Its not every day that I openly applaud Microsoft for doing something brilliant and until now Ive never been that cheerful about TFS [full_stop] But the historical querying in WIQL (work item query language) is bloody brilliant [bang] ',\n",
       " 'Drawing a CFD chart depends on an ability to get the historical state of any issue in the system at a specified point in time [full_stop] In WIQL this is done using the AsOf keyword:',\n",
       " '[sourcecode] Select [ID] [comma] [Title] [comma] [Effort - Microsoft Visual Studio Scrum 2_0] [comma] [Assigned To] From WorkItems Where [Team Project] = Project And [Work Item Type] = Bug And [Iteration Path] under Project\\\\Release AsOf 21/01/2013 [/sourcecode]',\n",
       " 'So the algorithm for drawing the CFD is pretty simple:',\n",
       " 'Well this post isnt really about movies [comma] its more about how I spent the last week or so brushing up my data manipulation skills using python [comma] matplotlib [comma] numpy [comma] pandas [comma] seaborn and the fantastic Google Colaboratory [full_stop] But I did learn some interesting things about Stan Lee in the process',\n",
       " 'Google Colaboratory is a python notebook within Google Docs [full_stop] So it has all the features youd expect from a vanilla notebook like jupyter (on which I think it is based) but you also get the online [comma] shared editing [comma] live update experience youd get with Google Sheets and the like [full_stop] When you fire up the notebook [comma] its attached to a blank debian instance [comma] running somewhere in the cloud [full_stop] You can install all the tools and libraries you like [comma] then jump straight into python code [full_stop] Review: 5 stars [bang] Would recommend [full_stop] ',\n",
       " 'You can find all the code and charts for this article on this shared colab notebook [full_stop] ',\n",
       " 'The movie data I downloaded is linked to a kaggle challenge to predict profitability [full_stop] I started off with some basic investigation of the covariance and correlation of various fields [full_stop] I soon got bored though - theres not really any field in the dataset that helps predict the profit a movie makes [full_stop] I guess thats why its a challenge but I was here to play with charts [comma] not machine learning [full_stop] ',\n",
       " 'So [comma] I started to look at the cast:',\n",
       " 'Samuel L Jackson appears in the highest number of movies in the dataset [comma] followed by a constellation of embarrassingly male stars [full_stop] ',\n",
       " 'Being in a large number of films doesnt imply youre a great actor [comma] or a sure fire sign of sky-high profits [full_stop] Each actor has their own unique spread of profits from the various films theyve been in [full_stop] Below I compare three actors with the power of histograms [full_stop] ',\n",
       " 'This chart is a bit busy [comma] but having categorised the movies by the amount of profit they made [comma] the bars show the proportion of movies in that category for a given actor [full_stop] Simply put [comma] having bigger bars on the right hand side is good [full_stop] SLJ has a healthy spread [comma] way over a billion dollars in some cases [full_stop] Nick Cage is doing the worst of the three [comma] with no movies over the half-billion mark and a big spike right on zero [full_stop] ',\n",
       " 'Lets look at the total profit made by films each actor has been in',\n",
       " 'Two key things to note here - first [comma] and thanks almost entirely to the Lord of the Rings trilogy [comma] there is finally a woman on the list [full_stop] Secondly [comma] Stan Lee shot right to the top [comma] hes appeared in films totalling an unbelievable $13billion [full_stop] Thats the GDP of Albania [bang] ',\n",
       " 'Looking at the profit distribution of Stan Lee films vs all films its clear that hes a sure sign of a large payday [full_stop] Hes never appeared in a film that made a loss and hes generally to be found in the half-billion zone [full_stop] For context [comma] compare this to your average film [comma] which is more than likely going to make just a few tens of millions on average [full_stop] ',\n",
       " 'Stan normally crops up in Super Hero movies [full_stop] Specifically movies based on comics [comma] I guess [full_stop] Its hard to find these in the dataset [comma] they are categorised as Action [comma] Sci-Fi and so on; theres no specific genre for heroes [full_stop] However [comma] in the keyword field there are movies tagged Marvel Comic which helps us [full_stop] Pulling these out [comma] we find that Stan doesnt appear in every one',\n",
       " 'So two key learnings here: Marvel Comic movies consistently make money and those which Stan Lee chooses to appear in make around a billion more on average [bang] ',\n",
       " 'Im not really sure what I set out to prove here [full_stop] But I guess what I did was',\n",
       " 'Maybe four things is enough for any blog post [eh] :)',\n",
       " 'Several years ago we finished the northern section of the South West Coast Path [full_stop] This week a smaller group of us went back to take on the next section - Lands End to The Lizard [full_stop] ',\n",
       " 'We managed 16 strenuous miles on day one [comma] 18 moderate miles on day two and then cut day three short to 8 more moderate miles [full_stop] We didnt make the Lizard [comma] but we had fun trying [bang] The landscape was beautiful but the weather was cruel to say the least [full_stop] Snack breaks on day three were bitterly cold with a constant freezing wind blowing head-on as we walked [full_stop] Cant complain too much though as the rest of the UK was hit by unseasonal snowstorms [full_stop] ',\n",
       " 'We witnessed a real life air-sea rescue on day two [full_stop] An RAF Sea King turned up to winch a fisherman from the freezing sea after hed been swept off the rocks [full_stop] We were convinced they were pulling out a corpse but next day the local radio confirmed he had miraculously survived the 10-foot swells [comma] jagged rocks and freezing water [full_stop] Watching the rescue unfold from the coast path left us in awe but also a little shaken [full_stop] Coastal erosion was in full swing and we often found ourselves on detours around huge new cracks in the ground or navigating slippery wet rocks above certain-death falls [full_stop] Maybe these things are best done in the summer [bang] ',\n",
       " 'That said [comma] it was fantastic to notch up another 42 miles of stunning coastline [full_stop] Hopefully next year will see us back in Cornwall for the next section of walking and more warm pub parlours [comma] good beer and fresh fish suppers [full_stop] ',\n",
       " '',\n",
       " 'I just completed my second ever air engine: The Chuffer [full_stop] With this project I decided that Id go for complexity [comma] rather than the simplistic approach I took when I built The Wobbler earlier this year [full_stop] [vine url= width=400 height=400] The Chuffer is a twin-cylinder double acting engine - which means each of its two cylinders both push and pull the piston [full_stop] This means it operates much more smoothly and can self-start as theres always a piston on a power stroke [comma] regardless of where the flywheel is [full_stop] [vine url= width=400 height=400] Unlike its predecessor [comma] the Chuffer has fixed cylinders and airflow is controlled by two spool valves [comma] operated by eccentrics next to the flywheel via some pushrods and a hopelessly over-engineered lever mechanism [full_stop] [vine url= width=400 height=400] And of course everything was made by yours truly [comma] in the garage [comma] from chunks of brass [comma] steel and aluminium bar and flat stock I bought on eBay [full_stop] Needless to say [comma] I have learned a lot since I started the project this February [bang] ',\n",
       " 'Whats next [eh] Well [comma] maybe a generator',\n",
       " 'We recently added a new build status indicator in the office [comma] using an excellent TP-Link LB130 Smart Bulb [full_stop] Though these bulbs are pretty expensive [comma] they are super-simple to set up and control via a simple REST API [full_stop] ',\n",
       " 'It took quite a lot of googling to find the correct commands to send through the API to control the bulb [comma] but once wed found the answer [comma] it was incredibly simple [full_stop] [video loop=on width=1920 height=1080 mp4=',\n",
       " 'There are many Python libraries out there for controlling these bulbs directly via the local network [comma] but the benefit of using the REST API is that you can control the bulb from anywhere [full_stop] You are also able to discover the bulbs associated with your TP-Link Kasa account [comma] so you dont need to know the IP address or MAC of your bulbs [full_stop] ',\n",
       " 'Heres the code',\n",
       " 'So [comma] Ive decided its about time I reinvent Logical Genetics - again [bang] The sites been around for over a decade now [full_stop] I remember sitting in my freezing attic room [comma] the year after I graduated [comma] embarking on my PhD studies and trying to think of a cool name for a website to document my adventures in artificial intelligence [full_stop] ',\n",
       " 'As time went on I did some blogging and knocked up some articles [full_stop] Thousands of posts were written on the forums by me and a group of like-minded friends [full_stop] I presented at conferences and user groups [comma] published papers and never once failed to promote the site [full_stop] I still get emails from lazy students asking for source code [bang] ',\n",
       " 'Since then [comma] much has changed [full_stop] I never finished my doctorate because to be honest [comma] Im an engineer not an academic [full_stop] Im on my third job and have managed to pick up a wife and a daughter too [full_stop] I spend more time blogging about Land Rovers and tricopters than I do about genetic algorithms [full_stop] The invention of Facebook and Twitter killed off the social side of Logical Genetics and my job ate my spare time [full_stop] Years went by with no activity [full_stop] ',\n",
       " 'Recently though [comma] Ive switched jobs and found time for some interesting hobbies [full_stop] Ive fallen in love with the Raspberry Pi and want to share my experiences using it with C# and mono because Im sick of reading peoples skewed opinions on the subject [full_stop] I also want somewhere to post about my continuous tricopter tinkerings and rant on the subject of software development and agile [full_stop] Plus I still knock up the odd evolutionary algorithm every now and again [full_stop] ',\n",
       " 'So the new-look Logical Genetics has a broader scope and is a bit more personal [full_stop] The old forums are still linked off the front page and Ive kept some of the old articles too [full_stop] Letss see where things go from here',\n",
       " 'I watched the BBC weather report with a growing sense of foreboding this morning [full_stop] A huge band of rain sweeping inexorably towards Newbury from the south coast [comma] set to smother the whole of Berkshire under a blanket of liquid misery until Saturday lunchtime [full_stop] Nothing unusual [comma] but particularly inopportune as I had a completed carbon copter and a suitcase full of batteries in the back seat of the car [full_stop] ',\n",
       " 'Luckily [comma] the rain held off and Dr Johnson and I met up on the green to fly some multi-rotors in the blustery wind [full_stop] The Carbon-Copter performed stunningly',\n",
       " '',\n",
       " 'Make sure you stick with the video (or hit fast-forward) as theres some cool shots of the FireGoat from the air towards the end [full_stop] ',\n",
       " 'A tiny bit of transmitter tinkering was required: un-reversing the rudder direction [comma] lowering the expo on the pitch and roll axes and so on [full_stop] After that it was just a case of whacking the throttle up and grinning like a fool as the quad danced around the skies [full_stop] ',\n",
       " 'In the end I got too cocky - of course [full_stop] Started with some backwards and forwards stuff [comma] then progressed to attempting some lazy 8s at increasing speeds [full_stop] Eventually I lost it and hit the ground [full_stop] Initially I thought everything was OK with the frame but on closer inspection I found that both legs and one of the front-back braces was cracked [full_stop] ',\n",
       " 'Amazingly though [comma] it still flew [full_stop] There was a noticeable twist in the frame as I lifted off [comma] but it continued to fly brilliantly [full_stop] Of course [comma] now that the legs were broken and my money effectively spent I had nothing to lose - the lazy eights became less and less lazy until they became simply eights then vigorous eights and finally broken quad eights [full_stop] ',\n",
       " 'Hit the floor pretty hard and the carbon rods shattered into a thousand useless shards [full_stop] Time to go home and make some more [bang] The carbon sheet remains intact [comma] so the repair will just involve cutting and drilling some new legs and braces [full_stop] 5 total cost [comma] which isnt a million miles from the cost of fixing the aluminium copter after similar crashes [full_stop] ',\n",
       " 'So I had fun and I perhaps went a bit too far with the aerobatics [full_stop] The carbon quad is an unmitigated success and I have a big smile on my face [full_stop] Its also worth remembering that I built the carbon quad for slow moving videography and FPV [comma] so maybe mental eights wont be happening again too soon [bang] ',\n",
       " 'Every car in the UK [comma] once itss three years old [comma] need to have an MOT test annually to prove its safe to drive [full_stop] The good people at the DVLA have made a large chunk of the data available as part of the governments push to make more data open [full_stop] You can all the code for this article in my GitHub [full_stop] ',\n",
       " 'Before I started doing any machine learning [comma] I did some basic visualisation of the data in a series of charts [comma] just to get an idea of the shape of things [full_stop] I used Spark to process the data (theres lots of it) and D3js to create some charts [full_stop] I havent been able to make the charts work in Wordpress yet but you can see them below as screenshots of elsewhere as a live document [full_stop] ',\n",
       " 'The data arrives in CSV format [comma] which is very easy to digest but pretty slow when youre dealing with tens of millions of rows [full_stop] So the first thing I did was to transform the data to Parquet using Sparks built in Parquet capabilities [full_stop] This improved query performance massively [full_stop] ',\n",
       " 'First thing to look at: how many tests are carried out on vehicles of a given age [eh] Basically [comma] how many 3-year-old [comma] 4-year-old [comma] 20-year-old cars are on the road [full_stop] The dataset contains records for MOTs on cars well over 100 years old [comma] but there arent many of them [full_stop] ',\n",
       " '',\n",
       " 'As you can see from the histogram [comma] most tests are carried out on cars between 3 and 15ish years old [full_stop] ',\n",
       " '',\n",
       " 'The accompanying CDF shows that the 95% percentile is roughly around the 15 year mark [full_stop] Lets zoom in a bit',\n",
       " 'The zoomed-in histogram makes the 10-15 year shelf life of most cars pretty apparent [full_stop] ',\n",
       " 'Are people throwing away their older cars because theyre uncool or because they are broken [eh] ',\n",
       " '',\n",
       " 'A look at the pass rate over time shows that its probably because theyre broken [full_stop] The pass rate starts off pretty high - well over 85% [comma] but dips top an all time low at 14 years of age [full_stop] ',\n",
       " '',\n",
       " 'Once cars get past the 14 year death zone their prospects get better though [full_stop] As cars get older and older the first-test pass rate heads back up towards 100% [full_stop] At around 60 years old [comma] cars have a better chance of passing their MOT than when theyre brand new [bang] ',\n",
       " 'I guess its safe to assume that cars over 30 years of age are treated with a little more respect [full_stop] Theyre classics after all [full_stop] Once a car is 80+ years old it probably lives in a museum or private collection and drives very little throughout the year [full_stop] The MOT test is much easier for older cars too - a 100 year old car does not have to pass emissions [bang] ',\n",
       " 'The pass rate changes differently as cars from different manufacturers get older [full_stop] Some manufacturers make disposable cars [comma] some make cars designed to be classics the day they leave the showroom (Aston Martin [comma] Lotus [comma] Rolls Royce) [full_stop] Some make cheap cars that people care less about (Vauxhall [comma] Ford) [comma] some make posh cars people take care of (Audi [comma] BMW) [full_stop] Japanese manufacturers seem to be able to build cars with very steady pass rates over time [full_stop] ',\n",
       " '',\n",
       " 'It might not be a shock that Bentley [comma] Porche are at the top here [comma] with TVR close behind [full_stop] For me the biggest surprise was that Ford takes the deepest dip at the 14 year mark [full_stop] Fords are clearly not built to last or maybe people dont care for them [full_stop] Renault and Alpha Romeo join Ford at the bottom of the table here [full_stop] ',\n",
       " 'Its all very well to be mean to Ford about their poor longevity [comma] but they do have more cars on the road that pretty much anyone else [full_stop] Check out the heatmap: ',\n",
       " 'While were counting cars [comma] it looks like silver is the most popular colour [full_stop] The MOT test data runs out in 2013 [comma] so Id expect to see a lot more white cars these days [full_stop] ',\n",
       " 'OK [comma] so weve looked at some charts not lets look at some code [full_stop] All the charts about were generated by simple Spark dataframe apps [comma] wrapped up in a unit test harness for ease of use [full_stop] Heres an example:',\n",
       " 'Not sure what else there is to say about the code [full_stop] Have a read or hit my github if you want to play around with it [bang] ',\n",
       " 'Sparks MLlib codes with all sorts of machine learning algorithms for predicting and classifying (mainly the latter) data [full_stop] I looked at decision trees [comma] random forests and neural networks for this [full_stop] The idea was to turn some properties of a vehicle such as age [comma] mileage [comma] manufacturer [comma] model [comma] fuel type and so on into a pass/fail prediction [full_stop] ',\n",
       " 'It didnt work [bang] Yes [comma] sorry [comma] thats right [comma] its not really possible to predict a straight pass or fail [full_stop] Even in the worst case [comma] the first-test pass rate for all different classes of car is over 50% [full_stop] Machine learning techniques being as they are [comma] this means that the simplest solution for any predictive model is simply to predict a pass every time [full_stop] ',\n",
       " 'This happened with all three techniques - neural nets [comma] decision trees and random forests all learned to predict a pass every time [comma] giving them a 50-60-ish% accuracy [full_stop] Darn it [bang] ',\n",
       " 'So [comma] if you cant predict two classes (PASS and FAIL) maybe its easier to predict Pass Probability Classes (50-60% [comma] 60-70% [comma] 70-80% etc) [full_stop] Well [comma] yes it was slightly more successful [comma] but not exactly stunning [bang] ',\n",
       " 'The best results I got were predicting 10 pass rate classes for each decile of probability [full_stop] This gave me these rather lame results:',\n",
       " 'Mean Error: 1 [full_stop] 1532896239958372 Precision: 0 [full_stop] 3961880457753499',\n",
       " 'So the mean error is greater than 1 - i [full_stop] e [full_stop] most test data entries are classified over one class away from their true class [full_stop] The precision shows only 40% of samples being predicted correctly [full_stop] Pants [full_stop] ',\n",
       " 'The confusion matrix tells a slightly more positive story though - here it is rendered as a colour map:',\n",
       " '',\n",
       " 'The confusion matrix shows the class predicted by the model (column) versus the actual class of the sample (row) [full_stop] A perfect predictor would give a diagonal green line from top left to bottom right [comma] showing every class predicted correctly [full_stop] ',\n",
       " 'In this case [comma] the random forest is attempting to predict the banded pass rate (0: 0% - 10% [comma] 1: 10 - 20% [comma] 2: 20 - 30% [comma] 9: 90% - 100%) [full_stop] Since virtually no classes of vehicle exist where the pass rate is less than 40% it doesnt do very well at those levels [comma] however [comma] from 40% to 80% it does pretty well [full_stop] ',\n",
       " 'The code is complex - Spark makes it easy to run machine learning algorithms [comma] but theres a lot of bits and bobs round the edges like UDFs and utility functions [full_stop] The following listing is the algorithm which gave me the results above (my best attempt) [full_stop] Hit the githib link at the top of this page if you want to dig further into the code [full_stop] ',\n",
       " 'I think I proved that MLlib and Spark are a great choice for writing machine learning algorithms very quickly and with very little knowledge [full_stop] ',\n",
       " 'I think I also proved that Data Scientists need to know a hell of a lot more than how to fire up a random forest [full_stop] I know a little bit about data and machine learning (thus the name of this website [bang] ) but in order to make much sense of a dataset like this you need a whole arsenal of tricks up your sleeve [full_stop] ',\n",
       " 'As usual [comma] D3 [full_stop] js and AngularJS are great [full_stop] ',\n",
       " 'This is part two of a mini series [full_stop] You can find part one here: Face Clustering with Python [full_stop] ',\n",
       " 'I coded my first neural network in 1998 or so literally last century [full_stop] I published my first paper on the subject in 2002 in a proper peer-reviewed publication and got a free trip to Hawaii for my troubles [full_stop] Then [comma] a few years later [comma] after a couple more papers [comma] I gave up my doctorate and went to work in industry [full_stop] Maybe Im more engineer than scientist [full_stop] Maybe I prefer building stuff than theorising about stuff [full_stop] Maybe anyway',\n",
       " 'Neural Networks back then were small and they took a while to train effectively [full_stop] Some people (even me [bang] ) built some cool stuff with them [comma] but their use-cases were limited [full_stop] The idea that you could use them to recognise a person was [comma] frankly [comma] laughable back then [full_stop] Sixteen years later we have The Cloud and compute resource is as cheap as well chips [full_stop] Things have changed [bang] ',\n",
       " 'In the last article in this series I looked into an algorithmic approach to clustering/grouping faces based on various feature measurements: size of nose [comma] distance between eyes etc [full_stop] I had some very minor success [comma] but in the end I fell foul of facial expressions [full_stop] Turns out [comma] people change the size and position of their facial features to communicate and express emotion [full_stop] Who knew [bang]  [eh] ',\n",
       " 'Where I went wrong [comma] with my brute force approach [comma] was in the way I generated an embedding from my face images [full_stop] Transforming source bitmap data into a numeric vector simply by measuring parts of the face is no good if people smile [comma] or frown [comma] or move [full_stop] ',\n",
       " 'As with all things [comma] people have already solved this problem [full_stop] They did this using all the CV enhancing buzz-words in the world Neural Networks and Deep Learning in the Cloud [full_stop] Back in my youth [comma] the idea that youd build a neural net with thousands of inputs and 20-odd layers of several hundred nodes was just bonkers - but these days its fine [full_stop] The term Deep Learning is simply a nod to this new freedom to train massive networks relatively cheaply [full_stop] ',\n",
       " 'A host of industry and academic experts have been training neural networks to recognise faces for some years [full_stop] The really clever bit for me is that they arent directly training the neural networks to recognise specific people - they are training them to create better embeddings [full_stop] ',\n",
       " 'It turns out that there are several [comma] freely available [comma] pre-trained neural nets available for download which have been taught to output distinct embeddings for any given face [full_stop] So the array of values these networks output given a picture of me [comma] will be different to the vector they produce for my wife [comma] or my daughters [full_stop] Two photos of my eldest will produce embeddings more similar to each other than to those from a photo of my wife [full_stop] ',\n",
       " 'Yet again [comma] using Python and the work of chap called face_recognition library wraps a pre-trained face recognition network [full_stop] Heres the sum total of the code I had to write to use it:',\n",
       " 'I wrapped the strategy for creating an embedding in a class so I could swap out the Old School strategy from the previous article [full_stop] The code loads the image [comma] gets the embeddings and returns them wrapped up with the filename [full_stop] Thats it [full_stop] Easy [bang] ',\n",
       " 'Well [comma] that worked a bit better than last time [comma] didnt it [bang] There is literally only one error visible in that image: the appearance of a stranger on the last row with yours truly [full_stop] ',\n",
       " 'The code I wrote to find faces in our holiday photos simply trawls through each image and grabs any face it finds [full_stop] Inevitably some unknown folks were caught in the background of some of the pictures we took [full_stop] Basically [comma] my assumption that there would be four clusters in the input data was wrong - and this is supported by the k-means cost function:',\n",
       " 'So [comma] I changed to k=5 to see what happened [full_stop] The results were initially a bit confusing [full_stop] The addition of a 5th group didnt separate the Strangers from the Taylors - or at least not in the way I expected:',\n",
       " 'The first four clusters look OK [full_stop] Theres a row for my wife [comma] one each for my two lovely daughters and one for me but then on the last row theres a mixture of strangers and my wife and daughter in sun glasses [bang] Thats not what Id expected at all [bang] ',\n",
       " 'So is k=5 worse than k=4 [eh] Did the cluster of strangers plan fail [eh] Well [comma] that depends [bang] ',\n",
       " 'With a lot of machine learning algorithms [comma] you need to think about whether you care more about False Positives or False Negatives [full_stop] Depending on the use case [comma] you might need to say I am certain that this is a photo of Dan or alternatively these are all the photos I think contain Dan [full_stop] ',\n",
       " 'My clustering attempt with k=5 lowered my false positive rate [comma] increasing my recall [full_stop] That is [comma] the first four clusters were now almost 100% correct: the rate of false positives was low [full_stop] The 5th cluster [comma] however [comma] contained a mixture of strangers (true negatives - a good thing) and family members (false negatives - a bad thing) [full_stop] If I wanted to be sure that every photo is who I think it is [comma] I might have been happy with this [full_stop] ',\n",
       " 'In the k=4 example [comma] the recall was higher - there were more photos of each of us in our respective clusters [comma] but the number of false positives was also higher - the precision was lower [full_stop] If I was searching CCTV for potential sightings of a master criminal [comma] I might well opt for this alternative as it increases the chances of finding who Im looking for and I could easily filter the false negatives manually [full_stop] ',\n",
       " 'Where k=5 there is in fact only one false positive in the whole dataset - this guy here [comma] who my algorithm thinks is me [full_stop] ',\n",
       " 'So why didnt K-Means make a cluster of strangers as we expected it to [eh] The answer to this is not that it was unable to differentiate between the Strangers and the Taylors - it actually comes back to the similarity theory that started this whole journey [full_stop] ',\n",
       " 'Simply put [comma] the male stranger looks more like me than he does the female strangers [full_stop] In the picture above [comma] Ive sketched out how I see this happening (projected down to 2D space [bang] ) [full_stop] The male stranger is clearly different to me - hes a fair distance away from the points which correspond to the photos of my face [full_stop] Likewise [comma] the female strangers are separated from the photos of Lorna (who is my wife [comma] by the way) but they are closer to the photos of her in sunglasses [full_stop] When K-Means tries to group these points together [comma] the male stranger and I share a group [comma] and the female strangers go with the bespectacled Lorna [full_stop] ',\n",
       " 'K-Means is not very good at dealing with outliers [bang] Also [comma] it is a very naive clustering algorithm - its not learning anything about my family [comma] its just clustering similar points [full_stop] ',\n",
       " 'Given that this all started with a theory on similarity [comma] before I close out this post [comma] I thought I should at least share the results of the Neural Clustering on the Unfamiliar Faces [full_stop] Im going to present them without comment - you can make your own mind up on which one is better and whether either does a good job',\n",
       " 'Clustered by naive feature measurements (Old School)',\n",
       " 'Remember [comma] the original challenge was to group similar looking people on each row',\n",
       " 'So when it comes to recognising family members and clustering them together [comma] I think Ive proved that neural network generated embeddings are the way to go [full_stop] Who knows what features of our faces they are looking at to create the vector of weird numbers [comma] but they are undoubtedly coping better with the facial expressions and differing camera angles than the direct measurement approach [full_stop] ',\n",
       " 'The downside of neural networks is exactly this uncertainty [full_stop] How did they generate those embeddings [eh] How do they rank nose size versus eye separation in terms of importance [eh] In a world where we need to explain why the faces are grouped the way they are [comma] neural nets are not so great [full_stop] But realistically [comma] when would that be important in real life [eh] ',\n",
       " 'Thanks for sticking around to the end of Part 2 [full_stop] I hope you enjoyed it as much as I did [bang] ',\n",
       " 'A few weeks ago I made a smoker out of an exhaust pipe and two old party-balloon helium cylinders [full_stop] Since then I have smoked many a brisket [bang] Check out the pics [full_stop] ',\n",
       " '',\n",
       " 'So its been a tradition in my office to use Slack to gauge the teams mood once a week [full_stop] Previously our PM would post a message asking for feedback and people would add a reaction to show how they were feeling [full_stop] This worked fine [comma] though there were a couple of issues: firstly it was pretty hard to interpret the weird collection of party parrots and doges [comma] and secondly people tend to follow the herd when they can see how others have reacted [full_stop] ',\n",
       " 'Heres my idea for the new [comma] automated workflow',\n",
       " 'By far the cheapest and maybe the simplest way to host all of the code to do this is to go serverless using many of the cool features available on AWS to host code [comma] databases and APIs on a pay-per-use basis [full_stop] Heret the technical architecture',\n",
       " 'Based on the above there are three broad areas for development: send the webhook to Slack; deal with responses when users click the buttons and serve up a chart showing the results for the week [full_stop] ',\n",
       " 'Slack allows you to post Interactive Messages using an Incoming Webhook [full_stop] In order to do this youll need to add a new slack bot integration using their very friendly web UI [full_stop] I called mine MoodBot [full_stop] Once you have a bot set up [comma] you need to enable Incoming Webhooks and add the target URL to an environment variable (see here for more details) [full_stop] ',\n",
       " 'The format of the message you send needs to be something like the following [full_stop] Note that the interactive part of the message is included as an attachment [full_stop] ',\n",
       " 'This gives you a slack message looking like this:',\n",
       " 'The webhook is sent by a Lambda function [comma] which is triggered crontab-style by a CloudWatch event rule [full_stop] The Lambda looks like this:',\n",
       " 'Setting up the rule to trigger the event is pretty simple [full_stop] Log into the AWS console [comma] select CloudWatch and choose Events -> Rules from the menu on the left [full_stop] You can specify when the rule will run using a crontab line [full_stop] I used',\n",
       " 'Which will run at 9am (GMT) every Wednesday [full_stop] All this is set up via a reasonably clunky web interface [bang] ',\n",
       " 'This is the most complicated bit (and theres an extra tricky bit to deal with too) [full_stop] To handle the responses when users click buttons on the interactive Slack message you need four things: 1 [full_stop] A lambda function to handle the POST request and push data to a database [comma] 2 [full_stop] an API Gateway resource to provide an HTTP end-point [comma] translate the request and forward it to the Lambda function [comma] 3 [full_stop] a database to store the data and finally 4 [full_stop] a config setting in Slack to tell it where to send the POST [full_stop] ',\n",
       " 'Heres the code for my Lambda function [full_stop] Its simple enough - it just takes the JSON in the incoming request [comma] grabs the bits it wants and adds a few dates and times to create another JSON object to post to DynamoDB [full_stop] The response sent back to slack is a replacement message [comma] which will overwrite the one already in the channel [full_stop] Here I add a list of users who have clicked so far (a better man would have pulled this list from the DB [bang] ) [full_stop] ',\n",
       " 'Setting up the API Gateway should be simple enough - you add a new API then a new resource then a new POST method [full_stop] Then configure the method to forward requests to the Lambda function you just created [full_stop] However [comma] there are a couple of issues [full_stop] ',\n",
       " 'Firstly [comma] you need to enable cross site access (CORS) which is easy enough - you just select Enable CORS from the Actions dropdown [full_stop] This will open your method up to calls from other sites [full_stop] ',\n",
       " 'The second and far more infuriating issue is that Slacks Interactive Buttons send the data in a funky way [comma] encoding it weirdly in the message body rather than just posting JSON as all the other calls do [full_stop] After a couple of days of intermittent head-scratching I finally found this Gist [comma] which contains the code to fix the problem:',\n",
       " '',\n",
       " 'This code needs to be placed into a Body Mapping Template for your POST method within the AWS API Gateway UI [full_stop] The following screenshot hopefully give you enough of a clue on how to set this up [full_stop] Now [comma] when Slack sends the malformed (IMHO) POST [comma] the API gateway will reformat it and pass it through to your lambda function as if it were a normal JSON payload [full_stop] ',\n",
       " 'I decided to use DynamoDB - Amazons Document Database as a Service (DDaaS [eh] ) [full_stop] Im not sure its the perfect choice for this work [comma] since querying is pretty limited [comma] but it is very cheap and incredibly simple to use [full_stop] ',\n",
       " 'For this step [comma] just use the web UI to create a new table called MoodResponses [full_stop] I used an id field as the index [full_stop] The lambda creates id by concatenating the user ID and current week [full_stop] This means you automatically limit each user to a single vote per week [comma] which is exactly the functionality I was looking for - more or less for free [bang] ',\n",
       " 'Final step is very simple - use the Slack admin UI for your bot to add the address of your API resource as the target for interactive message callbacks [full_stop] Go to the admin page and select Features -> Interactive Messages from the panel on the left and paste in the URL of your API Gateway method [full_stop] ',\n",
       " 'Though there are more boxes on the diagram below [comma] this is actually the easiest step by far [full_stop] We serve up a simple D3js single page app direct from S3 as static content [full_stop] This SPA page calls a GET method on the REST service we created above which in turn calls a Lambda function [full_stop] The Lambda hits out database [comma] pulls out the results and sends them back as a JSON payload [full_stop] ',\n",
       " 'Theres not much more to explain [comma] so Ill just link to a Fiddle which includes the code for my front end - this one actually hits my production database [comma] so youll be able to see how my team feel [bang] ',\n",
       " '[jsfiddle url= height=350px include=result [comma] html [comma] js [comma] css font-color=39464E menu-background-color=FFFFFF code-background-color=f3f5f6 accent-color=1C90F3]',\n",
       " 'Serving this code up as a static HTML file is very easy: Create an index [full_stop] html document and add the javascript [comma] HTML and CSS from the fiddle; create a new S3 bucket and [comma] in the properties for the bucket [comma] enable Static Website Hosting; upload your index [full_stop] html file to the bucket [comma] select it and select Make Public from the Actions dropdown [full_stop] ',\n",
       " 'Heres the code for the Lambda function which is servicing the GET request:',\n",
       " 'Dynamo can only query against fields which are part of an index [full_stop] Here we need to query by week number [comma] so I added a new index to my Dynamo table by week [full_stop] This took 5 minutes to update (even though the table only had 5 records in it at the time [bang] ) but was simple enough to do [full_stop] If you look at the code above [comma] you can see where I specify the index in the query parameters [full_stop] ',\n",
       " 'So this all works really well [full_stop] Theres lots left to do: making the results look prettier and looking at how the sourcecode is deployed and managed being two things at the top of my list [full_stop] ',\n",
       " 'Slack is the industry standard tool for team collaboration these days [comma] and bots and integrations amp up the power and productivity at your disposal [full_stop] Build status [comma] Jira tickets [comma] team morale [comma] coffee orders and whatever else you fancy can all be brought together with conversational APIs [comma] simplifying just about everything [full_stop] ',\n",
       " 'On the AWS side [comma] theres still a lot of googling required to build this sort of thing [comma] and sometimes information is scarce [full_stop] Those who enjoy building proper applications using IDEs like IntelliJ or Visual Studio are going to find this frustrating - the pieces feel disjoint and uncontrolled sometimes [full_stop] However [comma] all in all its pretty cool what you can do without a single server in the mix [full_stop] ',\n",
       " 'Its hard to deny that this development model is going to be the de-facto standard within the next couple of years [comma] as its just so damned quick and simple [full_stop] So get out and get serverless [bang] ',\n",
       " 'Years ago [comma] before they invented Google Earth and Bing Maps and all that [comma] I did some work to show GPS data using Mapserver [full_stop] At work yesterday I was forced to revisit it while maintaining an aged and creaking part of our product [full_stop] It took a while to get my head back into the right state but once Id got going I started to enjoy it again [full_stop] This time around [comma] because Im at work [comma] Im using found here [full_stop] ',\n",
       " '',\n",
       " 'When I last played with Mapserver there was very little decent data [full_stop] I had to settle for a very low resolution map of the world (vmap0) and some pretty poor raster data [full_stop] Yesterday [comma] after a little googling [comma] I found that OS have released reasonable online tutorials too [full_stop] I spent a little time getting this to render and the results are shown here in the scaled image above and full-size image below [full_stop] ',\n",
       " 'I decided to go with a black background as I was thinking of a Raspberry Pi based GPS system to show The Dukes location on a small screen [full_stop] ',\n",
       " '',\n",
       " 'Note that the image is a little jagged looking [full_stop] Turns out you can fix this by adding the following snippet into your map file (under the MAP element) [full_stop] Of course [comma] you pay a high price in processing time and image size [full_stop] ',\n",
       " '[sourcecode] OUTPUTFORMAT NAME png DRIVER AGG/PNG MIMETYPE image/png # Change to IMAGEMODE RGBA for transparent background IMAGEMODE RGB EXTENSION png FORMATOPTION INTERLACE=OFF END [/sourcecode]',\n",
       " '',\n",
       " 'The benefit of Mapserver over Google or Bing maps is that it works on local data with no requirement for an internet connection [full_stop] This means its more reliable for a GPS sort of system [full_stop] Of course [comma] the downside is that there are fewer layers available and you have to do a lot of coding [full_stop] Was good to play with [comma] though [comma] and I hope to get it working on the raspberry soon [full_stop] ',\n",
       " 'Heres my (large) Map file:',\n",
       " '[sourcecode] MAP IMAGETYPE PNG #Whole UK would be #EXTENT 0 0 660000 1230000 # SU Grid is #EXTENT 400000 100000 500000 200000 # The top right of SU is EXTENT 450000 150000 500000 200000',\n",
       " 'END [/sourcecode]',\n",
       " 'This is part three in a blog mini-series about the power of Streaming Platforms with Kafka [comma] KSQL and Confluent [full_stop] ',\n",
       " 'Part 1 was an introduction to the problem of converting events to models in real time Part 2 showed how we can do that conversion using Kafka [comma] Streams and KSQL Remember that code can be downloaded from my github [full_stop] ',\n",
       " 'This post is all about aggregations [full_stop] Looking at how we can get aggregated data out of the streaming platform both real-time and ad-hoc style [full_stop] Heres a reminder of the Beer Festival simulation Ive been using:',\n",
       " 'Two full days of work went into the job of extracting results from Kafka and displaying them on a chart [bang] I might write another article about how difficult it was [comma] but for now were just going to talk about aggregations in KSQL and forget the last 48 hours ever happened OK [eh] ',\n",
       " 'Writing the KSQL to do an aggregation is pretty simple [full_stop] Here I group by bar and return sum(price) [comma] just like in regular SQL [full_stop] I also created a new stream called live_sales with 4 partitions [comma] so we can do joins later [full_stop] These query results gets rolled up into a table [comma] from which we can select:',\n",
       " 'The results of the select [comma] however [comma] are not what we might immediately expect [full_stop] Rather than a row for every bar (four rows total) were getting what look like duplicates [full_stop] This is because the table is based on a stream of sales messages: the value of count(*) changes every time a new sale is recorded [comma] and KSQL returns the updated value each time this happens [full_stop] ',\n",
       " 'If I stop the flow of sales records [comma] by killing the producer [comma] and tell KSQL to read the table from the start [comma] I get more normal looking results:',\n",
       " 'Obviously [comma] the beer festival cant pause every time the organisers want to draw a bar chart [comma] so I added a little dictionary-based cache between Kafka and the front end [full_stop] You can find the code for my simple caching web service on Github as always [full_stop] ',\n",
       " 'The middle-tier is a basic self-contained REST service based on Scalatra [comma] the worlds leading web micro-framework for Scala [full_stop] The front-end is a D3 [full_stop] js chart (code here) [comma] which pulls data from the middle tier in JSON format [full_stop] Im not recommending this as a production architecture [comma] but its a nice demo [bang] ',\n",
       " 'The beer festival has been running for a few days now [comma] bar 1 is clearly nearer the door [comma] while [comma] tucked away behind the pork scratching stand [comma] bar 4 is only selling pilsners [full_stop] ',\n",
       " 'The bar chart in the previous part showed the sales for all time [full_stop] This isnt much use if we want to see whats happening right now [full_stop] So [comma] lets create another aggregated table to show the total beer sales [comma] by bar for the last minute:',\n",
       " 'The interesting bit in the query above is window tumbling which creates a one minute aggregation window [full_stop] When the minute elapses [comma] the window closes and a new one opens [comma] with the counts starting at zero again [full_stop] Heres a screenshot everything else is the same [comma] really [full_stop] ',\n",
       " 'Note that you can also do hopping and session windows which slide the window in smaller increments and lock windows to sessions [comma] respectively [full_stop] Tumbling works fine for the beer festival though [full_stop] ',\n",
       " 'Lets jump straight in with a join [comma] to get beer names and sales into a stream together',\n",
       " 'Getting the total numbers of each beer sold is easy - as you can see from the snippet below [full_stop] We just group the joined sales data by name and sum the price to get the total sales [full_stop] Note the funky key column for the table [comma] which is the name and ABV concatenated (since I needed both in the results and they are covariant I just grouped by both) [full_stop] ',\n",
       " 'Here comes the tricky part though [full_stop] KSQL does have a function called TOPK which will return the top k values for a given column [full_stop] However [comma] this will only return the counts [comma] not the associated rows [full_stop] Since it doesnt help us much to show only counts [comma] Im just going to do the sort on the client side [bang] ',\n",
       " 'You can find the code for the chart here [full_stop] ',\n",
       " 'Its a shame you cant see the chart moving [comma] because it animates beautifully [bang] ',\n",
       " 'So that was a very swift tour of some of Kafka and KSQLs aggregation functionality [comma] combined with joins to reference data and schema transformation too [full_stop] All in all Im very happy with the results and were now pretty much all the way through our use-case [full_stop] ',\n",
       " 'One thing I was reminded of [comma] while writing this blog [comma] is that KSQL may make Kafka look like a bunch of tables [comma] but it is most definitely a streaming platform under the hood [full_stop] Though we can use KSQL to turn our events into models [comma] squashing updates into single records and joining streams together [comma] were still going to need to write the output to a real database at some point if we want to do traditional reporting [full_stop] ',\n",
       " 'That said [comma] I was able to create two or three real-time reports in just a few hours [comma] with minimal code (ignoring the time I spent working out how to do it [bang] ) [full_stop] The charts look great and the web services I wrote are basic [comma] but could be productionised pretty easily [full_stop] All in all [comma] its been great - I hope you enjoyed it too [bang] ',\n",
       " 'While repairing the crash damage to the Carbon-Copter I decided to do a more scientific investigation into the differences between carbon and aluminium',\n",
       " '10mm x 10mm Carbon square tube weighs about 0 [full_stop] 5g/cm while aluminium bar of the same dimensions (purchased from B&Q) is just slightly under 1g/cm [full_stop] That makes the maths easy - 150cm of bar needed for the carbon quad means a 75g difference between carbon and aluminium - which isnt that bad [full_stop] ',\n",
       " 'Aluminium is cheaper and seems to be more crash-resistant too [comma] plus I can pop over the road at lunchtime to buy some as opposed to ordering it from China [full_stop] So [comma] I made up a complete set of parts using each material [full_stop] ',\n",
       " 'With the copter repaired I concentrated on setting up the FPV gear [full_stop] Sadly [comma] I broke my little FPV camera - I think I caught a screwdriver on some of the components and shorted something [full_stop] It was only a tenner but I am still quite cross with myself for not taking enough care [full_stop] ',\n",
       " 'Anyway [comma] I have the GoPro which can also output video for FPV use [full_stop] Theres a slight lag (less than half a second probably) in the video feed which makes it a bit weird and since Im not willing to drill the waterproof case the camera has to fly naked which is a little riskier [full_stop] On the plus side [comma] many people report success with exactly the setup pictured below:',\n",
       " ' ',\n",
       " 'The garden isnt the ideal place for testing with fences and walls in every direction [comma] but my darling wife was kind enough to snaps some pics of the video feed on the garage telly while I had a nose over the neighbours fence flying line-of-sight [full_stop] ',\n",
       " 'The feed was a little noisy but a lot of that is going to be due to the fact that the signal had to get through the garage roof [full_stop] Penetration is not good on 5 [full_stop] 8GHz signals [full_stop] Need to sort out something a bit more portable and pop over to Dr Js house for more testing this week [full_stop] Watch this space [bang] ',\n",
       " 'Got home from work and started working on this here MultiWii [full_stop] This time I finally built up the courage to flash the thing with some new software [full_stop] ',\n",
       " 'It took me ages to find any documentation on the HobbyKing MultiWii 328P 2 [full_stop] 1 board [full_stop] There are scores of different hardware versions of the Multi Wii controller [comma] so google tends to point you in odd directions [full_stop] Anyway [comma] it turns out theres a Files tab on the HobbyKing page which has a load of useful stuff [full_stop] ',\n",
       " 'Theres a code snippet [full_stop] These actually contradict each other quite seriously [full_stop] I can only assume that HobbyKing have put quite a few versions of this board out and the settings are different [full_stop] Here are the only changes I made',\n",
       " '[UPDATE] So far this is the best PID tuning guide I have found - though it doesnt really explain PID control very well [comma] it does give a decent practical guide to tuning a multirotor [full_stop] ',\n",
       " 'The Dukes been running pretty well recently [comma] so theres not much to report on the blog [full_stop] Ive been spending my time in the workshop instead [comma] making bits and bobs on the lathe and mill [full_stop] ',\n",
       " 'Last night [comma] Dr J came round to make a dog to connect his capstan winch to his engine [full_stop] They cost about 50 each on eBay - even for a copy - so we made one out of a chunk of 45mm bar scrounged from the offcuts bin at a local engineering firm [full_stop] ',\n",
       " 'First job was to drill an 18mm hole all the way through [full_stop] Slightly smaller than the 3/4\" bolt that will hold it to the front of the crankshaft [full_stop] We then bored the hole out to exactly the right diameter [full_stop] Boring out the dog end [full_stop] This didnt leave a massive thickness of metal round the edges (~4mm) so Id recommend getting 50mm diameter bar if available [full_stop] Milling out the slots was very boring but went without a hitch [full_stop] Perfect fit [bang] Ive also been working on The Dukes NATO hitch recently [comma] so heres a couple of pictures of that [full_stop] The packing plates are all home made (only two shown) and the swivel pin was made on the lathe out of silver steel [full_stop] Not exactly the cheapest option [comma] but it does mean it should be rust free for some time [full_stop] ',\n",
       " 'The hitch itself The DIY swivel pin [full_stop] I spent some time polishing it [bang] I couldnt find the split pin for the end for a reasonable price [comma] so Ive threaded the hole to take a 3mm screw as an interim measure [bang] So it cost very little to make the dog (wear and tear on the tools being the biggest cost) but it did take the two of us six hours [comma] so I can see why these things cost as much as they do [bang] I think its safe to say though [comma] Dr J and I would much rather spend six hours using a lathe and drinking beer than paying somebody else 50 to do the same [bang] ',\n",
       " 'Just a quick pair of photos to prove I have the Multi Wii controller plugged in [full_stop] ',\n",
       " '',\n",
       " 'Note the lack of props [bang] I still dont trust this thing to fly around the way I expect it to [full_stop] Theres a problem with the minimum throttle setting which means the motors are always spun up when the controller is armed [full_stop] Its possible to change this [comma] but you need to flash new software onto the board and Im not sure Im brave enough to do that yet [full_stop] ',\n",
       " '',\n",
       " 'Mounted the board with double-sided tape and a chunk of firm packing foam (my XBox was packed in it) [full_stop] ',\n",
       " 'I managed to get all the channels wired up and working in the correct direction and confirm that the motors spin up an down in the way Id expect them to [full_stop] Weirdly this involved reversing the aileron channel on my DX6i [full_stop] I also had to max out the travel of all the channels and jump through some hoops to make the gear switch drive channel 6 [full_stop] ',\n",
       " 'Even though I managed to get two switches on my Tx to send signals through to the Multi-Wii [comma] it weirdly wont read anything from AUX2 [comma] AUX3 or AUX4 [full_stop] I confirmed its not the Tx by swapping channels [full_stop] Not sure if this is another setting that needs to be Turns out this is true [bang] ] [full_stop] Will have to investigate as Id like to have separate switches to enable/disable the various different sensors as I get used to them [full_stop] ',\n",
       " 'The Java GUI for the Multi Wii is really starting to annoy me [full_stop] Its slow and clunky and very hard to use [full_stop] Today I found that you need to click in the top left hand corner of the 10x10 pixel grey/white squares that act a bit like checkboxes [full_stop] If you click round the bottom right edge it just ignores you [full_stop] Its also impossible to know whether its accepted a click onto buttons like Write or ACC Calibrate [comma] which is frustrating and also a little scary [full_stop] ',\n",
       " 'Anyway [comma] I am still positive so far [full_stop] Maybe I need to take this thing out and get to know it in the flesh rather than watching bar graphs jump up and down on my PC screen',\n",
       " 'It turns out its really quite simple to get data out of Team Foundation Server [full_stop] You can do it using the built-in reporting tools [comma] but that seems a bit boring to me [full_stop] After the Big Cheese agreed to buy us a super-sized telly to get some project propaganda into the office I decided to whack out a WPF app [full_stop] ',\n",
       " '',\n",
       " 'Three useful bits of information worth recording here [full_stop] How to get information out of TFS [comma] how to draw nice looking graphs and how to make a borderless [comma] click-anywhere-drag style app without the windows title bar [full_stop] ',\n",
       " 'Im not going to go into huge depth on how I structured by queries as everyone uses TFS slightly differently [full_stop] If youve got the smarts to knock up some TFS queries - and if you dont you might need to learn anyway - then you should be good to go with these code snippets [full_stop] ',\n",
       " 'Get a list of sprints (iterations) [full_stop] This one was a bit tricky and took some time to work out [bang] ',\n",
       " 'Get a list of stories waiting for signoff by product managers [full_stop] ',\n",
       " 'For the charts I used the WPF toolkit [full_stop] Its not seen much development in recent years but its still an easy and quick way to get a chart on screen [full_stop] Here are some XAML snippets [full_stop] ',\n",
       " 'Heres the chart itself [full_stop] Two line series here [full_stop] You select the ItemsSource: list of objects corresponding to points on the line; DependentValuePath: name of a property on the object to use as the value for the Y axis; IndependentValuePath: value/label for the X axis [full_stop] ',\n",
       " 'This one is used to hide the legend:',\n",
       " 'Control Template to give better control over the look of the chart area:',\n",
       " 'Two styles needed to render the lines with no data points and custom colour etc:',\n",
       " 'This last one was insanely simple but I dont want to forget [comma] so here it is [full_stop] All code is in MainWindow [full_stop] xaml [full_stop] Adding these few lines gives you an app with no border that can be dragged by clicking and dragging anywhere [full_stop] I also added a double-click-to-toggle-maximised-state feature [full_stop] Use Viewboxes libreally in your XAML to make it look nice [bang] ',\n",
       " 'This is one of those things thats easy when you know how [full_stop] Just so I dont forget [comma] heres how to combine shape files using ogr2ogr [full_stop] ',\n",
       " 'I wrote it as a batch file to combine all the OSGB grid squares from the OS VectorMap District dataset into a single large data file for use with MapServer [full_stop] ',\n",
       " 'After a little map file jiggery-pokery I can now render a huge map of the UK or tiles with smaller maps without the many layer definitions needed to use ~20 shape file sets [full_stop] ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Downloaded and installed the latest Arch Linux image for the Raspberry Pi today [comma] only to find that the whole world has changed since I last looked at wireless connectivity [bang] ',\n",
       " 'Theres no rc [full_stop] d any more [comma] theres a totally new way to manage networks (wireless and otherwise) and everything I thought I knew is wrong [full_stop] I think Im getting old [bang] ',\n",
       " 'After hours of faffing about [comma] it turns out I could have had this nailed with two very simple commands [full_stop] ',\n",
       " 'Assuming you have a recent Arch Linux [comma] I think you just need to:',\n",
       " '1 [full_stop] Use the GUI tool to connect to your WiFi',\n",
       " 'Select the right network and enter the password when prompted [full_stop] When you exit the application you should be connected to your router and the tool will have saved a config file to /etc etctl/wlan0-YourNetwork',\n",
       " '2 [full_stop] Change to a static IP address',\n",
       " 'Weirdly [comma] the above step fails first time with a dhcp error [full_stop] It saves a valid config file but doesnt manage to connect [full_stop] It seems to want to bring up dhcp against eth0 not wlan0 as Id expect/prefer [full_stop] Didnt manage to find out why this is or how to fix it (yet) so just swapped to a static IP [bang] ',\n",
       " 'Use your favorite editor (I like nano) to change the config file just generated removing [sourcecode] IP=dhcp [/sourcecode] and adding this in its place',\n",
       " 'After youve done that [comma] repeat step 1 and proceed to step 3 [full_stop] Note that when you run wifi-menu the second time it will show that there is an existing config for your network [full_stop] You might also find that step 1 works and you dont need to do step 2 at all [comma] because you might have downloaded a fixed Arch image [full_stop] In which case [comma] I salute your fortitude [bang] ',\n",
       " '3 [full_stop] Install the config',\n",
       " 'Since you probably want to reconnect the wireless after reboots [comma] you need to install the config using the funky new netctl tool [full_stop] ',\n",
       " 'Now you should have a working wireless connection after reboots and a raspberry pi that works like your old one did [bang] ',\n",
       " 'Rant: Who moved my cheese [eh] ',\n",
       " 'People always moan about Windows versions that are different to old Windows versions - why Windows 8 took away the start button [comma] why Windows Vista did security totally differently [comma] blah blah blah',\n",
       " 'Today I spent several hours digging around looking for something that should be easy [comma] only to find it is indeed very easy when you know how [full_stop] ',\n",
       " 'So today Im quite happy to argue that Linux does exactly the same thing as Windows [full_stop] I like Arch Linux a lot [comma] and I accept that its not for newbies [comma] but things change as often in Linux world as they do in Windows world [full_stop] On both sides of the fence [comma] Google is your only hope if you want to keep up to date with the changing software that underpins everything you do [full_stop] ',\n",
       " ' ',\n",
       " 'A couple of weeks ago I wrote a post about a very simple data pipeline which pulls weather data from the Met Office and stores it in an S3 bucket [comma] where I can query it with Athena [full_stop] The source API is a bit unreliable (as is my code [bang] ) and the resulting data is filled with duplicates [comma] anomalies and missing values [full_stop] Its a pain in the backside to use [bang] ',\n",
       " 'In this post I want to talk about the next component of my Worlds Simplest Data Platform: Data Modelling [full_stop] Ill demo a basic ETL to deduplicate and clean the source data [comma] define a simple table structure [comma] and briefly show some ideas for data quality solutions and how to apply them in real life [full_stop] ',\n",
       " 'The demo is available on my github if youd like to see the code [comma] but really my aim is to discuss some core concepts of managing data [comma] building ETLs and making the most of your data assets [full_stop] ',\n",
       " 'Data modelling is often seen as part of the wider discipline of Data Engineering [full_stop] I prefer to see it as a unique business function in and of itself [comma] concerned with the design [comma] build and management of data as an asset [full_stop] While data platform engineers typically concern themselves with platforms [comma] tooling and integrations [comma] data modelling engineers busy themselves with the structure [comma] governance and presentation of data [full_stop] These two areas overlap [comma] no doubt [comma] but they have very different success factors and motivations [full_stop] Good data modelling is all about:',\n",
       " 'Some may be expecting this post to be about technology - specific tools like Snowflake [comma] BigQuery [comma] DataBricks or key concepts like Data Lakes [comma] Pipelines [comma] Warehouses or just good old-fashioned relational databases [full_stop] I think the choice of tools is very important [comma] but its also very specific to you and your business [full_stop] In this post Id rather focus on the core principals that are relevant regardless of the tech you choose [full_stop] ',\n",
       " ' ',\n",
       " 'Above is a sketch of what Ive built for my demo [full_stop] Its deliberately simple [comma] using serverless [comma] free-tier tools in my personal AWS account [full_stop] There are all sorts of sensible reasons why a real-life data platform would be more complex than this [comma] so forgive me if this looks different to what youre used to [full_stop] ',\n",
       " ' ',\n",
       " 'As I mentioned [comma] the dataset we get from the ingest job is difficult to use and the obvious place to start is to:',\n",
       " ' ',\n",
       " 'On a philosophical level [comma] were also changing the purpose of the data as we move it into a structured model [full_stop] The purpose of the ingest pipeline is just to store everything - that means we can save it pretty much in the format we receive it [comma] with a schema that corresponds to the source system [full_stop] Meanwhile [comma] modelled tables should be designed for us [full_stop] We should transform the data into a structure that reflects our own business and processes [comma] enabling users to interact with data in a very natural and familiar way [full_stop] ',\n",
       " 'As engineers [comma] we also need to think about the balance of accuracy versus timeliness in the data models [full_stop] Do users need the data to arrive within minutes or can they wait until tomorrow [eh] Do they need 100% accuracy or is it enough to be mostly right [eh] Think of the people balancing the books in your Finance department [comma] who want every penny accounted for [comma] versus the people in marketing [comma] who want to create new audiences and personalisations to respond to real world events ASAP [full_stop] ',\n",
       " 'In this demo I am doing an overnight process [comma] which will rebuild the entire table every morning at 6am [full_stop] This is perhaps the simplest possible tactic for dealing with late arriving data [comma] duplication and so on [full_stop] It also has the benefit that any changes made to the ETL (data quality checks [comma] changes to calculations etc) are reflected across the whole dataset first thing next day [full_stop] If you process billions of rows every day [comma] this might not be a viable solution for you [comma] but for this demo it works wonderfully [full_stop] ',\n",
       " ' ',\n",
       " 'One thing we know about the ingest pipeline which loads this data is that it can and does create duplicate rows [full_stop] This is common in real world ingest systems too - where at least once delivery is the de-facto standard for resilience [full_stop] Better to get the message twice than not to get it at all [full_stop] ',\n",
       " 'So [comma] the main job of our ETL is to take the data from the source [comma] in the bucket [comma] remove duplicates and store the results in  [comma] ready for (fictional) users to query [full_stop] This is pretty easy to do with a SQL statement [comma] especially now Athena supports statements [full_stop] ',\n",
       " 'The clever bit of this statement is the partition logic [comma] which ensures we take only the first reading for any given hour [full_stop] Note that were using the to do this [comma] not the and partition columns [full_stop] Our source data is partitioned based on the date it was received but thats of little use in our modelled table [full_stop] So we switch over to partitioning on event time [full_stop] We can do this easily here because we know we are regenerating the whole table each day [comma] so any late arriving data will be pulled into the appropriate partition on the next ETL run [full_stop] ',\n",
       " 'Its worth noting that in a real life system [comma] SQL might not be the right answer for developing an ETL [full_stop] You might use something like DBT [comma] Spark or an ELT process within a scalable warehouse platform like Snowflake or RedShift Spectrum [full_stop] ',\n",
       " 'You can see from the chart below how deduplication impacts the number of rows in the dataset for a given hour [full_stop] No doubt its looking much better now [full_stop] ',\n",
       " ' ',\n",
       " 'Duplicated data dealt with [comma] its time to look for any other quality issues in the data',\n",
       " 'One of the simplest ways to find anomalies in data is to visualise it [full_stop] So [comma] I created this chart to look at min and max temperatures in the raw weather data [full_stop] ',\n",
       " ' ',\n",
       " 'There is something fishy going on here with the minimum temperatures [full_stop] Most obviously for July at -18C (it simply does not get that cold in the UK in Summer) but also in February 2022 (-28C is way colder than youd expect to see) [full_stop] Looking at the Met Offices own summary for July 2022 [comma] we can see that the official minimum temperature for the month was 2 [full_stop] 3C',\n",
       " 'Another query revealed that these unusually cold readings were recorded by the weather station at RMB Chivenor [comma] home of the Comando Logistics Regiment of the Royal Marines and [comma] more importantly from the point of view of this data [comma] on the relatively warm North Devon coast [full_stop] ',\n",
       " ' ',\n",
       " 'Looking into the data for Chivenor [comma] you can see there was some kind of problem around the 7th July [comma] with a couple of very low readings [comma] followed by some nulls [full_stop] Now weve found these dodgy values [comma] we can take steps to filter them out when we create our modelled data table or to flag the rows for users of our data [full_stop] Which tactic we choose here depends on our use-case [full_stop] ',\n",
       " 'The easiest way to exclude the bad data is in the SQL statement we created to build our model [full_stop] This might seem crude and simplistic at first glance (and maybe it is [bang] ) [comma] but it has the benefit of being simple [comma] explicit and tracked in version control - and those things are good things [full_stop] ',\n",
       " 'WOW [bang] This was hard work though [comma] wasnt it [eh] Finding these issues manually is possible [comma] but its very time-consuming [full_stop] ',\n",
       " 'The final data quality issue in our weather dataset is missing data [full_stop] It seems that weather stations suffer outages [comma] undergo maintenance and so on [full_stop] This means missing data is unavoidable [full_stop] ',\n",
       " 'What we can do here [comma] and might also choose to do in real life [comma] is clearly document the fact that data might be missing [full_stop] We can flag the sites with the best and worst reliability and so on [full_stop] If you cant make the data perfect [comma] the best course of action is to very clearly communicate what consumers can realistically expect to find when they use it [full_stop] ',\n",
       " ' ',\n",
       " 'In the table above [comma] taken from this notebook we can see a visualisation of the data loaded for a given month this year [full_stop] As a user [comma] you can see at a glance the quality of the data - which in turn will influence the way you use it [full_stop] If you see a table in the data lake called weather you might assume it is a complete record of weather in the UK [comma] but a quick glance at the notebook shows thats not the case [bang] ',\n",
       " 'Visualising data quality like this also helps engineering teams to find and fix bugs [full_stop] I can imagine the story of an engineer climbing to the summit of a peak in the Cairngorms to turn something off and on again on 29th [full_stop] You can see the fix clearly in the chart above [full_stop] They didnt manage to get to the summit of Aonach Mor yet though [full_stop] You can also see a more transient bug or outage that happened on 25th [comma] where a large number of sites lost a couple of hours worth of data [full_stop] That was more likely an error in the data ingest pipeline - something we should look into soon perhaps [full_stop] ',\n",
       " ' ',\n",
       " 'As well as storing a clean and complete model of the incoming data [comma] in many real world scenarios it also makes sense to create a suite of summary tables [comma] which are smaller and more targeted to a particular business application or use-case [full_stop] For example imagine were using this data to examine long term trends in UK weather [full_stop] We want to be able to quickly access temperature readings for different locations [comma] on a month by month basis [full_stop] ',\n",
       " 'Heres some SQL to compile such a summary',\n",
       " 'Its not unusual for a summary table like this to encapsulate some business logic [full_stop] In this example [comma] and are using the 5th and 95th percentile [comma] rather than the min and max values for temperature [full_stop] There are all sorts of sensible reasons for this - if youre interested in trends [comma] you might well exclude outliers and corner cases and so on [full_stop] ',\n",
       " 'The important thing to do when implementing this kind of logic is to very clearly document whats going on [full_stop] Theres a real risk that a new user of this summary table [comma] maybe picking it up two or three years down the road [comma] would make incorrect assumptions about the values in these two fields [full_stop] If you were asked to find the lowest temperature recorded in the UK you might do something like and this would of course be wrong [full_stop] Now imagine if this was a critical business metric like revenue [comma] unique users [comma] purchase price and the same mistake was made',\n",
       " 'There are many ways to document this kind of thing [comma] and Id advise you to use all of them [bang] You can manually create docs on your wiki [comma] you can make the code available to users of the data [comma] you can use a proper data catalogue to allow users to explore your schema [comma] and so on [full_stop] One of the simplest ways to make the business logic clear though is to just give the fields sensible names [comma] so becomes  [full_stop] Users might be slightly annoyed that they have to type a few more characters when running a query [comma] but they will be much less likely to misunderstand the field [full_stop] Self documenting schemas are very valuable [bang] ',\n",
       " ' ',\n",
       " 'Because the nature of incoming data is to change over time [comma] its hard specify in advance all checks [comma] measures and special cases you might see over the next few months or years [full_stop] Thinking about our weather data [comma] weve already seen that sites have outages and send invalid values [full_stop] We know sites will be decomissioned in future and that new ones will come online [full_stop] Maybe the data feed will change and new measures will be added [comma] data types changed or the relational constraints altered [full_stop] Even macro-level influences like climate change mean that range checks we might build today would be invalid in a few years time [full_stop] ',\n",
       " 'So Id suggest that its better to capture [comma] store and visualise data about your data than in is to try and handle errors internally [full_stop] Users will trust your model all the more if they can see a selection of charts and metrics which show the health of the data at a glance [full_stop] ',\n",
       " 'The KDE plot below shows temperatures this December vs the same month last year [full_stop] You can see weve recorded much lower temps this year and that seems pretty normal [full_stop] However [comma] theres enough in this basic chart to prompt some thinking - to validate that yes [comma] this December was a cold one; to confirm a pattern an analyst spotted elsewhere and so on [full_stop] Imagine a similar plot showing order values [comma] bank balance [comma] transaction counts and hopefully you can see the value of producing metadata like this alongside your models [full_stop] ',\n",
       " ' ',\n",
       " 'Giving humans the ability to visualise data quality is incredibly valuable [full_stop] The next step of course is to automate these quality checks - monitoring metadata and raising the alarm when things dont look right [full_stop] There are some excellent tools on the market to do just this: Great Expectations which I have used and redata and datafold which Id love to dive deeper into one day [full_stop] ',\n",
       " ' ',\n",
       " 'Thats been a very long post [comma] and if you made it this far - thank you [bang] Weve barely scratched the surface of the exciting and underappreciated world of Data Modelling [comma] but I hope that by going back to basics like this I have illustrated what I believe are the key considerations:',\n",
       " 'If youd like to read more back to basics content [comma] why not check out this post on data ingest or this one on building a data strategy [full_stop] If you fancy something a bit more unusual [comma] heres a post on face clustering [full_stop] ',\n",
       " 'A friend of mine has always said that young cars with high mileage are better than old cars with low mileage [full_stop] The theory being that company cars [comma] which have spent their time cruising on the motorways [comma] have had a much easier life than their stay-at-home cousins whove done short hops around town and sat on their driveways seizing up [full_stop] ',\n",
       " 'So I pointed some very simple Spark queries at the here) [full_stop] First factoid to note is that both mileage and age are relevant when it comes to predicting pass rates [full_stop] The following two charts show pass rate vs mileage and age [full_stop] ',\n",
       " 'To look at all three variables together I created the following chart which shows shows age on on the x axis and mileage on the y [full_stop] Pass rate is a colour scale with red being the worst and green the best [full_stop] Green squares show combinations of mileage and age at which vehicles are more likely to pass their MOT on the first attempt [full_stop] Red squares show combinations where a first-try failure is likely [full_stop] ',\n",
       " 'There is some truth to my mates theory - at least if this chart is to be believed - the pass rate for 3-5 year old cars looks pretty good even at very high mileages [full_stop] Looking horizontally for very-low-mileage cars of increasing age there seems to be something quite odd going on for vehicles on less than 20k miles [full_stop] For the 20k-40k range there does seem to be a green stripe across the ages [comma] but it is not as apparent as its vertical counterpart [full_stop] ',\n",
       " 'So should we all be buying a four-year-old car with 180k miles on the clock [eh] Well [comma] no [full_stop] At least not if we want to keep it for more than a year or two [full_stop] Cars with high mileages on the clock go into the red much earlier than those with low mileage (based on the fact that vehicles can only move right and up through the chart as they get older and drive further) [full_stop] ',\n",
       " 'That last chart shows the same heat-matrix view [comma] but to the full extents of the data [full_stop] There are some interesting facts hidden in that chart but Ill leave them as an exercise for the reader [bang] ',\n",
       " 'So it turns out that calculating correlation and covariance with Spark is pretty easy [full_stop] Heres the results and the code:',\n",
       " 'Looking at cars in the normal range (i [full_stop] e [full_stop] less than 20 years old and less than 250k miles) theres a stronger correlation between age and pass rate than between mileage and pass rate [full_stop] Interestingly [comma] looking over the full range of the data this relationship is inverted [comma] with mileage being very slightly better [full_stop] Theres little to separate the two as a predictor for pass or fail - not least because age and mileage are largely dependant on each other (with a correlation of 0 [full_stop] 277 across all data) [full_stop] ',\n",
       " 'Basic statistical functions are available under DataFrame [full_stop] stat [full_stop] See the calls hidden in the println lines below:',\n",
       " 'Every year I like to do a quick roundup of all the things Ive made [full_stop] Making stuff is important [comma] and its nice to record what Ive done for posterity [full_stop] ',\n",
       " 'Having made a kitchen sink for the school last year [comma] I went on to make a play-cooker this year [full_stop] All made from scrap wood and bits and bobs [full_stop] ',\n",
       " 'Kerb appeal [bang] This years mamouth outdoor project was the front garden [comma] where we cleared out a jungle of weeds and put down two tons of gravel [full_stop] I also made a nice bin area and a patio/path in front of the door [full_stop] This was much easier than the patio project and lawn from previous years [comma] but still involved a lot of digging [bang] ',\n",
       " '',\n",
       " 'Too many wooden things to mention [bang] A variety of boxes [comma] some new mirrors for the bathroom [comma] a sauce caddy for barbeques [comma] which makes setting the table 100 times faster and some more artistic creations',\n",
       " 'Im super-happy with how these two projects turned out [full_stop] The combination of raw steel and oak is beautiful [comma] even if the overall quality is limited by my dodgy welding [full_stop] ',\n",
       " 'Not content with a single shed [comma] I built another [comma] which houses general garden tat [comma] bikes and a huge quantity of scrap wood for future projects [full_stop] ',\n",
       " 'Routers are great [comma] but really hard to use on small parts [full_stop] I started to find that whenever I used the router [comma] it was clamped upsidedown in a vise and I was risking my fingers feeding things into it [full_stop] After a little welding and bodging [comma] I now have a servicable router table [comma] which makes these operations much simpler [full_stop] ',\n",
       " 'Having made 20-something picture frames last year [comma] this year I went for quality over quantity [full_stop] I made this one from oak [full_stop] Its simple [comma] but its clean and neat and should last a very long time [full_stop] It was a pressie for the in-laws [comma] so had to be perfect [bang] ',\n",
       " 'Finally [comma] after years of living with a broken lathe [comma] I replaced the broken DC motor and controller with a shiny new AC setup [full_stop] My budget would only run to 1 horsepower [comma] so the lathe does tend to stall if I push it too hard [comma] but its 10x more usable than before [full_stop] First job was to turn a new pulley for the new motor',\n",
       " 'Working in London eats up my time these days [comma] so I managed much less than I wanted/expected to get done in 2019 [full_stop] However [comma] looking back at these pictures from across the year [comma] I realise I still managed a fair bit [full_stop] Mustnt grumble [bang] ',\n",
       " 'Renaming a project in Team Foundation Server 2012 [full_stop] Seems like a sensible thing to do and not something that would take you very much time [full_stop] Except for the fact that',\n",
       " 'After some googling and much swearing we finally managed to get it to work [full_stop] The solution isnt to rename the project but to create a new one and move all your work items over [full_stop] Heres how I did it [full_stop] Note that this post only covers work items [comma] not source control [full_stop] ',\n",
       " 'I always forget where to click [comma] but the next image makes it clear [full_stop] Make sure you set the name and choose the correct project template [full_stop] ',\n",
       " 'If you havent customised the project template at all then you can skip this step [full_stop] If you have made any changes you need to make sure you copy them to the new project before you migrate [full_stop] If you dont want to move your changes over there are clever things you can do with an XML mapping file but Im not going to cover that today [full_stop] ',\n",
       " 'Work items can be moved between projects using the TFS Integration Tools from Microsoft [full_stop] You can download them here [comma] though Id suggest you search for the latest version [comma] just in case [bang] Install the tool on your TFS server and run it from the start menu [full_stop] ',\n",
       " 'Run the tool and select Create New from the menu on the left [full_stop] ',\n",
       " '',\n",
       " 'Choose the template: C:\\\\Program Files (x86)\\\\Microsoft Team Foundation Server Integration Tools\\\\Configurations\\\\Team Foundation Server\\\\WorkItemTracking [full_stop] xml',\n",
       " 'In the configuration you need to select a project on the left and on the right [full_stop] For a one way migration the left is the Source and the right is the Target [full_stop] Make sure you give the migration a sensible name and choose One Way Migration [full_stop] Click the Configure button on the left and select your old project [full_stop] Click the Configure button on the right and select your new project [full_stop] ',\n",
       " 'Click Save to Database then click the Start button on the menu bar on the left [full_stop] ',\n",
       " 'Off it goes Looking good so far Almost there Yes [bang] ',\n",
       " 'When I first tried to run a migration I hit a permissions error [full_stop] Three warnings popped up as soon as I started the migration [comma] the full text of which is below:',\n",
       " '[sourcecode]Microsoft [full_stop] TeamFoundation [full_stop] Migration [full_stop] Tfs2010WitAdapter [full_stop] PermissionException: TFS WIT bypass-rule submission is enabled [full_stop] However [comma] the migration service account Administrator is not in the Service Accounts Group on server  at Microsoft [full_stop] TeamFoundation [full_stop] Migration [full_stop] Tfs2010WitAdapter [full_stop] VersionSpecificUtils [full_stop] CheckBypassRulePermission(TfsTeamProjectCollection tfs) at Microsoft [full_stop] TeamFoundation [full_stop] Migration [full_stop] Tfs2010WitAdapter [full_stop] TfsCore [full_stop] CheckBypassRulePermission() at Microsoft [full_stop] TeamFoundation [full_stop] Migration [full_stop] Tfs2010WitAdapter [full_stop] TfsWITMigrationProvider [full_stop] InitializeTfsClient() at Microsoft [full_stop] TeamFoundation [full_stop] Migration [full_stop] Tfs2010WitAdapter [full_stop] TfsWITMigrationProvider [full_stop] InitializeClient() at Microsoft [full_stop] TeamFoundation [full_stop] Migration [full_stop] Toolkit [full_stop] MigrationEngine [full_stop] Initialize(Int32 sessionRunId) [/sourcecode]',\n",
       " '\"the migration service account Administrator is not in the Service Accounts Group\" being the important thing here [full_stop] The annoying thing is that the group in question turns out to be hidden [full_stop] Luckily [comma] somebody has posted a solution [full_stop] Basically you need to run the following command on your TFS server:',\n",
       " '[sourcecode]c:\\\\Program Files\\\\Microsoft Team Foundation Server 11 [full_stop] 0\\\\Tools\\\\tfssecurity /g+ Team Foundation Service Accounts n:YourMachine\\\\YourUser ALLOW /server: [/sourcecode]',\n",
       " 'The big thing for me is queries [full_stop] I have 20+ custom queries that all relate to different parts of the process [full_stop] All these will need to be moved over by hand as the tool doesnt do it for you [full_stop] ',\n",
       " 'If you use the reporting services and so on youll find that they are not dealt with properly either [full_stop] ',\n",
       " 'If you have a period of handover between the two projects you might want to run them in parallel or have a rolling handover [full_stop] The great thing about the migration you set up is that you can run it again and copy any changed made in the old project since your initial copy [full_stop] This means you can get the new project set up [comma] sort out all your documentation and stuff and have a structured migration plan for your team [full_stop] ',\n",
       " 'I made a Wobbler [bang] A single acting [comma] oscillating air engine [comma] to be precise [full_stop] I was inspired by the amazing Tubalcain [comma] whos YouTube channel has been the source of many top tips and the cause of many late nights [full_stop] Though the design of the wobbler wasnt new to me [comma] the old Mamod steam engine I had when I was a kid used the same principle [full_stop] ',\n",
       " '',\n",
       " 'The wobbler is probably the simplest engine design there is [full_stop] I tried to to a nice job of this one [comma] though didnt go overboard with the look of the thing - I dont see the point of pretending its not made from some bits and bobs I has knocking about in the garage [bang] Here are some photos:',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " 'In their wisdom [comma] Google have decided to remove FTP publishing from Blogger [full_stop] This is a right pain in the backside as it means setting up a subdomain and faffing about with DNS and Blogger settings [full_stop] ',\n",
       " ' Here are the hard-working Google engineers scrapping some perfectly reasonable functionality for no reason [full_stop] ',\n",
       " 'Normal service will resume in a couple of days when Ive managed to find time to sort everything out [full_stop] The blog can now be found at ',\n",
       " 'This year has been a year of making - mostly jobs around the house and garden [full_stop] Its been great fun and I thought Id document some of the projects here for posterity [full_stop] ',\n",
       " '',\n",
       " 'First make of the year was a mini Ikea Billy bookcase [full_stop] It only took a couple of hours [comma] but it was very satisfying indeed (and saw me joining the millions of Ikea hackers on the internet) [full_stop] ',\n",
       " 'Wed put three large bookcases across a wall in our living room [comma] but this left a bit of a gap at one side [comma] not quite wide enough for one of the smaller bookcases [full_stop] Since I didnt have anything but the bandsaw at the time [comma] I made a crude fence to ensure a straight and consistent cut and proceeded to chop all the shelves down to size [full_stop] Since you cant see the sides of the bookcase itself [comma] you cant see the wood screws I used to secure the shelves [bang] ',\n",
       " '',\n",
       " 'Undoubtedly the hardest thing Ive ever made (physically at least) [comma] this project saw me carting several tons of soil and hardcore backwards and forwards [comma] and laying turf in rainy and miserable weather [full_stop] The results are pretty awesome though [full_stop] Our garden now has plenty of space for the kids to practice cartwheels and generally go bananas [full_stop] ',\n",
       " 'Laying the brick edging round the lawn was incredibly difficult because I foolishly decided to mix all the cement by hand [full_stop] For future projects Ill buy or hire a cement mixer [bang] ',\n",
       " '',\n",
       " 'Having been given a welder (thanks Dad [bang] ) and an oil drum (thanks Mother in Law [bang] ) for Christmas [comma] and with the lawn out of action [comma] the next project on the list was a new barbecue [full_stop] I used some heavy gauge [comma] 100-year old steel fence posts rescued from a friends garden to make a solid base [comma] then chopped up the oil drum before adding hinges [comma] a chimney and a grill [full_stop] ',\n",
       " 'The chimney is made of old welding gas bottles [comma] chopped and welded into a tube and the side tables and bottom shelf are made of pallet wood from the turf delivery for the lawn so the total materials cost was no more than some hinges and a couple of oven shelves from eBay [bang] ',\n",
       " '',\n",
       " 'With a newly cleared gap in the bottom corner of the garden [comma] it seemed only fitting to build a playhouse for the girls [bang] This was pretty expensive [comma] with most of the timber [comma] cladding and OSB coming from the timber merchants [full_stop] Not to mention over 800 wood screws [bang] ',\n",
       " 'I built the house in pre-fab sections [comma] since there was very little room on the site [comma] I had to complete each section then lift it into place and secure it with screws on the inside [full_stop] I deliberately left everything as wonky as possible [comma] aiming for a gingerbread house sort of look [full_stop] ',\n",
       " '',\n",
       " 'The windows are made of 10mm perspex (found in a skip [bang] ) which worked really well as its the same thickness as the OSB skin on the walls [full_stop] This meant I could attach the windows and OSB together in square sections then screw the window frames and cladding over the top in whatever shape I wanted [full_stop] ',\n",
       " 'The roof is OSB too [comma] with 250+ wooden tiles made from pallet wood nailed on [full_stop] I should have left bigger gaps between the tiles though [comma] a few months on they have expanded in the rain and lifted in places [full_stop] ',\n",
       " 'I havent done many software projects this year [comma] but the Mood Bot is one Im quite happy with [full_stop] Its a Slack integration which tests a teams mood and draws some pretty graphs [full_stop] You can read more about it here [full_stop] ',\n",
       " '',\n",
       " 'Well [comma] actually its more of a TV stand for the loft conversion in our house [comma] which we converted into a study/second living room this year [full_stop] The table top is a slab of oak and the legs are 25x50mm steel box section [full_stop] Im pretty happy with the results [comma] but I need to find a tougher [comma] child-resistant finish [bang] ',\n",
       " '',\n",
       " 'While looking for an unusual picture for the top room [comma] we thought about buying some kind of world map [full_stop] This led me to start looking at Mapserver again [comma] and ultimately the creation of this map of Thatcham [full_stop] We got it printed on a canvas by one of those online photo companies and I matched the colours to our garish new sofa [bang] ',\n",
       " '',\n",
       " 'For one of this years work events we were supposed to dress for a masked ball [full_stop] I couldnt find any masks I liked on the internet and I left it too late to go to a fancy dress shop [comma] so the weekend before the party I headed out to the garage and started snipping and hammering some scrap steel from an old office letter tray [full_stop] I was very pleased with the results [comma] but sadly proceeded to get so drunk I left the mask itself behind [bang] I might have to make another',\n",
       " '',\n",
       " 'So all in all its been a make-tastic year [full_stop] Ill have to start thinking about what projects to do in 2018 - though I suspect a new desk for the top room [comma] a snazzy interior for the playhouse and maybe an electric rotisserie for the barbecue are on the cards at some stage',\n",
       " 'Wooo [bang] My parcel of carbon fibre bits arrived from HobbyKing yesterday so I shot out to the garage straight after dinner to start playing [full_stop] ',\n",
       " ' ',\n",
       " 'Inspired by The Firegoat I decided to knock up a quad frame [full_stop] I have just enough motors to have a quad and a tricopter in service at the same time and Im interested to play with the H-copter design that popped up on FliteTest recently [full_stop] ',\n",
       " 'Stability and high capacity are the plan for this one [full_stop] The design more or less mirrors the H copter Josh built [full_stop] The only difference is that I have used carbon and he used wood [full_stop] ',\n",
       " ' ',\n",
       " 'One thing I learned pretty quickly is that carbon fibre - especially the 10mm square bar - cant cope with compression very well at all [full_stop] I used washers and didnt tighten anything up too far [full_stop] The next picture shows what happened when I tried to bolt on the motor mount',\n",
       " ' ',\n",
       " 'The motors have to be pretty tight to keep them from spinning round or wobbling but as the photo shows [comma] they cant be bolted down [full_stop] So [comma] I made up four plugs which poke 20mm into the end of the legs and prevent the nut and bolt compressing things too far [full_stop] They should add some strength around the joint too [full_stop] ',\n",
       " ' ',\n",
       " 'One obvious annoyance with a quad as opposed to a tricopter is that you need to make four of everything [bang] The plugs werent hard as such [comma] but it took about 15 minutes to get them made up and got quite boring [full_stop] Well worth the effort though - they do a good job [full_stop] ',\n",
       " ' ',\n",
       " 'It was midnight before I shut up shop and I had a basic frame made up [full_stop] Four motors with 11x4 [full_stop] 7 props with mounts stolen from the now-dead double tricopter [full_stop] No electronics yet [comma] so tonight therell be much soldering in the garage I should think [full_stop] ',\n",
       " 'Complete with a 2 [full_stop] 2Ah battery and with motors and props fitted the new Carbon Copter weighs just 800g [bang] With the GoPro on the scales its just under 1kg [full_stop] The lightest tricopter I ever managed was about 1 [full_stop] 1kg without a GoPro and had (obviously) less thrust [full_stop] So I have high hopes for this little puppy [full_stop] ',\n",
       " ' ',\n",
       " 'Its traditional to end on a low [comma] so Ill do just that [full_stop] Carbon fibre is brittle [full_stop] I decided to do a little crash test with the motors attached - dropped the frame sideways onto the concrete floor of the garage from about 18\" up [full_stop] There was a sickening crunch [full_stop] No visible damage [comma] but one of the legs now twists much more easily than the others [full_stop] I can only assume that some of the fibres within the leg have come unstuck [full_stop] ',\n",
       " 'I think Ill have to make up some spare legs for the first test flight [comma] and work on some landing gear too [full_stop] ',\n",
       " 'Very excited to get this thing off the ground [bang] ',\n",
       " 'Over the last couple of weeks I have been doing a lot of work importing polygons into an SQL server database [comma] using them for some data processing tasks and then exporting the results as KML for display [full_stop] I thought itd be worth a post to record how I did it [full_stop] ',\n",
       " 'Inserting polygons (or any other geometry type) from a shape file to the database can be done with the ogr2ogr tool which ships with the gdal libraries (and with Mapserver for Windows) [full_stop] I knocked up a little batch file to do it:',\n",
       " 'The first ogr2ogr call is used to simplify the polygons [full_stop] The value 0 [full_stop] 01 is the minimum length of an edge (in degrees in this case) to be stored [full_stop] Results of this command are pushed to a temporary shape file set [full_stop] The second call to ogr2ogr pushes the polygons from the temp file up to a database in Windows Azure [full_stop] The same code would work for a local SQL Server [comma] you just need to tweak the connection string [full_stop] ',\n",
       " 'You can use SQL Server Management Studio to show the spatial results of your query [comma] which is nice [bang] Here I just did a select * from testPolygons to see the first 5000 polygons from my file [full_stop] ',\n",
       " '',\n",
       " 'Sql Server contains all sorts of interesting data processing options [comma] which Ill look at another time [full_stop] Here Ill just skip to the final step - exporting the polygon data from the database to a local KML file [full_stop] ',\n",
       " '',\n",
       " 'Obviously you can make the SQL in that command as complex as you like [full_stop] ',\n",
       " 'Polygons here are from this site which allows you to download various polygon datasets for various countries [full_stop] ',\n",
       " 'This article is part 1 of a mini-series about events and models [full_stop] The following parts are quite technical [comma] with Kafka [comma] Scala and KSQL code [comma] but this part is a bit of an introduction to the problem at hand [full_stop] Enjoy [bang] ',\n",
       " 'Im a Data Engineer by trade [full_stop] Ive been working with Data Platforms for some years now [comma] usually building tools to ingest and manage data [comma] in a whole heap of industries from supermarket fridges to telecoms networks to insurance websites [full_stop] Each industry has its own unique challenges [comma] be it latency [comma] data volume or data quality but each use-case has ended up being essentially the same',\n",
       " 'Our lives [comma] the universe [comma] everything is simply a stream of events [full_stop] People buy things on websites [comma] make calls on their phones [comma] order beers [comma] click like on cat videos [full_stop] Everything that makes us who we are can be encapsulated in a time-ordered change log [full_stop] These days [comma] many companies are adopting event-driven architectures to handle the endless firehose of stuff that happens [full_stop] ',\n",
       " 'Humans [comma] however [comma] dont think in terms of events - they think in terms of of things*: *How much money do I have [eh] How many customers bought avocados [eh] Whats the mobile coverage like in my street [eh] We just cant help but think in terms of entities and attributes [full_stop] As a result [comma] pretty much every company still has an old-school BI function; a SQL data warehouse; an enterprise data model (or just an ER diagram on the office wall); overnight batch ETL jobs; daily and monthly reports These old-school tools help us to rationalise the world in a way that feels natural [full_stop] ',\n",
       " 'The job of Data Engineers is to collect [comma] filter [comma] clean and store the endless stream of events and to map the changes they represent onto an entity-based model of the world [full_stop] ',\n",
       " 'What surprises me is that [comma] even though this problem has been around forever (as far back as the 50s and 60s in fact) there is no standard pattern for solving it [full_stop] As the latency of life decreases and we start to eek out the margin from every single minute and second [comma] the pressure mounts to paint an accurate picture of our customers and systems in real time [comma] 24x7 [full_stop] ',\n",
       " 'In the olden days by which I mean [comma] this still happens now [comma] but when asked about it [comma] people look sheepish and refer to it as legacy anyway [comma] in the olden days conversion from events to models happened as an overnight batch [full_stop] Take this classic example of a bank',\n",
       " 'Every night [comma] all reporting operations stop [comma] while a monstrous ETL job aggregates all the transactions from the past 24 hours [comma] adds them to the yesterdays account balances and updates the master database [full_stop] Remember when your bank balance changed once a day [eh] ',\n",
       " 'Theres actually nothing wrong with this [bang] Locking users out for a couple of hours while running an overnight job is a great way to ensure that everyone arrives in the office next day to a view of the world which is correct [comma] consistent [comma] stable and reliable [full_stop] Its perfect for financial reporting [comma] where accuracy is more important than timeliness but if you want a more up-to-date view [comma] you have to come back tomorrow [full_stop] ',\n",
       " 'A modern batch process would usually be used to populate bulk data in warehouses and data marts used for financial reporting and long term forecasting [full_stop] Updating overnight means results are consistent all day [comma] across all reports/departments etc [full_stop] In a typical AWS deployment [comma] data is tapped from the ingest pipeline and archived to a data lake as files in S3 [full_stop] Overnight a series of EMR pipelines pick up [comma] aggregate and transform the raw data [comma] storing the results into a relational database like RedShift [full_stop] ',\n",
       " 'In the front office [comma] things have been changing over the last few years [full_stop] For all sorts of sensible reasons [comma] the front-end folks are moving away from fat [comma] monolithic applications [comma] desktop clients and complex MVC apps towards lightweight micro-services [full_stop] The single responsibility principal now applies to services not classes [comma] and scores of them are wired together in complex webs promoting re-use [comma] scalability and a clear separation of concerns [full_stop] ',\n",
       " 'But getting consistent and repeatable results from a complex web of services is hard [full_stop] Timing starts to play a big part in what happens [comma] as calls happen slightly earlier or later and the state of data in services becomes prone to race conditions and emergent behaviour [full_stop] For this reason [comma] the next step on from micro services is Event Sourcing [comma] which uses a central log of events to give consistency [comma] repeatability and traceability [full_stop] ',\n",
       " 'To cut a long story short [comma] the front office is now sending events to record everything that happens [comma] as it happens [comma] in real time [full_stop] If Data teams are going to join in the fun [comma] then all our reports [comma] models and snazzy-pants neural networks need to play the same game [full_stop] We need to be predicting a customers next action now [comma] not overnight [full_stop] Which means we now need to convert events into models in real time [full_stop] ',\n",
       " 'The challenge is simple - we need to process a stream of events [comma] ensuring that every change is immediately mapped onto a database entity [full_stop] The standard [comma] simple [comma] usual way to do this is to just use a fast database as a cache [comma] thus:',\n",
       " 'Every time a customer buys a product [comma] we pull their customer record from the cache database [comma] update the appropriate fields and write the data back [full_stop] Since we want to avoid race conditions and respect running queries [comma] this is all done under a transaction [full_stop] ',\n",
       " 'On this plus side [comma] this gives us a database full of up-to-date model data [full_stop] There are downsides though:',\n",
       " 'It seems that there may be a better way though [full_stop] Over the past couple of years [comma] the folks working on Kafka [comma] especially those at Confluent [comma] have been looking into this exact problem - how can we turn events to models in real time [comma] without blowing all our cash on servers [eh] ',\n",
       " 'Kafka has [comma] for a long time now [comma] been the worlds greatest data integration tool [full_stop] It was just a scalable commit log; a queue of things which supported ordering and multiple producers and consumers [full_stop] It was incredibly reliable and popular [comma] exactly because of its simplicity [full_stop] ',\n",
       " 'With the addition of Kafka Streams and KSQL [comma] Kafka has grown to become a fully fledged streaming data platform [comma] which provides us with a solution to our events to models problem [comma] out of the box [full_stop] You can watch Jay Kreps [comma] one of the original authors of Kafka [comma] talking about this here [full_stop] ',\n",
       " 'In part 2 Ill walk through how I investigated these cool new features and show a demo based on a Beer Festival',\n",
       " 'In part 1 of this series [comma] I explained the age-old problem of converting events (stuff that happens [comma] yknow) into models (details about things; databases) [full_stop] In this post Im going to get down and dirty [comma] showing some awesome features of Apache Kafka which make this unbelievably simple [full_stop] ',\n",
       " 'You can find all the code [comma] details of the data and install instructions on my this kaggle [full_stop] ',\n",
       " 'The plan is to spin up a streaming platform [comma] with Kafka at its heart; push in some live sales events from the bar of a fictional beer festival [comma] along with some reference data on the beers and breweries',\n",
       " 'then to generate some simple reports and live dashboards using only Kafka [full_stop] This should demonstrate',\n",
       " 'If you want to follow along [comma] clone the gihub repo which contains all the code for this article [comma] get the confluent platform up and running on your laptop and build the Scala code with IntelliJ or your dev tool of choice [full_stop] Youll also need to download and tweak the source data [full_stop] All is explained in the README [full_stop] ',\n",
       " '',\n",
       " 'Below are the key methods from the SaleProducer object [comma] which sends a random sale record into the Kafka topic sales every second to simulate a busy beer festival [full_stop] ',\n",
       " 'Three simple fields are populated: beer_id is a random ID from the beer data [comma] bar is a random choice from 1 to 4 (the cash register used) and price is a random number of tokens: 1 for a half and 2 for a pint [full_stop] These fields appear in the Avro schema:',\n",
       " 'Once the SaleProducer is running [comma] we should see a new Sale event added to the sales topic every second [full_stop] To check this is working we can use the print command in the KSQL client [full_stop] You can run the KSQL command line client from the confluent directory:',\n",
       " '',\n",
       " 'Here we use KSQL to create a stream over the raw sales topic [comma] then do some filtering [comma] just to show its possible [full_stop] In the query Ill select the first 5 sales events from bar number 2',\n",
       " 'Several really cool things are happening here [full_stop] Firstly [comma] we created a stream over the top of a raw topic with a single line of KSQL [full_stop] Secondly [comma] we did a query on it OK [comma] that sounds pretty lame to those of us who have been doing databases for years but this is over the top of Kafka [comma] which until recently was just a queue of stuff [full_stop] All of a sudden [comma] we can query it from a SQL console trust me [comma] its a big deal [full_stop] ',\n",
       " 'OK [comma] lets unpack that requirement a little: First were going to need to load some reference data [comma] which maps beer_id to name [full_stop] Secondly [comma] we need to have a way to manage changes to this reference data [comma] as updates are part of real life [full_stop] Finally [comma] were going to need to do a join [comma] as the beer records arrive [comma] to add the name to the query results [full_stop] ',\n",
       " 'Loading the reference data is done by the BeerProducer object [full_stop] It reads the data from the CSV file [comma] registers a schema and sends each row over in avro format to a topic called beers [full_stop] Click through to the source code to see how that works [comma] but its much the same as with the SaleProducer above [full_stop] ',\n",
       " 'We did three things in the above snippet [full_stop] First we created a stream over the beers topic [full_stop] This stream allows us to query [comma] but has no key column [full_stop] The second statement creates a new stream [comma] based on the first [comma] converting the numeric ID to a string and setting it as the key [full_stop] KSQL can only support string keys at the moment [full_stop] The final step was to create a table [comma] beer_table [full_stop] Tables are very cool as they allow a table-like view over a stream [comma] collapsing rows down to their most recent version [full_stop] Selecting from the table is simple',\n",
       " 'Looks like there are four beers in the dataset with a strength above 10% (which is a silly strength for a beer really [bang] ) [full_stop] Now here comes the clever part [full_stop] In the background Ill edit the CSV data file and change the ABV of the imaginatively titled Pub Beer to 0 [full_stop] 11 [full_stop] Then Ill load the whole dataset again [comma] by posting a whole new set of records to Kafka [full_stop] Now the topic contains two rows for every beer but the table just shows the latest [comma] unique records:',\n",
       " 'Theres the Pub Beer [comma] in amongst the strongest beers and just to show the difference [comma] heres two more queries: one against the table and one against the stream [full_stop] Remember that the table and the stream are generated entirely from the raw kafka topic beers (to which I loaded quite a few duplicates [bang] ) [full_stop] ',\n",
       " 'So now we have a way to load and manage reference data - keeping track of the latest values as they change and tracking/managing versions with minimal effort [full_stop] Last step is to join the event data (sales) to the reference data (beers) and show some readable info as sales appear',\n",
       " 'So [comma] I think the following diagram shows where we are so far [full_stop] Weve loaded live events and reference data [comma] demonstrated how updates to reference data can be managed and shown how these two datasets can be joined within Kafka in real time personally [comma] I am both astonished and in love',\n",
       " 'For me [comma] the table semantics are the killer feature here [full_stop] This blog series is about converting events to models [comma] and these snazzy new tables do much of the heavy lifting in that area [full_stop] Updates to a cache are no longer needed [comma] as the streaming platform handles updates internally [full_stop] ',\n",
       " 'The ability to join and filter in SQL means that defining the rules for creating and updating models is easier too [full_stop] There is less software to write and fewer external applications to build and maintain [full_stop] ',\n",
       " 'In the next instalment Ill look at aggregations and reporting in batch and real time [full_stop] Stay tuned [bang] ',\n",
       " 'Last night I unpacked my new MultiWii controller and plugged it into my PC [full_stop] I bought it from Hobby King and it comes ready to fly out of the box - configured for an X-quad [comma] which is perfect for my carbon H-copter [full_stop] ',\n",
       " 'The Multi-Wii board attracted me because it comes with so many sensors out of the box [full_stop] Barometric pressure [comma] compass [comma] gyros and accelerometers as well as the option to add GPS in future [full_stop] Thats compared to just gyros on the KK board [full_stop] I love the KK board to bits - its a great board for line-of sight flying [comma] allowing a fair amount of acrobatics and some nimble and fast flight [full_stop] Its scary flying FPV with the KK though [full_stop] I managed a good FPV flight at the weekend (see video later) but it would be great to push more of the stability control onto the copter for less stressful remote piloting [full_stop] ',\n",
       " 'I didnt get the board hooked up to the quad or the receiver last night [comma] just plugged it into the PC and fired up the Java tool [full_stop] ',\n",
       " 'I have to say I was very impressed with the board and the UI tool [full_stop] The tool shows a live trace from all of the on-board sensors and a 3D model of the copter which moves in real-time [full_stop] Every single setting is configurable [comma] including the PID terms [comma] throttle travel [comma] behaviour of the auxiliary switch channels and so on [full_stop] ',\n",
       " 'Hardware wise [comma] the barometric pressure sensor was the star - it responds to changes in height of about 10cm [comma] which is spookily accurate [bang] The only worry I have is the effect of wind on this sensor - I am wondering whether a wind shield is going to be needed for breezy days [full_stop] ',\n",
       " 'Down sides so far: The GUI is a bit painful to use [full_stop] About 25% of mouse clicks are ignored or lost (I think because the refresh loop for the graph is running and the app is polling for user input) [full_stop] Also [comma] the numeric values are editted using very very very small sliders [full_stop] It would be much simpler to just enter the text [bang] Maybe these things annoy me more because I do that sort of thing for a living',\n",
       " 'Expect more soon [bang] In the mean time [comma] heres an FPV video from this weekend',\n",
       " '[embedplusvideo height=465 width=584 standard= vars=ytid=sKRGBYjTcPQ&width=584&height=465&start=&stop=&rs=w&hd=1&autoplay=0&react=1&chapters=&notes= id=ep9639 /]',\n",
       " 'Last year I published an annual project review blog post called Stuff I made in 2017 [full_stop] I found writing it to be quite a fun exercise (catharsis perhaps) [full_stop] As a middle-manager by trade [comma] I could easily fall into a trap where I spend my entire life using a sad combination of MS Office [comma] Netflix and XBox - never actually doing anything [full_stop] So [comma] I like to challenge myself to actually make as much stuff as possible both in my spare time and at work [full_stop] This might be as simple as keeping my GitHub commit rate up or fixing something round the house [comma] but I also like to try something challenging every now and again [full_stop] It doesnt always work out [comma] but its always a learning experience here goes',\n",
       " 'The first big project of the year made use of the new router Id got for Christmas and a ton of standard construction timber [full_stop] The girls had decided they fancied sharing a room and wed struggled to find a decent set of bunk beds that met our exact specifications [comma] so I decided to build some [full_stop] ',\n",
       " 'I took far too few photos as I went along [full_stop] Basically I bought Planed Square Edge (PSE) timber from Wickes in various sizes [full_stop] I gave every piece a roundover on all the corners and sanded to 150 grit - which gives it a nice soft feel even without varnish or paint [full_stop] ',\n",
       " '',\n",
       " 'All the joints are a variation on a mortice and tenon [comma] some glued and some pinned with 1/2inch oak dowels [full_stop] There are no screws or nails holding the main structure together at all [bang] I made a couple of jigs to follow with the router which allowed me to create consistent mortices many times over [full_stop] ',\n",
       " 'The jigs are still in the garage [comma] so when the girls want to move back into separate rooms we have the option to make combinations of high sleepers with desks underneath [comma] single beds and whathaveyou [full_stop] ',\n",
       " 'This project almost killed me [bang] Well OK [comma] maybe I was never at risk of actual death [comma] but hard manual labour [comma] coupled with an unexpected cold snap (right after a boozy trip to Minsk) left me off work for over a week [bang] ',\n",
       " '',\n",
       " 'The mission at hand: build a patio in the back garden [comma] right where the confused maze of random [comma] overgrown gravel paths and shrubbery was situated [full_stop] In the middle of the work site theres a nice cherry tree which we wanted to keep [comma] which made the project a bit more exciting [bang] ',\n",
       " '',\n",
       " 'The planter around the tree is made of oak sleepers [comma] joined with through-dowels [full_stop] Getting the dowels in place was much harder than I expected - I bought a huge forstner bit for my drill which bit into the wood and was almost impossible to hold back [bang] The drill almost twisted my hand off [comma] then started to smoke [comma] and in order to get the bit back out again I had to stand on the sleepers and heave the drill back out in reverse [bang] The result is pretty snazzy though and almost a year on [comma] the oak has faded to a nice silver colour too [full_stop] ',\n",
       " '',\n",
       " 'The process of actually building the patio was pretty hard work - especially on my own and over weekends [bang] First I levelled the site [comma] then rented the same cement mixer over three different weekends [comma] mixing 2 tons of cement and lugging a ton of sub-base through the garage to the back garden [full_stop] The trickiest [comma] but most satisfying part of the job was cutting the L-shaped slab pieces to fit around the planter and the tree [full_stop] ',\n",
       " 'A very small project that made me inexplicably happy [full_stop] One afternoon on a warm day in late spring I cobbled together these two boxes from old pallet wood [full_stop] They live under the barbecue and hold offcuts and kindling for the fire [full_stop] Big daughter and I created a cool pirate-themed logo which we stencilled onto the side of the box too [full_stop] ',\n",
       " '',\n",
       " 'One Saturday morning [comma] YouTube decided to recommend a video by a guy in Canada about carving wizards [full_stop] Two hours later I was sat in the garden with a coffee [comma] whittling away on some bunkbed offcuts [full_stop] Since then Ive bought a little carving knife and a few shaped chisels from eBay - nothing expensive [comma] but enough to crank out a wizard or two [full_stop] Heres a selection:',\n",
       " '',\n",
       " 'I started this too late in the summer to get really good at it before the weather prevented further work [full_stop] I can tell you with certainty than sitting in the evening sunshine with a beer [comma] maybe half-participating in a family chat [comma] while sculpting an effigy of a practitioner of the occult arts is exactly as enjoyable as it sounds [full_stop] Its also fun to paint them with your daughters art supplies [bang] ',\n",
       " 'So I built a new patio and developed an affinity for whittling [full_stop] Everything should have been fine and dandy for the rest of the summer [full_stop] Sadly though [comma] our old garden table was really lowering the tone [full_stop] ',\n",
       " '',\n",
       " 'Luckily [comma] it turns out you can buy huge slabs of oak on eBay [full_stop] You can even get free shipping on some of them [bang] Huge slabs of hardwood are not cheap though - the one I bought is badly cupped and brimming with character (i [full_stop] e [full_stop] woodworm [comma] rotten sapwood and a big hole [bang] ) and it still set me back over 200 [full_stop] That sounds pretty harsh - its still a beautiful piece of wood with amazing grain [comma] it just needs more love an attention than I managed to give it so far: basically I just planed [comma] sanded and treated it with Danish oil before the rains set in [full_stop] ',\n",
       " 'The real problem with the table is the legs [full_stop] I hate them [bang] I welded them together myself [comma] added adjustable levelling feet with PTFE ends to protect the patio [comma] accounted for seasonal wood movement and theyre solid as a rock they just dont look right [full_stop] I think this is partly down to the black finish [comma] which reminds me of office furniture [comma] but also they are too heavy looking for a thin-ish slab top [full_stop] Maybe if the tabletop was 6\" thick theyd look OK [full_stop] ',\n",
       " 'It was great fun building the table and I learned a huge amount [full_stop] Plus I have another project in the pipeline for this year [comma] involving yacht varnish and welding :)',\n",
       " 'One of our neighbours is a teaching assistant in foundation at the local school [comma] where small daughter will be starting in September [full_stop] She also happened to have a big summer house/shed thing built in her back garden [full_stop] This led to two things: an abundance of scrap tongue and groove boards and a request for a mud kitchen for the foundation playground [full_stop] Seemed like too good an opportunity to miss [bang] ',\n",
       " '',\n",
       " 'The sink was donated by another parent (my wifes big on the school social scene [bang] ) and the rest is made from recycled or scrap wood I had knocking about [full_stop] Extensive testing took place in our garden and apparently its a hit with the kids at school too [bang] ',\n",
       " 'It would be hard to talk about what I made in 2018 without mentioning picture frames [full_stop] I made a lot of picture frames [full_stop] You see [comma] there is a thing in Karate [comma] and also in Software Engineering [comma] called a Kata: a task you repeat over and over again to hone your skills [full_stop] Practice makes perfect and the repetition of a task builds subconscious muscle memory [comma] making you a powerful warrior [comma] great coder or erm adequate picture framer [full_stop] So I made about 30 picture frames in 2018 [full_stop] I wanted to make 50 [comma] but it didnt pan out that way [full_stop] ',\n",
       " 'Some were made from skuzzy pallet wood [comma] some from copper sheet [comma] old bench slats [comma] whittled scraps of bunkbed [comma] rusty angle-iron there are a few the same and many which are different [full_stop] I dont know if Im any better at framing pictures [comma] but I do know there are a lot of nails in the wall of the stairs to the top room [bang] ',\n",
       " '',\n",
       " 'Ive kind-of borrowed this one from 2019 [full_stop] I started the work in 2018 (on the 29th December [bang] ) and finished a couple of days into the new year [full_stop] Anyway [comma] I needed a place to store all our garden tools [comma] and free up some space in the garage - and still had some leftover wood from the neighbours summer house so',\n",
       " '',\n",
       " 'Its the tallest [comma] thinnest shed youre likely to see [comma] but it fits perfectly into the space we had available and I managed to fit a whole heap of garden gubbins in there [full_stop] Theres something inherently life-affirming about building sheds too',\n",
       " 'All in all [comma] 2018 was a pretty productive year [full_stop] 2019 promises to be just as busy [comma] with a front garden to sort out and a few indoor DIY projects that need doing [full_stop] There are more frames to make and wizards to carve [comma] plus I fancy making some wooden boxes [comma] a workbench for the garage and maybe another shed/wood store for the back garden [full_stop] Super exciting [bang] ',\n",
       " 'Amazon recently launched Athena - their answer to Googles Big Query [full_stop] Its basically an SQL interpreter which runs over files in S3 [full_stop] It reminds me of Apache Drill [comma] but people round the office say it looks more like Hive [full_stop] ',\n",
       " 'The barrier to entry is very low [full_stop] Upload the data files (CSV [comma] Parquet and JSON are supported [comma] amongst others) [comma] define a table [comma] run a query [full_stop] All this is done using a simple query editor [full_stop] ',\n",
       " 'To test Athena I uploaded some Parquet files [comma] containing data from the open house price dataset to an S3 bucket (I had wanted to load the CSV files as is but due to limitations in the CSV reader I couldnt) [full_stop] I then declared a table like so:',\n",
       " 'And a few seconds later were ready to go:',\n",
       " 'The ease of setup in simple cases makes this technology very lightweight [full_stop] If you already have data in S3 [comma] you can just start using Athena straight away [full_stop] Its perfect for ad-hoc querying [comma] sanity checking and QA/test activities [full_stop] ',\n",
       " 'Athena uses a server less model - you pay for the rows you scan - no need to set up a cluster etc [full_stop] At the time of writing [comma] its something like $5 per 1TB of data scanned [full_stop] As with everything on AWS [comma] this is bearable but not exactly cheap [full_stop] ',\n",
       " 'At the time of writing [comma] Athena is very new [full_stop] There are many missing features at the moment [comma] which I hope Amazon will be adding in future [full_stop] ',\n",
       " 'Firstly [comma] CSV read is limited to pure comma-separated data [full_stop] Quotes are not supported [full_stop] This is painfully annoying [comma] as almost all CSV data has quotes around string fields [full_stop] If you have to transform existing CSV data to remove quotes [comma] the cost is going to outweigh any benefit you might have got from doing the direct queries [full_stop] ',\n",
       " 'The other annoyance to me is the lack of options for saving data back to S3 [full_stop] select into and create as select style statements are not (yet) supported [full_stop] This breaks a key use-case for me: the ability to do one-off transforms of legacy or 3rd party data to new file formats [full_stop] Wouldnt it be nice to take a CSV file [comma] uploaded by a 3rd party [comma] change a few field names [comma] transform to parquet (or JSON or whatever) and save back into your data warehouse [eh] Yes it would [full_stop] But you cant [full_stop] Sorry [full_stop] ',\n",
       " 'Athena is pretty good if you want a simple tool for doing basic ad-hoc querying over data stored in S3 - provided that data is in a compatible format [full_stop] ',\n",
       " 'Sadly though [comma] Athena is just not ready for the big time [comma] as yet [full_stop] With the addition of support for more data formats and the ability to save data back to S3 [comma] it could be an incredibly useful tool [comma] but right now I could count the number of use-cases on one hand [full_stop] ',\n",
       " 'One to watch [bang] ',\n",
       " 'Well [comma] 2020 was a funny old year [bang] Sat here in January 2021 [comma] most people are talking about how things are going to improve this year [full_stop] Sitting there in the future reading this [comma] as you likely are [comma] thats probably a bit of an eyebrow-raiser [full_stop] Regardless [comma] 2020 was a year for staying at home [comma] cancelling season tickets and holidays and spending the money on home improvement [full_stop] Its the year builders [comma] plasterers and decorators became the highest paid people in the country - some seemingly building a backlog of jobs to keep them busy until retirement [full_stop] If there hasnt been a zombie apocalypse or third world war [comma] future reader [comma] you probably cant imagine a time when Geoff the electrician didnt drive round in a Bentley [comma] chauffeured by Charles [comma] former chief pilot for British Airways [full_stop] ',\n",
       " 'Anyway [comma] the Taylor family take pride in slavishly following all the social trends [comma] so 2020s DIY objectives centred around redecoration of three rooms: The spare room (now known as Dans Office or The Fortress of Solitude) [comma] the living room and the former play room [comma] soon to be Older Kids bedroom [full_stop] ',\n",
       " 'I think I surprised many in the household when [comma] instead of heading off to Ikea via B&Q to grab some paint and a bunk-bed-in-a-box I donned a welding mask and disappeared into the garage but you see [comma] this years projects turned out to be a complex interdependent web of prerequisites and requirements that took several months to unravel [full_stop] There was a hole in my bucket [comma] and mending it was going to be far more complex than anyone could ever have expected [full_stop] ',\n",
       " 'You see [comma] to sort out my desk I needed to weld up some new legs (not for me [comma] for the desk) [comma] find somewhere to put paperwork and pens and stash all the messy wires [full_stop] Welding up the legs meant cutting some steel box section at a 45 angle; making those cuts meant fixing the band saw [comma] on which the motor had burned out [full_stop] Stashing the wires meant bending some metal [comma] which meant making a metal brake [comma] which meant making some hinges on the lathe [comma] which meant sorting out the lathe table [full_stop] Stashing paper and pens meant building some boxes and drawers [comma] which meant I needed to learn more joinery and make a box joint jig for the router table I made last year [full_stop] None of these things could happen until I tidied the garage and sorted out the half metric ton of scrap wood I had knocking about and obviously wasting wood is a sin [comma] so I had to use it to make some planters for the garden',\n",
       " 'Similarly with the living room: we wanted some custom-sized cupboards under a large TV shelf [comma] and given that I could neither afford nor schedule a carpenter to do this [comma] I had to learn how to make shaker style doors [comma] which meant modifying the table saw to cut rebates and then building several practice doors to get my technique down [full_stop] It seemed pretty essential to make a tool tote to carry all the screws [comma] rawl plugs and whatnot I seemed to constantly need around the house and [comma] obviously [comma] having made all those practice doors [comma] I needed some practice handles (which helped practice the welding for the home-made brackets under the shelves that have sprung up all round the house) [full_stop] ',\n",
       " 'So [comma] as you can see [comma] its been complicated [bang] Short of planting some trees and setting up a foundry in the shed [comma] everythings gone back to first principals in 2020 [full_stop] Throughout all this [comma] my constant companion this year has been sawdust [full_stop] Lots and lots of sawdust [full_stop] ',\n",
       " 'So thats about it [full_stop] Ive achieved either three things or about a hundred things in 2020 [comma] depending on how you look at it [full_stop] Needless to say [comma] as I sit in the fortress of solitude blogging about it [comma] the garage is woefully messy [comma] two more rooms have been added to the list for this year and with the very real prospect of me being back in the office in as little as six months time [comma] the clock is ticking for 2021 [bang] ',\n",
       " 'Below are some details of some of the projects I remembered to take photos of as I went along [full_stop] Help yourself if youre interested [bang] ',\n",
       " 'All the DIY this year meant we got a couple of things delivered on pallets [full_stop] Im past the stage in my life when I drive round industrial estates slinging pallets on the roof rack because they might come in handy [comma] but given that they were already here [comma] I decided to use them to make a box [full_stop] Specifically a box to store all the paperwork that piles up on a desk [full_stop] The only design constraint was that it had to be big enough to store a big stack of A4 [full_stop] ',\n",
       " 'First step with any pallet project is removing about a hundred haphazardly placed nails and turning the resulting scruffy [comma] low quality pine pieces into a small stack of vaguely usable wood [full_stop] ',\n",
       " 'Next job in this case was to make and use a box joint jig to join the four sides [full_stop] I was super happy with the results here [full_stop] Pine [comma] being a soft wood [comma] is really forgiving when using joinery like this [comma] because if you keep the joints tight the wood will squish a little bit and fill the gaps [full_stop] ',\n",
       " 'The grain pattern is really nice on some of the sides of the box [comma] but the mix of wood species means the colours are pretty mismatched [full_stop] I fixed this with some really dark ebony wood stain (I think its basically India ink) which allows the grain to shine through [full_stop] I also added some twiddly bits around the edges and some brass hinges and handles [full_stop] I was super happy with the results [comma] but the kids first comment was Daddy [comma] why have you made a coffin [full_stop] I guess its good to know that [comma] if we ever have a family pet that needs a funeral [comma] Ill just need to find somewhere else to put the gas bill [full_stop] ',\n",
       " 'Since I spend most of my time sitting here at the moment - on Zoom calls [comma] doing work [comma] playing games and writing blog posts - my desk in the fortress of solitude has been a big focus for my maker activities this year [full_stop] ',\n",
       " 'I started with the basics: sorting out the height of the desk was important [comma] as I started 2020 with a bad back [comma] I needed the height of the desk and the monitor to be right for keeping me upright [full_stop] I also needed to do something about the tangle of wires [comma] chargers and plugs that tend to litter every desk these days [full_stop] ',\n",
       " 'The legs are welded together from steel box section at a height that makes me happy [full_stop] Theyre so over engineered I suspect I could park a car next to my macbook without any major issues [full_stop] ',\n",
       " 'The wires all get bundled into a simple metal tray [comma] which sits under the desk [full_stop] The tray itself is made from the side of an old filing cabinet and I added a flush mains socket and USB to the desktop itself [comma] which means I can plug in my laptop and charge up phones [comma] keyboards and whatnot without excessive clutter [full_stop] ',\n",
       " 'Making two perfect right angle bends in a very large chunk of metal is a challenge [full_stop] In the end [comma] I welded together a simple metal brake [comma] which clamps the sheet in place and hinges along the edge [full_stop] Its very heavy duty [comma] pivoting around a 6mm bolt and made from thick steel [full_stop] In all honesty [comma] Im not 100% happy with how crisp the bends turn out - but its a damn sight better than using a hammer [bang] ',\n",
       " 'The desk was the biggest metalwork project of 2020 and I am super happy with the results [full_stop] The desktop itself [comma] by the way [comma] is the family dining table from the parents house [full_stop] It was probably bought before I was born and still has scars from my childhood days [full_stop] ',\n",
       " 'These little drawers now live on my desk [comma] where I use them to store pens [comma] printer ink and general desktop junk [full_stop] Theyre made from oak [comma] with pine/plywood drawer boxes and finished with beeswax [full_stop] This was my first joinery project working with oak (having made a couple of tabletops in the past) and it was a huge step up from working with softwoods [full_stop] Oak is expensive [comma] brittle and unforgiving [comma] but on the plus side it smells amazing [comma] looks fantastic and will last for a hundred years [full_stop] ',\n",
       " 'The photos cant capture my favourite feature of these little drawers: the way they feel [full_stop] I sanded them to 320 grit (which isnt crazy fine or anything) and finished them with a bog standard beeswax polish [comma] but even so [comma] they feel amazing [full_stop] Its hard to explain [comma] but theres something truly unique about the feel of polished oak [full_stop] ',\n",
       " 'I cant tell you how handy this little caddy/tote thing is [full_stop] When doing DIY round the house - putting up shelves for example - I always ended up running backwards and forwards grabbing drill bits [comma] rawl plugs [comma] pliers and whatever [full_stop] Now [comma] when theres a job to do [comma] I just grab the tote [comma] throw in the things I need and off I go [full_stop] Super useful [full_stop] Pirate logo optional [full_stop] ',\n",
       " 'I made it from some spare/scrap wood I had knocking around after the living room project [full_stop] Of course [comma] it would have been really handy when doing the living room project but there you go [full_stop] ',\n",
       " 'Two major projects in the workshop to report on this year [full_stop] First was a new motor for the band saw [full_stop] The old one gave up the ghost last year - Partly because it was getting pretty old and partly because worn-out bearings and piles of swarf and sawdust had made it harder and harder to move the blade [full_stop] ',\n",
       " 'As well as fitting a lovely 1/2HP motor I swapped out the bearings and gave everything a good clean [full_stop] The saw should be good for a few more years now [full_stop] ',\n",
       " 'Second project in the garage was a new table for the lathe [full_stop] Using the now ubiquitous scaffold boards and 25x50mm square steel bar [comma] along with some old filing cabinet drawers [comma] I managed to weld this up in a day [full_stop] The drawers stick a little [comma] but storage has been greatly increased and using the lathe doesnt give me a bad back any more [full_stop] ',\n",
       " 'A couple of years ago I made a bunkbed [comma] so the girls could move into a room together [full_stop] Theyre bigger now (I know right [bang]  [eh] ) and after their own space [comma] so elder daughter decided she wanted to move to the tiny box room [full_stop] ',\n",
       " 'This meant squeezing a desk [comma] bed [comma] wardrobe and bookshelves into a tiny 2x2m space [full_stop] The only sensible solution was a high sleeper with bed built in underneath [full_stop] ',\n",
       " 'Thanks to a new router and much more care [comma] attention and experience [comma] the joinery on the new bed is way better than the old one [full_stop] I also bought an HVLP spray gun to paint it - which was a massive hassle [comma] but better than using a brush [full_stop] I hate painting [full_stop] ',\n",
       " 'Daughter seems happy with the results - and has as much space as she needs for art [comma] homework and spending 18 hours a day watching gamer videos on YouTube We did buy her a proper chair eventually too [bang] ',\n",
       " '2020 was the year I learned to make a shaker style door [full_stop] Largely down to the thousands of excellent tutorials on YouTube [comma] this went pretty well [full_stop] My first few practice doors [comma] made from scrap shed pieces and installed in the garage [comma] were a bit wonky [comma] but by the time I came to do the ones in the living room [comma] I was getting pretty decent results [bang] ',\n",
       " 'Obviously I also had to make some handles for the garage - which as I said above [comma] became the inspiration for various shelf brackets round the house [full_stop] Rebar is a great material to work with [comma] though it does need some serious muscle if you plan on bending it [comma] as I did here [full_stop] ',\n",
       " 'The living room is entirely DIY: shelves [comma] bench [comma] cupboards [comma] brackets basically everything except the dining table and sofa [full_stop] Pretty happy with the results - you just have to like green [full_stop] ',\n",
       " 'Just finished a new page for my Information Radiator [full_stop] This one shows the progress of user stories through the process [full_stop] We do scrum and this is a kanban-style board so Im mixing things up a little bit but I think its fine [full_stop] ',\n",
       " 'Most Salesmen understand waterfall [full_stop] They promise a feature to a customer within set timescales to get cash [full_stop] Thats what salesmen will always do [full_stop] Of course [comma] life gets in the way and something pushes their new feature back [full_stop] In waterworld they get a nice new Gantt chart which shows whats been added into the project [comma] they understand why its there and accept it [full_stop] The Kanban board is an agile answer to the same question: Why are you not working on the trans-galactic hyperdrive I promised [eh] Oh [comma] its because youre fixing a radiation leak in the cosmic flange-o-tron [full_stop] I agree thats important and I guess you should fix it before you do my feature [full_stop] ',\n",
       " '',\n",
       " 'The data is pulled from TFS [full_stop] I dont differentiate between development and QA on the board because I think it sends the wrong message - were just one big team [comma] after all [bang] ',\n",
       " '',\n",
       " 'Hopefully this board will help those outside of the development and product teams get a better understanding of what were working on now [comma] what weve finished [comma] what were doing next and why [full_stop] Knowing the answers to those questions gives people a warm and fuzzy glow inside and helps ease inter-departmental tensions [full_stop] ',\n",
       " 'So the new version of Arch Linux doesnt have runlevels [comma] rc [full_stop] d or any of that nonsense any more [full_stop] It just has systemd [full_stop] Super simple if you know how to use it [comma] but a right pain in the backside if you dont [full_stop] ',\n",
       " 'I have a little serial GPS module hooked up to my Raspberry Pi via the hardware serial port (ttyAMA0) [full_stop] My old instructions for getting this to work arent much use any more [full_stop] Heres the new procedure for getting serial data with the minimum of fuss:',\n",
       " '1 [full_stop] Disable serial output during boot',\n",
       " 'Edit /boot/cmdline [full_stop] txt using your favourite editor [full_stop] I like nano these days [full_stop] ',\n",
       " 'Remove all chunks of text that mention ttyAMA0 but leave the rest of the line intact [full_stop] Bits to remove look like:',\n",
       " '2 [full_stop] Disable the console on the serial port',\n",
       " 'This was the new bit for me [full_stop] The process used to involve commenting out a line in /etc/innitab but that file is long gone [full_stop] ',\n",
       " 'Systemd uses links in /etc to decide what to start up [comma] so once you find the right one [comma] removing it is easy [full_stop] You can find the files associated with consoles by doing:',\n",
       " 'One of the entries clearly refers to ttyAMA0 [full_stop] It can be removed using the following command:',\n",
       " '3 [full_stop] Check youre getting data',\n",
       " 'I used minicom for this as its very simple to use [full_stop] First of all [comma] make sure you plug in your device (with the power off [comma] if youre as clumsy as me [bang] ) [full_stop] ',\n",
       " 'You should see a lovely stream of data [full_stop] I my case it was a screen full of NMEA sentences [full_stop] Great stuff [bang] ',\n",
       " 'Ive been playing around with Apache NiFi in my spare time (on the train) for the last few days [full_stop] Im rather impressed so far so I thought Id document some of my findings here [full_stop] ',\n",
       " 'NiFi is a tool for collecting [comma] transforming and moving data [full_stop] Its basically an ETL with a graphical interface and a number of pre-made processing elements [full_stop] Stuffy corporate architects might call it a mediation platform but for me its more like ETL coding with Lego Mindstorms [full_stop] ',\n",
       " 'This is not a new concept - Talend have been around for a while doing the same thing [full_stop] Something just never worked with talend though [comma] perhaps they abstracted at the wrong level or prerhaps they tried to be too general [full_stop] Either way [comma] the difference between Talend and NiFi is like night and day [bang] ',\n",
       " '',\n",
       " 'So I dont have access to a huge amount of big data on my laptop [comma] and Ive done articles on MOT and National Rail data recently [comma] so I decided to use a couple of gigs of Garmin Track data to test NiFi [full_stop] The track data is a good test as its XML: exactly the sort of data you dont want going into your big data system and therefore exactly the right use-case for NiFi [full_stop] ',\n",
       " 'The only data in the file Im particularly interested in is where I went [full_stop] The calorie counts and suchlike are great on the day [comma] but dont tell us much after the fact [full_stop] So [comma] the plan is to extract the Latitude and Longitude fields from the Track element [full_stop] Everything else is just noise [full_stop] ',\n",
       " 'NiFi uses files as the fundamental unit of work [full_stop] Files are collected [comma] processed and output by a flow of processors [full_stop] Files can be transformed [comma] split or combined into more files as needed [full_stop] The links between processors act as buffers [comma] queuing files between processing stages [full_stop] ',\n",
       " '',\n",
       " 'The first part of the flow gathers the XML files from their location on disk (since Garmin charge an obcene amount for access to your own data via their API) [comma] splits the XML into multiple files then uses a simple XPath expression to extract out the Latitude and Longitude [full_stop] ',\n",
       " 'A GetFile processor reads whole XML file [full_stop] Next a SplitXml processor takes the XML in each file and splits into multiple files by chopping the XML at a secified level (in this case 5) making a set of new files [comma] one per TrackPoint element [full_stop] Following that [comma] an EvaluateXPath processor extracts the Lat and Long and stores them as attributes on each individual file [full_stop] ',\n",
       " '',\n",
       " 'The rather naive XML split will return all elements at the specified level within the document tree [full_stop] XPath is fine with that [comma] it will either match a Lat and Long or it wont [full_stop] The issue is that well end up with a large number of files where no location was found [full_stop] The RouteOnAttribure process can be used to discard all these empty files [full_stop] Settings shown below:',\n",
       " '',\n",
       " 'So [comma] now we have a stream of files (actually empty files [bang] ) each of which is decorated with attribues for Latitude and Longitude [full_stop] The last part of the flow is all about saving these to a file [full_stop] ',\n",
       " '',\n",
       " 'The first processor in this part of the flow takes the attributes of each file and converts them to JSON [comma] dropping the JSON string into the file body [full_stop] We could just save the file at this stage [comma] but that would be a lot of files [full_stop] The second block takes a large number of single-record JSON files and joins them together to create a single line-delimited JSON file which culd be read by something like Storm or Spark [full_stop] I had all sorts of trouble escaping a carriage return within the MergeContent block [comma] so in the end I stored a carriage return character in a file called ~ ewLine [full_stop] txt and referenced that in the processor settings [full_stop] Not pretty [comma] but it works [full_stop] The last block in the flow saves files - not much more to say about that [bang] ',\n",
       " 'It took a little over one train journey to get this workflow set up and working and most of that was using Google [bang] Compared to using Talend for the same job it was an abslute dream [bang] ',\n",
       " 'Perhaps the only shortcoming of the system is that it cant do things like aggregations - so I cant convert the stream of locations to a binned map wit counts per 50x50m square for example [full_stop] De-duplication doesnt seem possible either but if you think about how these operations would have to be implemented [comma] you realise how complicated and resource hungry they would make the system [full_stop] If you want to do aggregations [comma] de-duplication and all that jazz [comma] you can plug NiFi into Spark Streaming [full_stop] ',\n",
       " 'Most data integration jobs Ive seen are pretty simple: moving data from a database table to HDFS [comma] pulling records from a REST API [comma] downloading things from a dropzone and for all of these jobs [comma] NiFi is pretty much perfect [full_stop] It has the added benefit that it can be configured and maintaned by non-technical people [comma] which makes it cheaper to integrate into a workflow [full_stop] ',\n",
       " 'I like it [bang] ',\n",
       " 'Have been working on The Dukes brain today [full_stop] Needed to get the mobile internet connection to start automatically when the Raspberry Pi boots [full_stop] Im using a Huawei E220 dongle with a Giff Gaff SIM [full_stop] The easiest way to get it connected to the internet is to use the fantastic sakis3g script [full_stop] ',\n",
       " 'The best way to get anything to run at startup is with an rc script [comma] but even after searching for some time I couldnt find any examples [full_stop] So heres one I crafted myself [comma] in case anyone finds it useful [full_stop] Obviously youll have to change the ARGS line to set the correct APN and so on for your connection [full_stop] Youll also need to copy the sakis3g script itself to /usr/sbin [full_stop] ',\n",
       " '/etc/rc [full_stop] d/sakis:',\n",
       " 'This can be started from rc [full_stop] conf by adding sakis toi your daemons line [full_stop] ',\n",
       " '/etc/rc [full_stop] conf',\n",
       " 'There are some downsides to using sakis3g for connecting to the internet though - especially for my project [full_stop] I have both a 3g and a WiFi dongle attached to the RaspberryPi [comma] but sakis sets up the routes in such a way once youre connected via the mobile network [comma] you always route that way [full_stop] Even though the wireless is faster [comma] when I run a package upgrade or download a file it comes over 3g [comma] not WiFi [full_stop] Worse than that [comma] if you stop sakis [comma] it doesnt restore your routes [comma] so you cant connect to the internet at all without issuing some route commands [full_stop] ',\n",
       " 'I really liked my old motor mounts for the Quadrotor [full_stop] I made them on the lathe out of Engineering Nylon which costs very little [comma] is easy to machine and can be found on eBay (search for nylon round bar) [full_stop] The motor snuggly fits into a 9mm hole and is fixed with two 3mm hex head set screws from either side [full_stop] Another 3mm set screw goes through the bottom and bolts the mount onto the frame [full_stop] To stop the torque from the motor spinning the mount I milled a 10mm wide slot into the bottom which fits over the aluminium frame [full_stop] ',\n",
       " 'I liked them because they are light [comma] look nice and form an integral part of the frame (so no extra cable ties and fixings [comma] just one bolt to do everything) [full_stop] The big problem made itself very apparent when I had a crash on the airbase; the motor hit the ground first and because there was no give in the motor mount I bent the arm and broke the motor (detaching one of the magnets) [full_stop] ',\n",
       " '',\n",
       " 'This would never have happened in the old days - when the factory motor mount was attached to the frame with cable ties [full_stop] It might have been ugly [comma] but the cable ties would snap long before any other damage could be done [full_stop] ',\n",
       " 'So [comma] how could I improve my nice looking purpose built motor mounts to add a weak point [eh] The answer is in the next picture:',\n",
       " '',\n",
       " 'Basically its exactly the same design [comma] but with an added cable tie [bang] This time I made the mounts in two parts: a base part which bolts securely to the frame and a motor part which sits on the base and is held down by a cable tie [full_stop] Theres a little knob on the motor part which sit in the 5 [full_stop] 5mm hole in the base part (at the bottom of which the hex-head of the set screw goes) [full_stop] This keeps the two parts locked together as long as the cable tie is in place [full_stop] ',\n",
       " 'In a crash the cable tie will snap and the motor part will detach - hopefully protecting the motor from damage and protecting the frame in the event of a motor-first crash [full_stop] ',\n",
       " '',\n",
       " 'Have I tested them yet [eh] Well [comma] no [full_stop] Im not going to crash on purpose [bang] ',\n",
       " 'Woke up early and met Dr Johnson just after dawn at Greenham Common [full_stop] The control tower car park was closed so we stopped in a lay-by with the dog walkers and wandered over to the north taxiway [full_stop] The ground is very wet and I am hopelessly out of practice with FPV so I stuck to line-of-sight flying [full_stop] Turns out I am also out of practice at that as after about 10 minutes I got disoriented and crashed [comma] snapping a leg off and ending the festivities for the morning [full_stop] ',\n",
       " 'Since flying was out of the question [comma] we decided to go on a survey mission round the base [full_stop] It was my first time on the airbase so was quite interesting to wander over to the bunkers and stare through the fence [full_stop] The runway has been torn up and bulldozed into lumps to stop planes landing [comma] so where the mighty B52s once rumbled in to land theres now just gravel and gorse bushes [full_stop] Saw the odd cow and some horses too [comma] since the common is now common land again [full_stop] ',\n",
       " 'Here are some photos and a video from the copter [full_stop] I am somewhat annoyed that a drop of water got on the lens (again [bang] ) and smudged up the image [full_stop] We shall return to the base soon I hope [full_stop] ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Spark is a big deal these days [comma] people are using this for all sorts of exciting data wrangling [full_stop] Theres a huge trend for ease of use within the Spark community and with tools like Apache Zeppelin coming onto the scene the barrier to entry is very low [full_stop] This is all good stuff: open source projects live and die in the first half an hour of use [full_stop] New users need to get something cool working quickly or theyll get bored and wander off',\n",
       " 'But for those of us who got past Hello World some time ago and are now using Spark as the basis of large and important projects theres also the chance to do things right [full_stop] In fact [comma] since Spark is based on a proper language (Scala [comma] not R or python please [bang] ) its a great chance to bring some well established best practices into a world where uncontrolled script hackers have held sway for too long [bang] ',\n",
       " 'Check out the source for this article on my GitHub: ',\n",
       " 'Behaviour Driven Development [comma] or BDD [comma] is a bit like unit testing [full_stop] Like unit testing done by an experienced master craftsman [full_stop] On the surface they look the same - you write some test code which calls your production code with known inputs and checks the outputs are what you want them to be [full_stop] It can be run from your IDE and automated in your CI build because it uses the same runner as your unit tests under the hood [full_stop] ',\n",
       " 'For me [comma] TDD and BDD differ in these two critical ways: BDD tests at the right level; because youre writing Specifications in pseudo-English not Tests in code you feel less inclined to test every function of every class [full_stop] You test at the external touch-points of your app (load this data [comma] write to this table [comma] show this on the UI) [comma] which makes your tests less brittle and more business oriented [full_stop] Which leads to the second difference: BDD specs are written in Cucumber [comma] a language easily accessible to less techie folks like testers [comma] product owners and stakeholders [full_stop] Because Cucumber expresses business concepts in near-natural language [comma] even your Sales team have a fighting chance of understanding it well [comma] maybe [full_stop] ',\n",
       " 'Before we can crack on and write some Cucumber [comma] there is some setup to be done in the project [full_stop] I am using IntelliJ [comma] but these steps should work for command line SBT also [full_stop] ',\n",
       " 'First job [comma] get build [full_stop] sbt set up for Spark and BDD:',\n",
       " 'For this example I am wrapping Spark up in an object to make it globally available and save me mocking it out properly [full_stop] In a production app [comma] where you need tighter control of the options you pass to spark [comma] you might want to mock it out and write a Given to spin Spark up [full_stop] Heres my simple object in Spark [full_stop] scala:',\n",
       " 'If using IntelliJ [comma] like me [comma] youll also need a test class to run your cucumber [full_stop] Mines in Runtests [full_stop] scala [full_stop] Right click on this and select Run tests from the context menu and itll run the tests [full_stop] ',\n",
       " 'If using the command line [comma] add this line to project/plugins [full_stop] sbt:',\n",
       " 'And these to build [full_stop] sbt:',\n",
       " 'Heres the first bit of actual cucumber [full_stop] Were using it for a contrived word-counting example here [full_stop] The file starts with some furniture [comma] defining the name of the Feature and some information on its purpose [comma] usually in the format In order to achieve some business aim [comma] As the user or beneficiary of the feature [comma] I want some feature [full_stop] ',\n",
       " '[code lang=gherkin] Feature: Basic Spark',\n",
       " 'In order to prove you can do simple BDD with spark As a developer I want some spark tests',\n",
       " 'Scenario: Count some words with an RDD When I count the words in the complete works of Shakespeare Then the number of words is 5 [/code]',\n",
       " 'The rest of the file is devoted to a series of Scenarios [comma] these are the important bits [full_stop] Each scenario should test a very specific behaviour [comma] theres no limit to the number of scenarios you can define [comma] so take the opportunity to keep them focussed [full_stop] As well as a descriptive name [comma] each scenario is made of a number of steps [full_stop] Steps can be Givens [comma] Whens or Thens [full_stop] ',\n",
       " 'Each step is bound up to a method as shown in the Steps class below [full_stop] When the feature file is executed the function bound to each step is executed [full_stop] You can pass parameters to steps as shown here with the input string and the expected number of words [full_stop] You can re-use steps in as many scenarios and features as you like [full_stop] Note that the binding between steps and their corresponding functions is done with regular expressions [full_stop] ',\n",
       " 'The Context object here is used to store things any variables needed by the steps [full_stop] You could use private fields on the step classes to achieve this [comma] but youd quickly encounter problems when you began to define steps over multiple classes [full_stop] ',\n",
       " 'I dont particularly like using a Context object like this [comma] as it relies on having vars [comma] which isnt nice [full_stop] If you know a better way [comma] please do let me know via the comments box below [bang] ',\n",
       " 'So the word counting example above shows how we can do BDD with spark - we pass in some data and check the result [full_stop] Great [bang] But its not very real [full_stop] The following example uses Spark DataFrames and Cucumber DataTables to do something a bit more realistic:',\n",
       " 'You only need to write the code to translate the data tables defined in your cucumber to data frames once [full_stop] Heres my version:',\n",
       " 'Likewise [comma] you can define a function to compare the output data frame with the expected data from the cucumber table [full_stop] This is a simple implementation [comma] I have seen some much classier versions which report the row and column of the mismatch etc [full_stop] ',\n",
       " 'Theres a great coverage plugin for Scala which can easily be added to the project by adding a single line to plugins [full_stop] sbt:',\n",
       " 'The report is generated with the following SBT command and saved to HTML and XML formats for viewing or ingest by a tool (like SonarQube) [full_stop] ',\n",
       " '',\n",
       " 'So [comma] hopefully this long and rambling article has made one key point: You can use BDD to develop Spark apps [full_stop] The fact that you should isnt something anyone can prove [comma] its just something youll have to take on faith [bang] ',\n",
       " 'This evening I was innocently setting up a wireless dongle for my darling wife [full_stop] I casually typed in the address of this very web page into her browser to check it was working [comma] only to find that all the posts were missing [bang] ',\n",
       " '404 errors on every page but the front page [full_stop] Poo [bang] I desperately dived in to the wordpress settings [comma] everything was set up fine [full_stop] I updated wordpress but it made no difference [full_stop] In the end [comma] I went back to the post URL settings and clicked Apply again [full_stop] It fixed the problem [bang] ',\n",
       " 'Looking at the stats [comma] Logical Genetics seems to have been off the air since Independence Day [full_stop] Almost a month [full_stop] Miserable [full_stop] ',\n",
       " 'Back now though [comma] and soon to be posting an article on my Build Status Traffic Lights [full_stop] ',\n",
       " 'Popped to the Green last night to test out the 6-way tricopter from my last post [full_stop] Something was horribly wrong with the control system and after the second massive crash an arm snapped off [full_stop] I doubt Ill bother to rebuild the stupid thing [bang] ',\n",
       " '',\n",
       " 'The second crash also took a blade off The Firegoat - Dr Johnsons newly completed quad [full_stop] ',\n",
       " '',\n",
       " 'Flying a quad is a totally different kettle of fish [full_stop] Much more docile and easy to control it just hangs about in the air [full_stop] The tricopter is very much more acrobatic - able to spin 360 degrees in about a second and with (I am told) a very helicopter-like feel [full_stop] The quad feels slower and more docile with very sluggish rudder perfect for learning to fly [comma] carrying a GoPro and learning FPV [full_stop] ',\n",
       " 'Inspired by this [comma] and the never ending quest to minimise weight [comma] this months man budget went on a selection of carbon fibre tubes and sheets to make a quad and hopefully Tricopter 4 [full_stop] 0 [full_stop] The 6-Way project will be put on hold for now I think [bang] ',\n",
       " 'You can find the code for this article on my github: ',\n",
       " 'Having found myself time-wealthy for a couple of weeks Ive been playing around with some open data sets [full_stop] One of which is the National Rail SOAP API [full_stop] Its not a new dataset [comma] I think its been around for a decade or so [comma] but it seemed like a sensible thing for me to play with as Ill be on trains a lot more when I start selling my time to a new employer next month [bang] ',\n",
       " 'I live about a mile away from the local station (Thatcham) so it only takes a few minutes to get there [full_stop] If a train is delayed or cancelled Id like to know so I can have another coffee [full_stop] So what I need is a live departures board [comma] for my local station [comma] in my house [full_stop] Something like this:',\n",
       " '',\n",
       " 'The UI is web based - using AngularJS again [full_stop] Sadly though [comma] the cross origin nonsense means I cant make the soap calls directly from the web client [comma] I need a back-end to gather and store the data for use on the UI [full_stop] I used Python for this because that gives me the option to (easily) run it on a Raspberry Pi [comma] reducing power and space costs as well as noise [full_stop] Pythons library support is stunning [comma] and this is another great reason to use it for small hacks like this one [full_stop] ',\n",
       " 'SOAP is horrible [full_stop] Its old [comma] its heavy [comma] its complex and worst of all its XML based [full_stop] This isnt a huge handicap though [comma] as we can hit a SOAP service using the requests HTTP library - simply sending a POST with some XML like so:',\n",
       " 'So [comma] I take a pre-formatted XML request [comma] add the key and station code then POST it to the API URL [full_stop] Easy [full_stop] The result comes back in XML which can be parsed very easily using the Postman to test the calls before translating to Python (and if youre lazy [comma] Postman will even write the code for you [bang] ) [full_stop] ',\n",
       " 'The Python script takes the data its gathered and stores it in an SQLite database [full_stop] Im not going to show the code because its all in github anyway [full_stop] ',\n",
       " 'So the data is all in a DB [comma] now it needs to be made available to the Javascript client somehow [full_stop] To do this I created a simple REST service using the excellent microservice frameworks I love to use [full_stop] Heres all the code you need to serve up data via REST:',\n",
       " 'The front end is a very simple Angular JS app [full_stop] Not much point showing the code here (see Bootswatch [full_stop] ',\n",
       " 'The design is based on a real life station departures board like this: ',\n",
       " 'All in all the project took me a little over a day [full_stop] A leisurely day with many interruptions from my daughters [bang] Feel free to pull the code down and play with it - let me know what you do [full_stop] ',\n",
       " '',\n",
       " 'Today [comma] logicalgenetics [full_stop] com and danandtheduke [full_stop] co [full_stop] uk merged to make one Super-Blog [bang] It seemed daft having two websites and two blogs [comma] so Ive pulled all the Land Rover posts into the Logical Genetics blog and pointed both addresses right here [full_stop] You dont need to change your bookmarks or format your brain - both logicalgenetics [full_stop] com and danandtheduke [full_stop] co [full_stop] uk still work fine [full_stop] ',\n",
       " 'The old Land Rover blog wont see any more action - Ill be posting everything here [full_stop] Youll still get all the mission reports and photos of mechanicals [full_stop] Youll also get weird posts about Software [comma] Machine Tools [comma] Multi-Rotors and model flight and [comma] well [comma] just about anything else I can dream up [full_stop] ',\n",
       " 'If you just want to read about Land Rovers or Adventures [comma] use the Blog Posts menu at the top to choose your poison [full_stop] ',\n",
       " 'Enjoy [bang] ',\n",
       " 'Just been out to do a quick 20 minute flight with the MultiWii plugged in [full_stop] Verdict is overall a good one [comma] though to be honest it flew better with everything but the gyros disabled [bang] ',\n",
       " 'Started off with some setup trouble [full_stop] Though I was sure Id got all the motors spinning up at the right time on the bench last night [comma] we ended up swapping a couple of the motor cables over and tweaking around with motor directions [full_stop] I also needed to reverse my rudder channel (in addition to the ailerons) but that wasnt a big deal [full_stop] ',\n",
       " 'Flying with just the gyros enabled seemed fine [full_stop] Slightly different feel in the sticks but that was probably due to the different rates and expo settings [full_stop] It seemed very stable - in fact it was stunningly locked in on the yaw axis [full_stop] ',\n",
       " 'Flipping the switch to turn on the accelerometers [comma] magnetometer (compass) and barometric pressure sensor made a big difference too [full_stop] I terms of altitude hold [comma] the pressure sensor seemed to make things worse - there was a large [comma] slow oscillation up and down which needed to be counteracted on the throttle in a very unnatural sort of way [full_stop] As far as heading hold was concerned I didnt notice any change [full_stop] With the compass off there was no movement on the yaw and there was still no movement with it enabled [full_stop] In terms of pitch and roll [comma] the accelerometers did seem to lock this in more [comma] but tended to give a drift backwards and to the right [full_stop] Compensating this with the trims on the transmitter didnt seem to work too well [full_stop] ',\n",
       " 'I think there is more calibration on the cards [bang] ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_dan_paragraphs():\n",
    "    for filename in glob.glob('/Users/dan/Development/dantelore/hugo/public/posts/*/*.html'):\n",
    "        html = None\n",
    "        with open(filename) as f:\n",
    "            html = f.read()\n",
    "        if html:\n",
    "            soup = BeautifulSoup(html, features='html.parser')\n",
    "\n",
    "            # Remove code and preformatted blocks\n",
    "            for x in soup.findAll('code'):\n",
    "                x.extract()\n",
    "\n",
    "            # Remove footers\n",
    "            for x in soup.findAll('footer'):\n",
    "                x.extract()\n",
    "\n",
    "            # Remove header/nav\n",
    "            for x in soup.findAll('nav'):\n",
    "                x.extract()\n",
    "\n",
    "            # Remove post list\n",
    "            for x in soup.findAll('ul', {'id': 'post-list'}):\n",
    "                x.extract()\n",
    "\n",
    "            for p in soup.findAll('p'):    \n",
    "                text = p.get_text()\n",
    "\n",
    "                # Remove long whitespaces\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "                # Remove non ASCII chars\n",
    "                text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "                # Remove urls\n",
    "                text = re.sub(r'http(s?)[^\\s]+', '', text)\n",
    "\n",
    "                # Remove newlines\n",
    "                text = text.replace('/n', ' ')\n",
    "\n",
    "                yield text\n",
    "\n",
    "\n",
    "def get_dan_sentences():\n",
    "    for text in get_dan_paragraphs():\n",
    "        sentences = [x for x in re.split(r'[\\.\\?\\!;:]+\\s+', text) if len(x) > 10]\n",
    "        for s in sentences:\n",
    "            yield s.strip()\n",
    "\n",
    "\n",
    "punctuation = {\n",
    "    r'\\.\\s*': ' [full_stop] ',\n",
    "    r'\\,\\s*': ' [comma] ',\n",
    "    r'\\!\\s*': ' [bang] ',\n",
    "    r'\\?\\s*': ' [eh] '\n",
    "}\n",
    "\n",
    "def get_dan_chunks():\n",
    "    for text in get_dan_paragraphs():\n",
    "        chunk = text\n",
    "        for f, r in punctuation.items():\n",
    "            chunk = re.sub(f, r, chunk)\n",
    "            \n",
    "        yield chunk\n",
    "\n",
    "sentences = list(get_dan_chunks())\n",
    "\n",
    "sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "This approach extracts full sentenses from the input data - then converts these into n-grams for training.  All punctuation is removed.  This is probably not the best way to do this - as it's stopping the model understanding what a sentence is.  \n",
    "\n",
    "Better to include key punctuation marks like full stop, comma, hyphen, colon etc as words/tokens in and of themselves.  This way, the model will learn to add sentence structure - and there will be an easier way to stop reading from the output - rather than just getting a set number of words, we could stop after the nth full stop, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>297</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46053</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>114</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>5393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46054</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>114</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>5393</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46055</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>114</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>5393</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46056</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>114</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>5393</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46057</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>114</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>5393</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2707</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46058 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1   2    3    4    5    6     7     8     9     10    11\n",
       "0       0   0   0    0    0    0    0     0     0     0     1    52\n",
       "1       0   0   0    0    0    0    0     0     0     1    52    13\n",
       "2       0   0   0    0    0    0    0     0     1    52    13    14\n",
       "3       0   0   0    0    0    0    0     1    52    13    14   297\n",
       "4       0   0   0    0    0    0    1    52    13    14   297    11\n",
       "...    ..  ..  ..  ...  ...  ...  ...   ...   ...   ...   ...   ...\n",
       "46053   0   0   0    0    0    0    9   114    57    11    50  5393\n",
       "46054   0   0   0    0    0    9  114    57    11    50  5393    17\n",
       "46055   0   0   0    0    9  114   57    11    50  5393    17     1\n",
       "46056   0   0   0    9  114   57   11    50  5393    17     1  2707\n",
       "46057   0   0   9  114   57   11   50  5393    17     1  2707    21\n",
       "\n",
       "[46058 rows x 12 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=2500, oov_token='<OOV>')\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1\n",
    "\n",
    "max_length = 12\n",
    "\n",
    "input_sequences = []\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('./data/nlp_models/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "for line in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[max(i-max_length, 0):i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_length, padding='pre'))\n",
    "\n",
    "xs = input_sequences[:,:-1]\n",
    "labels = input_sequences[:,-1]\n",
    "# One-hot encode the output, as the words are categorical\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "# Wrap in a Pandas Dataframe just for nicer display\n",
    "pd.set_option('display.min_rows', 10)\n",
    "pd.DataFrame(input_sequences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/Development/simple-prediction/.venv/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 200, input_length=max_length - 1))\n",
    "model.add(Bidirectional(LSTM(500)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run 0/1\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 15:08:38.161417: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-19 15:08:38.450884: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-19 15:08:38.465486: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-19 15:08:38.545063: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-19 15:08:38.567796: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440/1440 - 36s - loss: 6.4333 - accuracy: 0.1183 - 36s/epoch - 25ms/step\n",
      "Epoch 2/10\n",
      "1440/1440 - 33s - loss: 5.6354 - accuracy: 0.1527 - 33s/epoch - 23ms/step\n",
      "Epoch 3/10\n",
      "1440/1440 - 34s - loss: 4.8720 - accuracy: 0.1805 - 34s/epoch - 23ms/step\n",
      "Epoch 4/10\n",
      "1440/1440 - 34s - loss: 4.1909 - accuracy: 0.2214 - 34s/epoch - 23ms/step\n",
      "Epoch 5/10\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "history_df = pd.DataFrame(columns=['loss', 'accuracy'])\n",
    "\n",
    "run_count = 1\n",
    "epochs_per_run = 10\n",
    "\n",
    "for run in range(0, run_count):\n",
    "    print(f\"Starting Run {run}/{run_count}\")\n",
    "    # Use verbose=2 here to prevent progreess bars locking up jupyter after a few hours\n",
    "    history = model.fit(xs, ys, epochs=epochs_per_run, verbose=2)\n",
    "    \n",
    "    dt = datetime.now() \n",
    "    model_filename = f\"data/nlp_models/model_{dt.year}_{dt.month}_{dt.day}_{dt.hour}_{dt.minute}.h5\"\n",
    "    model.save(model_filename)\n",
    "\n",
    "    history_df = pd.concat([history_df, pd.DataFrame(history.history)], ignore_index=True)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    plt.plot(history_df['accuracy'])\n",
    "    plt.title('Model Training Progress')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See how well the replacement Dan is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 05:17:25.826850: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-19 05:17:25.951195: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-19 05:17:25.963706: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lathe the new one over one over one over one afternoon about saving so now is managed directly kafka connect into\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"lathe\"\n",
    "next_words = 20\n",
    "\n",
    "word_lookup = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "seed_texts = [\n",
    "    \"the landrover\",\n",
    "    \"a data strategy is important because\",\n",
    "    \"it flies really\",\n",
    "    \"the diesel engine pumps out black smoke because\",\n",
    "    \"this article is about\",\n",
    "    \"Supply for indicators comes from aux relay\",\n",
    "    \"so we made one out of a chunk of\",\n",
    "    \"If a train is delayed or cancelled\",\n",
    "    \"Here we use KSQL to create\",\n",
    "    \"Looks like there are four\",\n",
    "    \"How much you invest in your data engineering capability\",\n",
    "    \"I believe\"\n",
    "]\n",
    "next_words = 30\n",
    "max_length = 12\n",
    "\n",
    "seed_texts = [s.lower() for s in seed_texts]\n",
    "for seed_text in seed_texts:\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_length-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=1)[0]\n",
    "        output_word = word_lookup[predicted]\n",
    "        seed_text += ' ' + output_word\n",
    "    print(seed_text)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c9d8d1d19ff2cb76f7bd7f9322b68ec18224c4c5af702e8deab1393bce8d7d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
